{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4R4EJElGk7H"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "Initially I tried to use the original version of the character level Recurrent Neural Network: https://github.com/karpathy/char-rnn which took a lot of setup to ru in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_QnLawzMTuLW",
        "outputId": "e1248a12-80d6-4e4e-e58d-a51b35fb2766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package python-software-properties is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  software-properties-common\n",
            "\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "fatal: destination path '/root/torch' already exists and is not an empty directory.\n",
            "Prefix set to /root/torch/install\n",
            "Installing Lua version: LUAJIT21\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/CMakeLists.txt:10 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:14 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:15 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:403 (INSTALL):\n",
            "  Policy CMP0177 is not set: install() DESTINATION paths are normalized.  Run\n",
            "  \"cmake --help-policy CMP0177\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luarocks/CMakeLists.txt:3 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luarocks/CMakeLists.txt:4 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "Found CUDA on your machine. Installing CMake 3.6 modules to get up-to-date FindCUDA\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/cmake/3.6/build\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"\"\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/parse_cubin.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/make2cmake.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/run_nvcc.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/select_compute_arch.cmake\n",
            "FindCuda bits of CMake 3.6 installed\n",
            "Installing common Lua packages\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/lfs.c -o src/lfs.o\n",
            "gcc -shared -o lfs.so -L/root/torch/install/lib src/lfs.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "luafilesystem 1.6.3-1 is now built and installed in /root/torch/install/ (license: MIT/X11)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "penlight scm-1 is now built and installed in /root/torch/install/ (license: MIT/X11)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c lua_cjson.c -o lua_cjson.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c strbuf.c -o strbuf.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c fpconv.c -o fpconv.o\n",
            "gcc -shared -o cjson.so -L/root/torch/install/lib lua_cjson.o strbuf.o fpconv.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "lua-cjson 2.1devel-1 is now built and installed in /root/torch/install/ (license: MIT)\n",
            "\n",
            "Installing core Torch packages\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c test.c -o test.o\n",
            "gcc -shared -o ffi/libtest.so -L/root/torch/install/lib test.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c call.c -o call.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c ctype.c -o ctype.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c ffi.c -o ffi.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c parser.c -o parser.o -Idynasm\n",
            "gcc -shared -o ffi.so -L/root/torch/install/lib call.o ctype.o ffi.o parser.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "luaffi scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/autolink.c -o src/autolink.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/buffer.c -o src/buffer.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/markdown.c -o src/markdown.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/stack.c -o src/stack.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/houdini_href_e.c -o html/houdini_href_e.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/houdini_html_e.c -o html/houdini_html_e.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/html.c -o html/html.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/html_smartypants.c -o html/html_smartypants.o -Isrc/ -Ihtml/\n",
            "gcc -shared -o libsundown.so -L/root/torch/install/lib src/autolink.o src/buffer.o src/markdown.o src/stack.o html/houdini_href_e.o html/houdini_html_e.o html/html.o html/html_smartypants.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "sundown scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "cwrap scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DLUALIB= -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" -DLUADIR=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1/lua\" -DLIBDIR=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1/lib\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1\" && make\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/paths/build\n",
            "[100%] Built target paths\n",
            "cd build && make install\n",
            "[100%] Built target paths\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/paths/scm-1/lua/paths/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/paths/scm-1/lib/libpaths.so\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "paths scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DLUA=/root/torch/install/bin/luajit -DLUALIB= -DLUA_BINDIR=\"/root/torch/install/bin\" -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" -DLUADIR=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lua\" -DLIBDIR=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lib\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1\" && make -j$(getconf _NPROCESSORS_ONLN)\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:5 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:6 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Compiling with OpenMP support\n",
            "\u001b[0mCMake Deprecation Warning at lib/TH/CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/TH/CMakeLists.txt:5 (CMAKE_POLICY):\n",
            "  The OLD behavior for policy CMP0026 will be removed from a future version\n",
            "  of CMake.\n",
            "\n",
            "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
            "  policies are deprecated and that a policy should be set to OLD only under\n",
            "  specific short-term circumstances.  Projects should be ported to the NEW\n",
            "  behavior and not rely on setting a policy to OLD.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Compiling with OpenMP support\n",
            "\u001b[33mCMake Warning (dev) at lib/TH/cmake/FindARM.cmake:5 (EXEC_PROGRAM):\n",
            "  Policy CMP0153 is not set: The exec_program command should not be called.\n",
            "  Run \"cmake --help-policy CMP0153\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "\n",
            "  Use execute_process() instead.\n",
            "Call Stack (most recent call first):\n",
            "  lib/TH/CMakeLists.txt:96 (FIND_PACKAGE)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could not find hardware support for NEON on this machine.\n",
            "-- No OMAP3 processor on this machine.\n",
            "-- No OMAP4 processor on this machine.\n",
            "-- SSE2 Found\n",
            "-- SSE3 Found\n",
            "-- AVX Found\n",
            "-- AVX2 Found\n",
            "-- TH_SO_VERSION: 0\n",
            "-- Atomics: using C11 intrinsics\n",
            "-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\n",
            "--   Library mkl_gf_lp64: /usr/lib/x86_64-linux-gnu/libmkl_gf_lp64.so\n",
            "--   Library mkl_gnu_thread: /usr/lib/x86_64-linux-gnu/libmkl_gnu_thread.so\n",
            "--   Library mkl_core: /usr/lib/x86_64-linux-gnu/libmkl_core.so\n",
            "--   Library iomp5: /usr/local/lib/libiomp5.so\n",
            "--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.a\n",
            "--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\n",
            "-- MKL library found\n",
            "-- Found a library with BLAS API (mkl).\n",
            "-- Found a library with LAPACK API. (mkl)\n",
            "\u001b[0mCMake Deprecation Warning at lib/luaT/CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  The OLD behavior for policy CMP0026 will be removed from a future version\n",
            "  of CMake.\n",
            "\n",
            "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
            "  policies are deprecated and that a policy should be set to OLD only under\n",
            "  specific short-term circumstances.  Projects should be ported to the NEW\n",
            "  behavior and not rely on setting a policy to OLD.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.1s)\n",
            "\u001b[33mCMake Warning (dev) at lib/TH/CMakeLists.txt:38 (ADD_LIBRARY):\n",
            "  Policy CMP0115 is not set: Source file extensions must be explicit.  Run\n",
            "  \"cmake --help-policy CMP0115\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "\n",
            "  File:\n",
            "\n",
            "    /root/torch/pkg/torch/lib/TH/THGeneral.h.in\n",
            "Call Stack (most recent call first):\n",
            "  lib/TH/CMakeLists.txt:266 (ADD_TORCH_LIBRARY)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/torch/build\n",
            "[  5%] Built target luaT\n",
            "[ 58%] Built target TH\n",
            "[ 61%] \u001b[32m\u001b[1mLinking C shared module libtorch.so\u001b[0m\n",
            "[100%] Built target torch\n",
            "cd build && make install\n",
            "[  5%] Built target luaT\n",
            "[ 58%] Built target TH\n",
            "[100%] Built target torch\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchExports.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchExports-release.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchConfig.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchWrap.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchPathsInit.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchPackage.cmake\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lib/libtorch.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lib/libtorch.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/File.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/Tensor.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/CmdLine.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/FFInterface.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/Tester.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/TestSuite.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/paths.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/README.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/tensor.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/cmdline.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/file.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/gather.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/index.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/timer.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/tester.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/maths.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/memoryfile.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/utility.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/diskfile.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/random.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/serialization.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/storage.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/pipefile.md\n",
            "-- Up-to-date: /root/torch/install/lib/libTH.so.0\n",
            "-- Up-to-date: /root/torch/install/lib/libTH.so\n",
            "-- Up-to-date: /root/torch/install/include/TH/TH.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THAllocator.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THMath.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THBlas.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THDiskFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THFilePrivate.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGeneral.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateAllTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateDoubleType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateFloatType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateHalfType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateLongType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateIntType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateShortType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateCharType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateByteType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateFloatTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateIntTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THLogAdd.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THMemoryFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THRandom.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THSize.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THStorage.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensor.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorApply.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorDimApply.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorMacros.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THVector.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THAtomic.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THHalf.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/vector/AVX.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/vector/AVX2.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THBlas.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THBlas.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THLapack.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorage.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorage.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorageCopy.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorageCopy.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensor.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensor.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorConv.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorConv.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorCopy.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorCopy.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorLapack.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorMath.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorMath.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorRandom.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorRandom.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THVectorDispatch.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THVector.h\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/THConfig.cmake\n",
            "-- Up-to-date: /root/torch/install/lib/libluaT.so.0\n",
            "-- Up-to-date: /root/torch/install/lib/libluaT.so\n",
            "-- Up-to-date: /root/torch/install/include/luaT.h\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/luaTConfig.cmake\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "torch scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "dok scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c utils.c -o utils.o\n",
            "gcc -shared -o treplutils.so -L/root/torch/install/lib utils.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c readline.c -o readline.o\n",
            "gcc -shared -o readline.so -L/root/torch/install/lib readline.o -lreadline\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "trepl scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DCMAKE_BUILD_TYPE=Release  -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/sys/1.1-0\" && make\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/sys/build\n",
            "[100%] Built target sys\n",
            "cd build && make install\n",
            "[100%] Built target sys\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lib/libsys.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/sys/1.1-0/lib/libsys.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/colors.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/fpath.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "sys 1.1-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "xlua 1.0-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "moses 1.6.1-1 is now built and installed in /root/torch/install/ (license: MIT <http://www.opensource.org/licenses/mit-license.php>)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/nn/scm-1\"  -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" && make\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "\u001b[0mCMake Deprecation Warning at lib/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THNN/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THNN/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- TH_LIBRARIES: TH\n",
            "-- Compiling with OpenMP support\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/nn/build\n",
            "[100%] Built target THNN\n",
            "cd build && make install\n",
            "[100%] Built target THNN\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Abs.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/AbsCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Add.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/AddConstant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/BCECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/BatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Bilinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Bottle.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAdd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAddTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAddTensorTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CDivTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMaxTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMinTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMul.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMulTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CSubTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Clamp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ClassNLLCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ClassSimplexCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Collapse.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Concat.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ConcatTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Constant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Container.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Contiguous.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Convert.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Copy.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Cosine.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CosineDistance.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CosineEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Criterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CriterionTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CrossEntropyCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Decorator.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DepthConcat.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DistKLDivCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DistanceRatioCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DontCast.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DotProduct.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Dropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ELU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ErrorMessages.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Euclidean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Exp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/FlattenTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GPU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GatedLinearUnit.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GradientReversal.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HardShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HardTanh.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HingeEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Identity.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Index.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/IndexLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Jacobian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/JoinTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Kmeans.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1Cost.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1HingeEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1Penalty.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LayerNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LeakyReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Linear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LinearWeightNorm.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Log.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LogSigmoid.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LogSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LookupTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MM.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MV.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MapTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MarginRankingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MaskedSelect.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Max.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Maxout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Mean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Min.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MixtureTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Module.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ModuleCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Mul.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MulConstant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiLabelMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiLabelSoftMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/NaN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Narrow.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/NarrowTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Normalize.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/OneHot.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Padding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PairwiseDistance.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Parallel.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ParallelCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ParallelTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PartialLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PixelShuffle.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Power.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PrintSize.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Profile.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/RReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ReLU6.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Replicate.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Reshape.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Select.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SelectTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sequential.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sigmoid.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SmoothL1Criterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMin.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftPlus.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftSign.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SparseJacobian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SparseLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAdaptiveAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAdaptiveMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAutoCropMSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialBatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialClassNLLCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialContrastiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionLocal.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionMM.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionMap.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialCrossMapLRN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDepthWiseConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDilatedConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDilatedMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDivisiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFractionalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFullConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFullConvolutionMap.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialLPPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialLogSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialMaxUnpooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialReflectionPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialReplicationPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSubSampling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSubtractiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialUpSamplingBilinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialUpSamplingNearest.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialZeroPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SplitTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sqrt.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Square.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Squeeze.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/StochasticGradient.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sum.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/THNN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/THNN_h.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Tanh.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TanhShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalDynamicKMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalRowConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalSubSampling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Threshold.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Transpose.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Unsqueeze.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/View.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricBatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDilatedConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDilatedMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricFractionalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricFullConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricMaxUnpooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricReplicationPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightNorm.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightedEuclidean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightedMSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WhiteNoise.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZeroGrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZipTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZipTableOneToMany.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/hessian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/utils.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/table.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/containers.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/testing.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/module.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/training.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/index.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/simple.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/logsoftmax.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sigmmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/prelu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/abs.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/elu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/relu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/tanh.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softmin.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/lenap.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/lena.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/rrelu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/relu6.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softmax.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/logsigmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softplus.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/power.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sigmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/htanh.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sshrink.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/square.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softsign.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/exp.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sqrt.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/hshrink.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/transfer.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/criterion.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/overview.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/convolution.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/README.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lib/libTHNN.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/nn/scm-1/lib/libTHNN.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Up-to-date: /root/torch/install/include/THNN/THNN.h\n",
            "-- Up-to-date: /root/torch/install/include/THNN/generic/THNN.h\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "nn scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/graph/scm-1\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/graph/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/Edge.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/Node.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/graphviz.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/init.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "graph scm-1 is now built and installed in /root/torch/install/ (license: UNKNOWN)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/nngraph/scm-1\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/nngraph/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/JustElement.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/JustTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/ModuleFromCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/gmodule.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/graphinspecting.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/nest.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/nesting.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/node.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/simple_print.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/utils.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "nngraph scm-1 is now built and installed in /root/torch/install/ (license: UNKNOWN)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\"  -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Compiling with OpenMP support\n",
            "-- Configuring done (0.1s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/image/build\n",
            "[ 25%] Built target ppm\n",
            "[ 50%] Built target jpeg\n",
            "[ 75%] Built target lua_png\n",
            "[100%] Built target image\n",
            "cd build && make install\n",
            "[ 25%] Built target ppm\n",
            "[ 50%] Built target jpeg\n",
            "[ 75%] Built target lua_png\n",
            "[100%] Built target image\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libppm.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libppm.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libjpeg.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libjpeg.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/liblua_png.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/liblua_png.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libimage.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libimage.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/win.ui\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P5.pgm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rgb2x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P2.pgm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P6.ppm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/gray3x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/gray16-1x2.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/grace_hopper_512.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P4.pbm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rectangle.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/fabio.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/fabio.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/foobar.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/bmp-without-ext\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/grace_hopper_512.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/corrupt-ihdr.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rgb16-2x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/README.md\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "image 1.1.alpha-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/optim/1.0.5-0\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/optim/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/ConfusionMatrix.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/Logger.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adadelta.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adagrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adam.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adamax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/asgd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/cg.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/checkgrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/cmaes.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/de.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/fista.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/lbfgs.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/lswolfe.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/nag.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/polyinterp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/rmsprop.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/rprop.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/sgd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/logger_plot.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/algos.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.svg.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.svg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/intro.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/logger.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/README.md\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "optim 1.0.5-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Found CUDA on your machine. Installing CUDA packages\n",
            "Warning: unmatched variable LUALIB\n",
            "\n",
            "jopts=$(getconf _NPROCESSORS_CONF)\n",
            "\n",
            "echo \"Building on $jopts cores\"\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DLUA_INCDIR=/root/torch/install/include -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS} -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/cutorch/scm-1\" && make -j$jopts install\n",
            "\n",
            "Building on 2 cores\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "\u001b[0mCMake Deprecation Warning at /root/torch/install/share/cmake/torch/FindCUDA.cmake:368 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:7 (FIND_PACKAGE)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THC/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THC/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Removing -DNDEBUG from compile flags\n",
            "-- TH_LIBRARIES: TH\n",
            "-- MAGMA not found. Compiling without MAGMA support\n",
            "-- Automatic GPU detection failed. Building for common architectures.\n",
            "-- Autodetected CUDA architecture(s): 3.0;3.5;5.0;5.2;6.0;6.1;6.1+PTX\n",
            "-- got cuda version 12.5\n",
            "-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n",
            "-- CUDA_NVCC_FLAGS: -gencode;arch=compute_30,code=sm_30;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_61,code=compute_61;-DCUDA_HAS_FP16=1\n",
            "-- THC_SO_VERSION: 0\n",
            "-- Configuring done (1.5s)\n",
            "\u001b[0mCMake Error: The following variables are used in this project, but they are set to NOTFOUND.\n",
            "Please set them or make sure they are set and tested correctly in the CMake files:\n",
            "CUDA_cublas_device_LIBRARY (ADVANCED)\n",
            "    linked by target \"THC\" in directory /root/torch/extra/cutorch/lib/THC\n",
            "\u001b[0m\n",
            "-- Generating done (0.0s)\n",
            "\u001b[0mCMake Generate step failed.  Build files cannot be regenerated correctly.\u001b[0m\n",
            "\n",
            "Error: Build error: Failed building.\n"
          ]
        }
      ],
      "source": [
        "!curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash\n",
        "!git clone https://github.com/torch/distro.git ~/torch --recursive\n",
        "! cd ~/torch;\n",
        "! ./install.sh      # and enter \"yes\" at the end to modify your bashrc\n",
        "! source ~/.bashrc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-hw8uCfXUoD",
        "outputId": "49e70599-023a-44b2-cec7-c67f65e430b5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lua5.1 is already the newest version (5.1.5-8.1build4).\n",
            "luajit is already the newest version (2.1.0~beta3+dfsg-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "luarocks is already the newest version (3.8.0+dfsg1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
            "\n",
            "Error: No results matching query were found for Lua 5.1.\n",
            "To check if it is available for other Lua versions, use --check-lua-versions.\n",
            "Installing https://luarocks.org/nn-1.0.5-1.src.rock\n",
            "\n",
            "nn 1.0.5-1 depends on lua >= 5.1, <= 5.3 (5.1-1 provided by VM)\n",
            "gcc -O2 -fPIC -I/usr/include/lua5.1 -c lnn.c -o lnn.o -I/usr/include\n",
            "gcc -shared -o nn.so lnn.o -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/x86_64-linux-gnu -lnanomsg\n",
            "nn 1.0.5-1 is now installed in /usr/local (license: MIT)\n",
            "\n",
            "\n",
            "Error: No results matching query were found for Lua 5.1.\n",
            "To check if it is available for other Lua versions, use --check-lua-versions.\n",
            "\n",
            "Error: No results matching query were found for Lua 5.1.\n",
            "To check if it is available for other Lua versions, use --check-lua-versions.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y lua5.1 luajit\n",
        "!apt-get install -y luarocks\n",
        "\n",
        "!luarocks install torch\n",
        "!luarocks install nn\n",
        "!luarocks install optim\n",
        "!luarocks install nngraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sLPBMELPX10g",
        "outputId": "34a01418-95d5-47a8-8c87-b6508ceaf968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (3.161.136.36)] [Connected to r2u.stat.il\r                                                                                                    \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Waiting for headers] [2 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r-project.org (3.161.1\r                                                                                                    \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [3 InRelease 12.7 kB/128 kB 10%] [2 InRelease 25.8 kB/129 kB 20%] [Waiting for headers] [Connecte\r                                                                                                    \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [3 InRelease 15.6 kB/128 kB 12%] [2 InRelease 43.1 kB/129 kB 33%] [Connected to r2u.stat.illinois\r                                                                                                    \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [3 InRelease 47.5 kB/128 kB 37%] [2 InRelease 85.1 kB/129 kB 66%] [Connected to r2u.stat.illinois\r0% [3 InRelease 73.5 kB/128 kB 57%] [Waiting for headers] [Connected to ppa.launchpadcontent.net (18\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.8\r                                                                                                    \rGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,639 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,664 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,531 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,941 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 11.2 MB in 2s (6,376 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libnanomsg-dev is already the newest version (1.1.5+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Install nanomsg dependency\n",
        "!apt-get update\n",
        "!apt-get install -y libnanomsg-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVFW8ZaMX7we",
        "outputId": "20c6a0a7-f4fa-4a4c-910e-45f3d101b8b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing https://luarocks.org/nn-1.0.5-1.src.rock\n",
            "\n",
            "nn 1.0.5-1 depends on lua >= 5.1, <= 5.3 (5.1-1 provided by VM)\n",
            "gcc -O2 -fPIC -I/usr/include/lua5.1 -c lnn.c -o lnn.o -I/usr/include\n",
            "gcc -shared -o nn.so lnn.o -L/usr/lib/x86_64-linux-gnu -Wl,-rpath,/usr/lib/x86_64-linux-gnu -lnanomsg\n",
            "nn 1.0.5-1 is now installed in /usr/local (license: MIT)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!luarocks install nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y lua5.1\n",
        "!git clone https://github.com/torch/distro.git ~/torch --recursive\n",
        "!cd ~/torch && bash install-deps\n",
        "!cd ~/torch && ./install.sh\n",
        "!source ~/.bashrc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SdL_4DFq5_z4",
        "outputId": "8d672c2b-fc94-4c2f-f79d-2ae5add55e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lua5.1 is already the newest version (5.1.5-8.1build4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
            "fatal: destination path '/root/torch' already exists and is not an empty directory.\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Updated successfully.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package python-software-properties is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  software-properties-common\n",
            "\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Prefix set to /root/torch/install\n",
            "Installing Lua version: LUAJIT21\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/CMakeLists.txt:10 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:14 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:15 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:403 (INSTALL):\n",
            "  Policy CMP0177 is not set: install() DESTINATION paths are normalized.  Run\n",
            "  \"cmake --help-policy CMP0177\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luarocks/CMakeLists.txt:3 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luarocks/CMakeLists.txt:4 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "Found CUDA on your machine. Installing CMake 3.6 modules to get up-to-date FindCUDA\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/cmake/3.6/build\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"\"\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/parse_cubin.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/make2cmake.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/run_nvcc.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/select_compute_arch.cmake\n",
            "FindCuda bits of CMake 3.6 installed\n",
            "Installing common Lua packages\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/lfs.c -o src/lfs.o\n",
            "gcc -shared -o lfs.so -L/root/torch/install/lib src/lfs.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "luafilesystem 1.6.3-1 is now built and installed in /root/torch/install/ (license: MIT/X11)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "penlight scm-1 is now built and installed in /root/torch/install/ (license: MIT/X11)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c lua_cjson.c -o lua_cjson.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c strbuf.c -o strbuf.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c fpconv.c -o fpconv.o\n",
            "gcc -shared -o cjson.so -L/root/torch/install/lib lua_cjson.o strbuf.o fpconv.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "lua-cjson 2.1devel-1 is now built and installed in /root/torch/install/ (license: MIT)\n",
            "\n",
            "Installing core Torch packages\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c test.c -o test.o\n",
            "gcc -shared -o ffi/libtest.so -L/root/torch/install/lib test.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c call.c -o call.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c ctype.c -o ctype.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c ffi.c -o ffi.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c parser.c -o parser.o -Idynasm\n",
            "gcc -shared -o ffi.so -L/root/torch/install/lib call.o ctype.o ffi.o parser.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "luaffi scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/autolink.c -o src/autolink.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/buffer.c -o src/buffer.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/markdown.c -o src/markdown.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/stack.c -o src/stack.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/houdini_href_e.c -o html/houdini_href_e.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/houdini_html_e.c -o html/houdini_html_e.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/html.c -o html/html.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/html_smartypants.c -o html/html_smartypants.o -Isrc/ -Ihtml/\n",
            "gcc -shared -o libsundown.so -L/root/torch/install/lib src/autolink.o src/buffer.o src/markdown.o src/stack.o html/houdini_href_e.o html/houdini_html_e.o html/html.o html/html_smartypants.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "sundown scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "cwrap scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DLUALIB= -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" -DLUADIR=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1/lua\" -DLIBDIR=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1/lib\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1\" && make\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/paths/build\n",
            "[100%] Built target paths\n",
            "cd build && make install\n",
            "[100%] Built target paths\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/paths/scm-1/lua/paths/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/paths/scm-1/lib/libpaths.so\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "paths scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DLUA=/root/torch/install/bin/luajit -DLUALIB= -DLUA_BINDIR=\"/root/torch/install/bin\" -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" -DLUADIR=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lua\" -DLIBDIR=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lib\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1\" && make -j$(getconf _NPROCESSORS_ONLN)\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:5 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:6 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Compiling with OpenMP support\n",
            "\u001b[0mCMake Deprecation Warning at lib/TH/CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/TH/CMakeLists.txt:5 (CMAKE_POLICY):\n",
            "  The OLD behavior for policy CMP0026 will be removed from a future version\n",
            "  of CMake.\n",
            "\n",
            "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
            "  policies are deprecated and that a policy should be set to OLD only under\n",
            "  specific short-term circumstances.  Projects should be ported to the NEW\n",
            "  behavior and not rely on setting a policy to OLD.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Compiling with OpenMP support\n",
            "\u001b[33mCMake Warning (dev) at lib/TH/cmake/FindARM.cmake:5 (EXEC_PROGRAM):\n",
            "  Policy CMP0153 is not set: The exec_program command should not be called.\n",
            "  Run \"cmake --help-policy CMP0153\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "\n",
            "  Use execute_process() instead.\n",
            "Call Stack (most recent call first):\n",
            "  lib/TH/CMakeLists.txt:96 (FIND_PACKAGE)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could not find hardware support for NEON on this machine.\n",
            "-- No OMAP3 processor on this machine.\n",
            "-- No OMAP4 processor on this machine.\n",
            "-- SSE2 Found\n",
            "-- SSE3 Found\n",
            "-- AVX Found\n",
            "-- AVX2 Found\n",
            "-- TH_SO_VERSION: 0\n",
            "-- Atomics: using C11 intrinsics\n",
            "-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\n",
            "--   Library mkl_gf_lp64: /usr/lib/x86_64-linux-gnu/libmkl_gf_lp64.so\n",
            "--   Library mkl_gnu_thread: /usr/lib/x86_64-linux-gnu/libmkl_gnu_thread.so\n",
            "--   Library mkl_core: /usr/lib/x86_64-linux-gnu/libmkl_core.so\n",
            "--   Library iomp5: /usr/local/lib/libiomp5.so\n",
            "--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.a\n",
            "--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\n",
            "-- MKL library found\n",
            "-- Found a library with BLAS API (mkl).\n",
            "-- Found a library with LAPACK API. (mkl)\n",
            "\u001b[0mCMake Deprecation Warning at lib/luaT/CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  The OLD behavior for policy CMP0026 will be removed from a future version\n",
            "  of CMake.\n",
            "\n",
            "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
            "  policies are deprecated and that a policy should be set to OLD only under\n",
            "  specific short-term circumstances.  Projects should be ported to the NEW\n",
            "  behavior and not rely on setting a policy to OLD.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.1s)\n",
            "\u001b[33mCMake Warning (dev) at lib/TH/CMakeLists.txt:38 (ADD_LIBRARY):\n",
            "  Policy CMP0115 is not set: Source file extensions must be explicit.  Run\n",
            "  \"cmake --help-policy CMP0115\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "\n",
            "  File:\n",
            "\n",
            "    /root/torch/pkg/torch/lib/TH/THGeneral.h.in\n",
            "Call Stack (most recent call first):\n",
            "  lib/TH/CMakeLists.txt:266 (ADD_TORCH_LIBRARY)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/torch/build\n",
            "[  5%] Built target luaT\n",
            "[ 58%] Built target TH\n",
            "[100%] Built target torch\n",
            "cd build && make install\n",
            "[  5%] Built target luaT\n",
            "[ 58%] Built target TH\n",
            "[100%] Built target torch\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchExports.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchExports-release.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchConfig.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchWrap.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchPathsInit.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchPackage.cmake\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lib/libtorch.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lib/libtorch.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/File.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/Tensor.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/CmdLine.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/FFInterface.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/Tester.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/TestSuite.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/paths.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/README.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/tensor.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/cmdline.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/file.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/gather.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/index.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/timer.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/tester.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/maths.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/memoryfile.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/utility.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/diskfile.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/random.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/serialization.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/storage.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/pipefile.md\n",
            "-- Up-to-date: /root/torch/install/lib/libTH.so.0\n",
            "-- Up-to-date: /root/torch/install/lib/libTH.so\n",
            "-- Up-to-date: /root/torch/install/include/TH/TH.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THAllocator.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THMath.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THBlas.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THDiskFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THFilePrivate.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGeneral.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateAllTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateDoubleType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateFloatType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateHalfType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateLongType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateIntType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateShortType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateCharType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateByteType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateFloatTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateIntTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THLogAdd.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THMemoryFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THRandom.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THSize.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THStorage.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensor.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorApply.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorDimApply.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorMacros.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THVector.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THAtomic.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THHalf.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/vector/AVX.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/vector/AVX2.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THBlas.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THBlas.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THLapack.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorage.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorage.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorageCopy.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorageCopy.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensor.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensor.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorConv.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorConv.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorCopy.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorCopy.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorLapack.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorMath.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorMath.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorRandom.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorRandom.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THVectorDispatch.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THVector.h\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/THConfig.cmake\n",
            "-- Up-to-date: /root/torch/install/lib/libluaT.so.0\n",
            "-- Up-to-date: /root/torch/install/lib/libluaT.so\n",
            "-- Up-to-date: /root/torch/install/include/luaT.h\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/luaTConfig.cmake\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "torch scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "dok scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c utils.c -o utils.o\n",
            "gcc -shared -o treplutils.so -L/root/torch/install/lib utils.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c readline.c -o readline.o\n",
            "gcc -shared -o readline.so -L/root/torch/install/lib readline.o -lreadline\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "trepl scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DCMAKE_BUILD_TYPE=Release  -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/sys/1.1-0\" && make\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/sys/build\n",
            "[100%] Built target sys\n",
            "cd build && make install\n",
            "[100%] Built target sys\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lib/libsys.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/sys/1.1-0/lib/libsys.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/colors.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/fpath.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "sys 1.1-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "xlua 1.0-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "moses 1.6.1-1 is now built and installed in /root/torch/install/ (license: MIT <http://www.opensource.org/licenses/mit-license.php>)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/nn/scm-1\"  -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" && make\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "\u001b[0mCMake Deprecation Warning at lib/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THNN/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THNN/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- TH_LIBRARIES: TH\n",
            "-- Compiling with OpenMP support\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/nn/build\n",
            "[100%] Built target THNN\n",
            "cd build && make install\n",
            "[100%] Built target THNN\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Abs.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/AbsCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Add.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/AddConstant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/BCECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/BatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Bilinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Bottle.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAdd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAddTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAddTensorTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CDivTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMaxTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMinTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMul.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMulTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CSubTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Clamp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ClassNLLCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ClassSimplexCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Collapse.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Concat.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ConcatTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Constant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Container.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Contiguous.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Convert.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Copy.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Cosine.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CosineDistance.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CosineEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Criterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CriterionTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CrossEntropyCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Decorator.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DepthConcat.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DistKLDivCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DistanceRatioCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DontCast.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DotProduct.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Dropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ELU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ErrorMessages.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Euclidean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Exp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/FlattenTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GPU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GatedLinearUnit.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GradientReversal.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HardShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HardTanh.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HingeEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Identity.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Index.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/IndexLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Jacobian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/JoinTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Kmeans.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1Cost.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1HingeEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1Penalty.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LayerNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LeakyReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Linear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LinearWeightNorm.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Log.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LogSigmoid.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LogSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LookupTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MM.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MV.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MapTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MarginRankingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MaskedSelect.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Max.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Maxout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Mean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Min.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MixtureTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Module.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ModuleCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Mul.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MulConstant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiLabelMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiLabelSoftMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/NaN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Narrow.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/NarrowTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Normalize.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/OneHot.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Padding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PairwiseDistance.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Parallel.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ParallelCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ParallelTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PartialLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PixelShuffle.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Power.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PrintSize.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Profile.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/RReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ReLU6.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Replicate.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Reshape.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Select.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SelectTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sequential.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sigmoid.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SmoothL1Criterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMin.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftPlus.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftSign.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SparseJacobian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SparseLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAdaptiveAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAdaptiveMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAutoCropMSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialBatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialClassNLLCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialContrastiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionLocal.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionMM.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionMap.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialCrossMapLRN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDepthWiseConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDilatedConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDilatedMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDivisiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFractionalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFullConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFullConvolutionMap.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialLPPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialLogSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialMaxUnpooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialReflectionPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialReplicationPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSubSampling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSubtractiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialUpSamplingBilinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialUpSamplingNearest.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialZeroPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SplitTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sqrt.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Square.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Squeeze.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/StochasticGradient.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sum.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/THNN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/THNN_h.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Tanh.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TanhShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalDynamicKMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalRowConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalSubSampling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Threshold.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Transpose.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Unsqueeze.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/View.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricBatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDilatedConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDilatedMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricFractionalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricFullConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricMaxUnpooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricReplicationPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightNorm.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightedEuclidean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightedMSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WhiteNoise.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZeroGrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZipTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZipTableOneToMany.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/hessian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/utils.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/table.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/containers.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/testing.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/module.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/training.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/index.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/simple.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/logsoftmax.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sigmmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/prelu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/abs.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/elu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/relu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/tanh.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softmin.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/lenap.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/lena.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/rrelu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/relu6.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softmax.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/logsigmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softplus.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/power.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sigmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/htanh.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sshrink.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/square.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softsign.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/exp.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sqrt.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/hshrink.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/transfer.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/criterion.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/overview.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/convolution.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/README.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lib/libTHNN.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/nn/scm-1/lib/libTHNN.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Up-to-date: /root/torch/install/include/THNN/THNN.h\n",
            "-- Up-to-date: /root/torch/install/include/THNN/generic/THNN.h\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "nn scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/graph/scm-1\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/graph/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/Edge.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/Node.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/graphviz.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/init.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "graph scm-1 is now built and installed in /root/torch/install/ (license: UNKNOWN)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/nngraph/scm-1\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/nngraph/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/JustElement.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/JustTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/ModuleFromCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/gmodule.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/graphinspecting.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/nest.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/nesting.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/node.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/simple_print.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/utils.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "nngraph scm-1 is now built and installed in /root/torch/install/ (license: UNKNOWN)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\"  -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Compiling with OpenMP support\n",
            "-- Configuring done (0.2s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/image/build\n",
            "[ 25%] Built target ppm\n",
            "[ 50%] Built target jpeg\n",
            "[ 75%] Built target lua_png\n",
            "[100%] Built target image\n",
            "cd build && make install\n",
            "[ 25%] Built target ppm\n",
            "[ 50%] Built target jpeg\n",
            "[ 75%] Built target lua_png\n",
            "[100%] Built target image\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libppm.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libppm.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libjpeg.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libjpeg.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/liblua_png.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/liblua_png.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libimage.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libimage.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/win.ui\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P5.pgm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rgb2x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P2.pgm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P6.ppm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/gray3x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/gray16-1x2.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/grace_hopper_512.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P4.pbm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rectangle.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/fabio.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/fabio.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/foobar.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/bmp-without-ext\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/grace_hopper_512.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/corrupt-ihdr.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rgb16-2x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/README.md\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "image 1.1.alpha-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/optim/1.0.5-0\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/optim/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/ConfusionMatrix.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/Logger.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adadelta.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adagrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adam.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adamax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/asgd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/cg.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/checkgrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/cmaes.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/de.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/fista.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/lbfgs.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/lswolfe.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/nag.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/polyinterp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/rmsprop.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/rprop.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/sgd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/logger_plot.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/algos.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.svg.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.svg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/intro.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/logger.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/README.md\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "optim 1.0.5-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Found CUDA on your machine. Installing CUDA packages\n",
            "Warning: unmatched variable LUALIB\n",
            "\n",
            "jopts=$(getconf _NPROCESSORS_CONF)\n",
            "\n",
            "echo \"Building on $jopts cores\"\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DLUA_INCDIR=/root/torch/install/include -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS} -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/cutorch/scm-1\" && make -j$jopts install\n",
            "\n",
            "Building on 2 cores\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "\u001b[0mCMake Deprecation Warning at /root/torch/install/share/cmake/torch/FindCUDA.cmake:368 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:7 (FIND_PACKAGE)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THC/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THC/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Removing -DNDEBUG from compile flags\n",
            "-- TH_LIBRARIES: TH\n",
            "-- MAGMA not found. Compiling without MAGMA support\n",
            "-- Automatic GPU detection failed. Building for common architectures.\n",
            "-- Autodetected CUDA architecture(s): 3.0;3.5;5.0;5.2;6.0;6.1;6.1+PTX\n",
            "-- got cuda version 12.5\n",
            "-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n",
            "-- CUDA_NVCC_FLAGS: -gencode;arch=compute_30,code=sm_30;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_61,code=compute_61;-DCUDA_HAS_FP16=1\n",
            "-- THC_SO_VERSION: 0\n",
            "-- Configuring done (1.7s)\n",
            "\u001b[0mCMake Error: The following variables are used in this project, but they are set to NOTFOUND.\n",
            "Please set them or make sure they are set and tested correctly in the CMake files:\n",
            "CUDA_cublas_device_LIBRARY (ADVANCED)\n",
            "    linked by target \"THC\" in directory /root/torch/extra/cutorch/lib/THC\n",
            "\u001b[0m\n",
            "-- Generating done (0.0s)\n",
            "\u001b[0mCMake Generate step failed.  Build files cannot be regenerated correctly.\u001b[0m\n",
            "\n",
            "Error: Build error: Failed building.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash\n",
        "!git clone https://github.com/torch/distro.git ~/torch --recursive\n",
        "!cd ~/torch && ./install.sh -b\n",
        "\n",
        "import os\n",
        "os.environ[\"PATH\"] += \":/root/torch/install/bin\"\n",
        "!th -e \"require 'torch'; print('Torch installed successfully')\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i_h2eOGi6hJZ",
        "outputId": "a9b1c0e7-e0be-4871-abb5-89e671519be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connected to cloud.r-project.org (3.161.\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package python-software-properties is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  software-properties-common\n",
            "\n",
            "E: Package 'python-software-properties' has no installation candidate\n",
            "fatal: destination path '/root/torch' already exists and is not an empty directory.\n",
            "Prefix set to /root/torch/install\n",
            "Installing Lua version: LUAJIT21\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/CMakeLists.txt:9 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/CMakeLists.txt:10 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:14 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:15 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at exe/luajit-rocks/luajit-2.1/CMakeLists.txt:403 (INSTALL):\n",
            "  Policy CMP0177 is not set: install() DESTINATION paths are normalized.  Run\n",
            "  \"cmake --help-policy CMP0177\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luarocks/CMakeLists.txt:3 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at exe/luajit-rocks/luarocks/CMakeLists.txt:4 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "Found CUDA on your machine. Installing CMake 3.6 modules to get up-to-date FindCUDA\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/cmake/3.6/build\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"\"\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/parse_cubin.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/make2cmake.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/run_nvcc.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/FindCUDA/select_compute_arch.cmake\n",
            "FindCuda bits of CMake 3.6 installed\n",
            "Installing common Lua packages\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/lfs.c -o src/lfs.o\n",
            "gcc -shared -o lfs.so -L/root/torch/install/lib src/lfs.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "luafilesystem 1.6.3-1 is now built and installed in /root/torch/install/ (license: MIT/X11)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "penlight scm-1 is now built and installed in /root/torch/install/ (license: MIT/X11)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c lua_cjson.c -o lua_cjson.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c strbuf.c -o strbuf.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c fpconv.c -o fpconv.o\n",
            "gcc -shared -o cjson.so -L/root/torch/install/lib lua_cjson.o strbuf.o fpconv.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "lua-cjson 2.1devel-1 is now built and installed in /root/torch/install/ (license: MIT)\n",
            "\n",
            "Installing core Torch packages\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c test.c -o test.o\n",
            "gcc -shared -o ffi/libtest.so -L/root/torch/install/lib test.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c call.c -o call.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c ctype.c -o ctype.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c ffi.c -o ffi.o -Idynasm\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c parser.c -o parser.o -Idynasm\n",
            "gcc -shared -o ffi.so -L/root/torch/install/lib call.o ctype.o ffi.o parser.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "luaffi scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/autolink.c -o src/autolink.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/buffer.c -o src/buffer.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/markdown.c -o src/markdown.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c src/stack.c -o src/stack.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/houdini_href_e.c -o html/houdini_href_e.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/houdini_html_e.c -o html/houdini_html_e.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/html.c -o html/html.o -Isrc/ -Ihtml/\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c html/html_smartypants.c -o html/html_smartypants.o -Isrc/ -Ihtml/\n",
            "gcc -shared -o libsundown.so -L/root/torch/install/lib src/autolink.o src/buffer.o src/markdown.o src/stack.o html/houdini_href_e.o html/houdini_html_e.o html/html.o html/html_smartypants.o\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "sundown scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "cwrap scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DLUALIB= -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" -DLUADIR=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1/lua\" -DLIBDIR=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1/lib\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/paths/scm-1\" && make\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/paths/build\n",
            "[100%] Built target paths\n",
            "cd build && make install\n",
            "[100%] Built target paths\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/paths/scm-1/lua/paths/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/paths/scm-1/lib/libpaths.so\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "paths scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DLUA=/root/torch/install/bin/luajit -DLUALIB= -DLUA_BINDIR=\"/root/torch/install/bin\" -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" -DLUADIR=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lua\" -DLIBDIR=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lib\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/torch/scm-1\" && make -j$(getconf _NPROCESSORS_ONLN)\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:5 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:6 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Compiling with OpenMP support\n",
            "\u001b[0mCMake Deprecation Warning at lib/TH/CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/TH/CMakeLists.txt:5 (CMAKE_POLICY):\n",
            "  The OLD behavior for policy CMP0026 will be removed from a future version\n",
            "  of CMake.\n",
            "\n",
            "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
            "  policies are deprecated and that a policy should be set to OLD only under\n",
            "  specific short-term circumstances.  Projects should be ported to the NEW\n",
            "  behavior and not rely on setting a policy to OLD.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Compiling with OpenMP support\n",
            "\u001b[33mCMake Warning (dev) at lib/TH/cmake/FindARM.cmake:5 (EXEC_PROGRAM):\n",
            "  Policy CMP0153 is not set: The exec_program command should not be called.\n",
            "  Run \"cmake --help-policy CMP0153\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "\n",
            "  Use execute_process() instead.\n",
            "Call Stack (most recent call first):\n",
            "  lib/TH/CMakeLists.txt:96 (FIND_PACKAGE)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could not find hardware support for NEON on this machine.\n",
            "-- No OMAP3 processor on this machine.\n",
            "-- No OMAP4 processor on this machine.\n",
            "-- SSE2 Found\n",
            "-- SSE3 Found\n",
            "-- AVX Found\n",
            "-- AVX2 Found\n",
            "-- TH_SO_VERSION: 0\n",
            "-- Atomics: using C11 intrinsics\n",
            "-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - iomp5 - pthread - m]\n",
            "--   Library mkl_gf_lp64: /usr/lib/x86_64-linux-gnu/libmkl_gf_lp64.so\n",
            "--   Library mkl_gnu_thread: /usr/lib/x86_64-linux-gnu/libmkl_gnu_thread.so\n",
            "--   Library mkl_core: /usr/lib/x86_64-linux-gnu/libmkl_core.so\n",
            "--   Library iomp5: /usr/local/lib/libiomp5.so\n",
            "--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.a\n",
            "--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\n",
            "-- MKL library found\n",
            "-- Found a library with BLAS API (mkl).\n",
            "-- Found a library with LAPACK API. (mkl)\n",
            "\u001b[0mCMake Deprecation Warning at lib/luaT/CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  The OLD behavior for policy CMP0026 will be removed from a future version\n",
            "  of CMake.\n",
            "\n",
            "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
            "  policies are deprecated and that a policy should be set to OLD only under\n",
            "  specific short-term circumstances.  Projects should be ported to the NEW\n",
            "  behavior and not rely on setting a policy to OLD.\n",
            "\n",
            "\u001b[0m\n",
            "-- Configuring done (0.1s)\n",
            "\u001b[33mCMake Warning (dev) at lib/TH/CMakeLists.txt:38 (ADD_LIBRARY):\n",
            "  Policy CMP0115 is not set: Source file extensions must be explicit.  Run\n",
            "  \"cmake --help-policy CMP0115\" for policy details.  Use the cmake_policy\n",
            "  command to set the policy and suppress this warning.\n",
            "\n",
            "  File:\n",
            "\n",
            "    /root/torch/pkg/torch/lib/TH/THGeneral.h.in\n",
            "Call Stack (most recent call first):\n",
            "  lib/TH/CMakeLists.txt:266 (ADD_TORCH_LIBRARY)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/torch/build\n",
            "[  5%] Built target luaT\n",
            "[ 58%] Built target TH\n",
            "[100%] Built target torch\n",
            "cd build && make install\n",
            "[  5%] Built target luaT\n",
            "[ 58%] Built target TH\n",
            "[100%] Built target torch\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchExports.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchExports-release.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchConfig.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchWrap.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchPathsInit.cmake\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/TorchPackage.cmake\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lib/libtorch.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/torch/scm-1/lib/libtorch.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/File.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/Tensor.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/CmdLine.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/FFInterface.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/Tester.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/TestSuite.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/paths.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/README.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/tensor.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/cmdline.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/file.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/gather.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/index.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/timer.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/tester.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/maths.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/memoryfile.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/utility.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/diskfile.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/random.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/serialization.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/storage.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/torch/scm-1/lua/torch/doc/pipefile.md\n",
            "-- Up-to-date: /root/torch/install/lib/libTH.so.0\n",
            "-- Up-to-date: /root/torch/install/lib/libTH.so\n",
            "-- Up-to-date: /root/torch/install/include/TH/TH.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THAllocator.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THMath.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THBlas.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THDiskFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THFilePrivate.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGeneral.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateAllTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateDoubleType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateFloatType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateHalfType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateLongType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateIntType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateShortType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateCharType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateByteType.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateFloatTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THGenerateIntTypes.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THLogAdd.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THMemoryFile.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THRandom.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THSize.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THStorage.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensor.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorApply.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorDimApply.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THTensorMacros.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THVector.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THAtomic.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/THHalf.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/vector/AVX.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/vector/AVX2.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THBlas.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THBlas.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THLapack.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorage.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorage.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorageCopy.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THStorageCopy.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensor.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensor.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorConv.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorConv.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorCopy.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorCopy.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorLapack.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorLapack.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorMath.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorMath.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorRandom.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THTensorRandom.h\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THVectorDispatch.c\n",
            "-- Up-to-date: /root/torch/install/include/TH/generic/THVector.h\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/THConfig.cmake\n",
            "-- Up-to-date: /root/torch/install/lib/libluaT.so.0\n",
            "-- Up-to-date: /root/torch/install/lib/libluaT.so\n",
            "-- Up-to-date: /root/torch/install/include/luaT.h\n",
            "-- Up-to-date: /root/torch/install/share/cmake/torch/luaTConfig.cmake\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "torch scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "dok scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c utils.c -o utils.o\n",
            "gcc -shared -o treplutils.so -L/root/torch/install/lib utils.o\n",
            "gcc -O2 -fPIC -I/root/torch/install/include -c readline.c -o readline.o\n",
            "gcc -shared -o readline.so -L/root/torch/install/lib readline.o -lreadline\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "trepl scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DCMAKE_BUILD_TYPE=Release  -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/sys/1.1-0\" && make\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/sys/build\n",
            "[100%] Built target sys\n",
            "cd build && make install\n",
            "[100%] Built target sys\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lib/libsys.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/sys/1.1-0/lib/libsys.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/colors.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/sys/1.1-0/lua/sys/fpath.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "sys 1.1-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "xlua 1.0-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "moses 1.6.1-1 is now built and installed in /root/torch/install/ (license: MIT <http://www.opensource.org/licenses/mit-license.php>)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/nn/scm-1\"  -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\" && make\n",
            "\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "\u001b[0mCMake Deprecation Warning at lib/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THNN/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THNN/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- TH_LIBRARIES: TH\n",
            "-- Compiling with OpenMP support\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/nn/build\n",
            "[100%] Built target THNN\n",
            "cd build && make install\n",
            "[100%] Built target THNN\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Abs.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/AbsCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Add.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/AddConstant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/BCECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/BatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Bilinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Bottle.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAdd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAddTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CAddTensorTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CDivTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMaxTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMinTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMul.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CMulTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CSubTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Clamp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ClassNLLCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ClassSimplexCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Collapse.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Concat.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ConcatTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Constant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Container.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Contiguous.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Convert.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Copy.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Cosine.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CosineDistance.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CosineEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Criterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CriterionTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/CrossEntropyCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Decorator.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DepthConcat.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DistKLDivCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DistanceRatioCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DontCast.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/DotProduct.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Dropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ELU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ErrorMessages.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Euclidean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Exp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/FlattenTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GPU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GatedLinearUnit.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/GradientReversal.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HardShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HardTanh.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/HingeEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Identity.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Index.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/IndexLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Jacobian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/JoinTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Kmeans.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1Cost.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1HingeEmbeddingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/L1Penalty.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LayerNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LeakyReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Linear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LinearWeightNorm.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Log.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LogSigmoid.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LogSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/LookupTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MM.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MV.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MapTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MarginRankingCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MaskedSelect.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Max.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Maxout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Mean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Min.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MixtureTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Module.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ModuleCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Mul.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MulConstant.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiLabelMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiLabelSoftMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/MultiMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/NaN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Narrow.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/NarrowTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Normalize.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/OneHot.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Padding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PairwiseDistance.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Parallel.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ParallelCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ParallelTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PartialLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PixelShuffle.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Power.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/PrintSize.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Profile.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/RReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ReLU.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ReLU6.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Replicate.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Reshape.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Select.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SelectTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sequential.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sigmoid.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SmoothL1Criterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMarginCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftMin.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftPlus.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SoftSign.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SparseJacobian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SparseLinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAdaptiveAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAdaptiveMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAutoCropMSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialBatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialClassNLLCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialContrastiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionLocal.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionMM.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialConvolutionMap.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialCrossMapLRN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDepthWiseConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDilatedConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDilatedMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDivisiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialDropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFractionalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFullConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialFullConvolutionMap.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialLPPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialLogSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialMaxUnpooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialReflectionPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialReplicationPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSoftMax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSubSampling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialSubtractiveNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialUpSamplingBilinear.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialUpSamplingNearest.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SpatialZeroPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/SplitTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sqrt.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Square.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Squeeze.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/StochasticGradient.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Sum.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/THNN.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/THNN_h.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Tanh.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TanhShrink.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalDynamicKMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalRowConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/TemporalSubSampling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Threshold.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Transpose.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/Unsqueeze.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/View.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricAveragePooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricBatchNormalization.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDilatedConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDilatedMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricDropout.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricFractionalMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricFullConvolution.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricMaxPooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricMaxUnpooling.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/VolumetricReplicationPadding.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightNorm.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightedEuclidean.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WeightedMSECriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/WhiteNoise.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZeroGrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZipTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/ZipTableOneToMany.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/hessian.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/utils.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/table.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/containers.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/testing.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/module.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/training.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/index.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/simple.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/logsoftmax.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sigmmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/prelu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/abs.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/elu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/relu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/tanh.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softmin.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/lenap.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/lena.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/rrelu.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/relu6.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softmax.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/logsigmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softplus.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/power.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sigmoid.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/htanh.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sshrink.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/square.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/softsign.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/exp.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/sqrt.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/image/hshrink.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/transfer.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/criterion.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/overview.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/doc/convolution.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lua/nn/README.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nn/scm-1/lib/libTHNN.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/nn/scm-1/lib/libTHNN.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Up-to-date: /root/torch/install/include/THNN/THNN.h\n",
            "-- Up-to-date: /root/torch/install/include/THNN/generic/THNN.h\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "nn scm-1 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/graph/scm-1\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/graph/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/Edge.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/Node.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/graphviz.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/graph/scm-1/lua/graph/init.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "graph scm-1 is now built and installed in /root/torch/install/ (license: UNKNOWN)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/nngraph/scm-1\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/extra/nngraph/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/JustElement.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/JustTable.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/ModuleFromCriterion.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/gmodule.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/graphinspecting.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/nest.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/nesting.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/node.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/simple_print.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/nngraph/scm-1/lua/nngraph/utils.lua\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "nngraph scm-1 is now built and installed in /root/torch/install/ (license: UNKNOWN)\n",
            "\n",
            "Warning: unmatched variable LUALIB\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DLUA_INCDIR=\"/root/torch/install/include\" -DLUA_LIBDIR=\"/root/torch/install/lib\"  -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Compiling with OpenMP support\n",
            "-- Configuring done (0.1s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/image/build\n",
            "[ 25%] Built target ppm\n",
            "[ 50%] Built target jpeg\n",
            "[ 75%] Built target lua_png\n",
            "[100%] Built target image\n",
            "cd build && make install\n",
            "[ 25%] Built target ppm\n",
            "[ 50%] Built target jpeg\n",
            "[ 75%] Built target lua_png\n",
            "[100%] Built target image\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libppm.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libppm.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libjpeg.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libjpeg.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/liblua_png.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/liblua_png.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libimage.so\n",
            "-- Set non-toolchain portion of runtime path of \"/root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lib/libimage.so\" to \"$ORIGIN/../lib:/root/torch/install/lib:/usr/local/lib\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/win.ui\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/test.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P5.pgm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rgb2x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P2.pgm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P6.ppm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/gray3x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/gray16-1x2.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/grace_hopper_512.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/P4.pbm\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rectangle.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/fabio.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/fabio.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/foobar.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/bmp-without-ext\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/grace_hopper_512.jpg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/corrupt-ihdr.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/assets/rgb16-2x1.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/image/1.1.alpha-0/lua/image/README.md\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "image 1.1.alpha-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "cmake -E make_directory build && cd build && cmake .. -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/optim/1.0.5-0\" && make\n",
            "   \n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:3 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /root/torch/pkg/optim/build\n",
            "cd build && make install\n",
            "\u001b[36mInstall the project...\u001b[0m\n",
            "-- Install configuration: \"Release\"\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/ConfusionMatrix.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/Logger.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adadelta.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adagrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adam.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/adamax.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/asgd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/cg.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/checkgrad.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/cmaes.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/de.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/fista.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/init.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/lbfgs.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/lswolfe.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/nag.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/polyinterp.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/rmsprop.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/rprop.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/sgd.lua\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/logger_plot.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/algos.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.svg.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.svg\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/image/parameterflattening.png\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/intro.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/doc/logger.md\n",
            "-- Installing: /root/torch/install/lib/luarocks/rocks/optim/1.0.5-0/lua/optim/README.md\n",
            "Updating manifest for /root/torch/install/lib/luarocks/rocks\n",
            "optim 1.0.5-0 is now built and installed in /root/torch/install/ (license: BSD)\n",
            "\n",
            "Found CUDA on your machine. Installing CUDA packages\n",
            "Warning: unmatched variable LUALIB\n",
            "\n",
            "jopts=$(getconf _NPROCESSORS_CONF)\n",
            "\n",
            "echo \"Building on $jopts cores\"\n",
            "cmake -E make_directory build && cd build && cmake .. -DLUALIB= -DLUA_INCDIR=/root/torch/install/include -DCMAKE_CXX_FLAGS=${CMAKE_CXX_FLAGS} -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=\"/root/torch/install/bin/..\" -DCMAKE_INSTALL_PREFIX=\"/root/torch/install/lib/luarocks/rocks/cutorch/scm-1\" && make -j$jopts install\n",
            "\n",
            "Building on 2 cores\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  No project() command is present.  The top-level CMakeLists.txt file must\n",
            "  contain a literal, direct call to the project() command.  Add a line of\n",
            "  code such as\n",
            "\n",
            "    project(ProjectName)\n",
            "\n",
            "  near the top of the file, but after cmake_minimum_required().\n",
            "\n",
            "  CMake is pretending there is a \"project(Project)\" command on the first\n",
            "  line.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) in CMakeLists.txt:\n",
            "  cmake_minimum_required() should be called prior to this top-level project()\n",
            "  call.  Please see the cmake-commands(7) manual for usage documentation of\n",
            "  both commands.\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch7 in /root/torch/install\n",
            "\u001b[0mCMake Deprecation Warning at /root/torch/install/share/cmake/torch/FindCUDA.cmake:368 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "Call Stack (most recent call first):\n",
            "  CMakeLists.txt:7 (FIND_PACKAGE)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THC/CMakeLists.txt:1 (CMAKE_MINIMUM_REQUIRED):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Deprecation Warning at lib/THC/CMakeLists.txt:2 (CMAKE_POLICY):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Removing -DNDEBUG from compile flags\n",
            "-- TH_LIBRARIES: TH\n",
            "-- MAGMA not found. Compiling without MAGMA support\n",
            "-- Automatic GPU detection failed. Building for common architectures.\n",
            "-- Autodetected CUDA architecture(s): 3.0;3.5;5.0;5.2;6.0;6.1;6.1+PTX\n",
            "-- got cuda version 12.5\n",
            "-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n",
            "-- CUDA_NVCC_FLAGS: -gencode;arch=compute_30,code=sm_30;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_61,code=compute_61;-DCUDA_HAS_FP16=1\n",
            "-- THC_SO_VERSION: 0\n",
            "-- Configuring done (2.1s)\n",
            "\u001b[0mCMake Error: The following variables are used in this project, but they are set to NOTFOUND.\n",
            "Please set them or make sure they are set and tested correctly in the CMake files:\n",
            "CUDA_cublas_device_LIBRARY (ADVANCED)\n",
            "    linked by target \"THC\" in directory /root/torch/extra/cutorch/lib/THC\n",
            "\u001b[0m\n",
            "-- Generating done (0.0s)\n",
            "\u001b[0mCMake Generate step failed.  Build files cannot be regenerated correctly.\u001b[0m\n",
            "\n",
            "Error: Build error: Failed building.\n",
            "\u001b[0mTorch installed successfully\u001b[0m\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!luarocks --check-lua-versions install nngraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh3i-xtP6YGf",
        "outputId": "4b4e2c9d-a01d-4586-a770-ca7be04e24b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: luarocks [-h] [--version] [--dev] [--server <server>]\n",
            "       [--only-server <server>] [--only-sources <url>]\n",
            "       [--namespace <namespace>] [--lua-dir <prefix>]\n",
            "       [--lua-version <ver>] [--tree <tree>] [--local] [--global]\n",
            "       [--verbose] [--timeout <seconds>] [--pin] [<command>] ...\n",
            "\n",
            "Error: unknown option '--check-lua-versions'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2IqiQrEZGzs",
        "outputId": "032a8435-dc99-4bc0-d118-43a32ee55dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'char-rnn'...\n",
            "remote: Enumerating objects: 256, done.\u001b[K\n",
            "remote: Total 256 (delta 0), reused 0 (delta 0), pack-reused 256 (from 1)\u001b[K\n",
            "Receiving objects: 100% (256/256), 516.44 KiB | 8.33 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n",
            "/root/torch/char-rnn/char-rnn\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/char-rnn.git\n",
        "%cd char-rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfJZ_vxwZ5Uf",
        "outputId": "ed8f2b57-fbb4-434f-d57a-d47b6325d4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: /root/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th [options] <model>\n",
            "\n",
            "Sample from a character-level language model\n",
            "\n",
            "Options\n",
            "  <model>      model checkpoint to use for sampling\n",
            "  -seed        random number generator's seed [123]\n",
            "  -sample       0 to use max at each timestep, 1 to sample at each timestep [1]\n",
            "  -primetext   used as a prompt to \"seed\" the state of the LSTM using a given sequence, before we sample. []\n",
            "  -length      number of characters to sample [2000]\n",
            "  -temperature temperature of sampling [1]\n",
            "  -gpuid       which gpu to use. -1 = use CPU [0]\n",
            "  -opencl      use OpenCL (instead of CUDA) [0]\n",
            "  -verbose     set to 0 to ONLY print the sampled text, no diagnostics [1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!th sample.lua -help"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Epochs - Original RNN"
      ],
      "metadata": {
        "id": "SHKXETWwnuON"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Pli9g5bUWW",
        "collapsed": true,
        "outputId": "b1c4b5ea-9f9f-4905-9baa-dc013b2a3568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mvocab.t7 and data.t7 do not exist. Running preprocessing...\u001b[0m\t\n",
            "\u001b[0mone-time setup: preprocessing input text file data/tinyshakespeare/input.txt...\u001b[0m\t\n",
            "\u001b[0mloading text file...\u001b[0m\t\n",
            "\u001b[0mcreating vocabulary mapping...\u001b[0m\t\n",
            "\u001b[0mputting data into tensor...\u001b[0m\t\n",
            "\u001b[0msaving data/tinyshakespeare/vocab.t7\u001b[0m\t\n",
            "\u001b[0msaving data/tinyshakespeare/data.t7\u001b[0m\t\n",
            "\u001b[0mloading data files...\u001b[0m\t\n",
            "\u001b[0mcutting off end of data so that the batches/sequences divide evenly\u001b[0m\t\n",
            "\u001b[0mreshaping tensor...\u001b[0m\t\n",
            "\u001b[0mdata load done. Number of data batches in train: 423, val: 23, test: 0\u001b[0m\t\n",
            "\u001b[0mvocab size: 65\u001b[0m\t\n",
            "\u001b[0mcreating an lstm with 2 layers\u001b[0m\t\n",
            "\u001b[0msetting forget gate biases to 1 in LSTM layer 1\u001b[0m\t\n",
            "\u001b[0msetting forget gate biases to 1 in LSTM layer 2\u001b[0m\t\n",
            "\u001b[0mnumber of parameters in the model: 240321\u001b[0m\t\n",
            "\u001b[0mcloning rnn\u001b[0m\t\n",
            "\u001b[0mcloning criterion\u001b[0m\t\n",
            "\u001b[0m1/2115 (epoch 0.002), train_loss = 4.19803724, grad/param norm = 5.1721e-01, time/batch = 1.1066s\u001b[0m\t\n",
            "\u001b[0m2/2115 (epoch 0.005), train_loss = 3.93712096, grad/param norm = 1.4679e+00, time/batch = 0.4108s\u001b[0m\t\n",
            "\u001b[0m3/2115 (epoch 0.007), train_loss = 3.43751771, grad/param norm = 9.5793e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m4/2115 (epoch 0.009), train_loss = 3.41289302, grad/param norm = 7.5153e-01, time/batch = 0.4201s\u001b[0m\t\n",
            "\u001b[0m5/2115 (epoch 0.012), train_loss = 3.33699641, grad/param norm = 6.9269e-01, time/batch = 0.5391s\u001b[0m\t\n",
            "\u001b[0m6/2115 (epoch 0.014), train_loss = 3.37105608, grad/param norm = 5.2300e-01, time/batch = 0.9191s\u001b[0m\t\n",
            "\u001b[0m7/2115 (epoch 0.017), train_loss = 3.36710176, grad/param norm = 4.3214e-01, time/batch = 1.0096s\u001b[0m\t\n",
            "\u001b[0m8/2115 (epoch 0.019), train_loss = 3.33051405, grad/param norm = 3.9960e-01, time/batch = 0.8714s\u001b[0m\t\n",
            "\u001b[0m9/2115 (epoch 0.021), train_loss = 3.29338812, grad/param norm = 3.8692e-01, time/batch = 0.4829s\u001b[0m\t\n",
            "\u001b[0m10/2115 (epoch 0.024), train_loss = 3.38265345, grad/param norm = 3.5570e-01, time/batch = 0.4105s\u001b[0m\t\n",
            "\u001b[0m11/2115 (epoch 0.026), train_loss = 3.30180840, grad/param norm = 3.5802e-01, time/batch = 0.4248s\u001b[0m\t\n",
            "\u001b[0m12/2115 (epoch 0.028), train_loss = 3.32234028, grad/param norm = 2.7511e-01, time/batch = 0.4023s\u001b[0m\t\n",
            "\u001b[0m13/2115 (epoch 0.031), train_loss = 3.30897648, grad/param norm = 2.4441e-01, time/batch = 0.4273s\u001b[0m\t\n",
            "\u001b[0m14/2115 (epoch 0.033), train_loss = 3.28692222, grad/param norm = 3.4632e-01, time/batch = 0.4215s\u001b[0m\t\n",
            "\u001b[0m15/2115 (epoch 0.035), train_loss = 3.36003191, grad/param norm = 3.9644e-01, time/batch = 0.4026s\u001b[0m\t\n",
            "\u001b[0m16/2115 (epoch 0.038), train_loss = 3.33848421, grad/param norm = 3.4806e-01, time/batch = 0.4304s\u001b[0m\t\n",
            "\u001b[0m17/2115 (epoch 0.040), train_loss = 3.29889104, grad/param norm = 3.9853e-01, time/batch = 0.4002s\u001b[0m\t\n",
            "\u001b[0m18/2115 (epoch 0.043), train_loss = 3.31901480, grad/param norm = 2.5557e-01, time/batch = 0.4271s\u001b[0m\t\n",
            "\u001b[0m19/2115 (epoch 0.045), train_loss = 3.30151821, grad/param norm = 2.5695e-01, time/batch = 0.4103s\u001b[0m\t\n",
            "\u001b[0m20/2115 (epoch 0.047), train_loss = 3.27959467, grad/param norm = 3.9650e-01, time/batch = 0.4013s\u001b[0m\t\n",
            "\u001b[0m21/2115 (epoch 0.050), train_loss = 3.32289037, grad/param norm = 4.0551e-01, time/batch = 0.4341s\u001b[0m\t\n",
            "\u001b[0m22/2115 (epoch 0.052), train_loss = 3.34279904, grad/param norm = 4.2532e-01, time/batch = 0.4091s\u001b[0m\t\n",
            "\u001b[0m23/2115 (epoch 0.054), train_loss = 3.34371623, grad/param norm = 3.1156e-01, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m24/2115 (epoch 0.057), train_loss = 3.34361341, grad/param norm = 2.6665e-01, time/batch = 0.4028s\u001b[0m\t\n",
            "\u001b[0m25/2115 (epoch 0.059), train_loss = 3.38630147, grad/param norm = 2.8602e-01, time/batch = 0.4045s\u001b[0m\t\n",
            "\u001b[0m26/2115 (epoch 0.061), train_loss = 3.34342088, grad/param norm = 3.1997e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m27/2115 (epoch 0.064), train_loss = 3.29437431, grad/param norm = 3.1243e-01, time/batch = 0.4152s\u001b[0m\t\n",
            "\u001b[0m28/2115 (epoch 0.066), train_loss = 3.28385704, grad/param norm = 3.0503e-01, time/batch = 0.4272s\u001b[0m\t\n",
            "\u001b[0m29/2115 (epoch 0.069), train_loss = 3.27431357, grad/param norm = 2.9510e-01, time/batch = 0.3995s\u001b[0m\t\n",
            "\u001b[0m30/2115 (epoch 0.071), train_loss = 3.28938585, grad/param norm = 2.8415e-01, time/batch = 0.4012s\u001b[0m\t\n",
            "\u001b[0m31/2115 (epoch 0.073), train_loss = 3.33446110, grad/param norm = 3.1308e-01, time/batch = 0.4390s\u001b[0m\t\n",
            "\u001b[0m32/2115 (epoch 0.076), train_loss = 3.36726280, grad/param norm = 3.4115e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m33/2115 (epoch 0.078), train_loss = 3.29241376, grad/param norm = 3.8644e-01, time/batch = 0.8282s\u001b[0m\t\n",
            "\u001b[0m34/2115 (epoch 0.080), train_loss = 3.31882061, grad/param norm = 3.4167e-01, time/batch = 1.0625s\u001b[0m\t\n",
            "\u001b[0m35/2115 (epoch 0.083), train_loss = 3.30913373, grad/param norm = 2.8830e-01, time/batch = 0.8083s\u001b[0m\t\n",
            "\u001b[0m36/2115 (epoch 0.085), train_loss = 3.30367187, grad/param norm = 2.9479e-01, time/batch = 0.6571s\u001b[0m\t\n",
            "\u001b[0m37/2115 (epoch 0.087), train_loss = 3.30589620, grad/param norm = 3.3605e-01, time/batch = 0.4327s\u001b[0m\t\n",
            "\u001b[0m38/2115 (epoch 0.090), train_loss = 3.27835787, grad/param norm = 3.6931e-01, time/batch = 0.4080s\u001b[0m\t\n",
            "\u001b[0m39/2115 (epoch 0.092), train_loss = 3.33376212, grad/param norm = 4.5782e-01, time/batch = 0.4291s\u001b[0m\t\n",
            "\u001b[0m40/2115 (epoch 0.095), train_loss = 3.33414193, grad/param norm = 3.2741e-01, time/batch = 0.4101s\u001b[0m\t\n",
            "\u001b[0m41/2115 (epoch 0.097), train_loss = 3.33889606, grad/param norm = 2.2643e-01, time/batch = 0.4248s\u001b[0m\t\n",
            "\u001b[0m42/2115 (epoch 0.099), train_loss = 3.25347000, grad/param norm = 4.2083e-01, time/batch = 0.4224s\u001b[0m\t\n",
            "\u001b[0m43/2115 (epoch 0.102), train_loss = 3.30742461, grad/param norm = 7.4355e-01, time/batch = 0.4142s\u001b[0m\t\n",
            "\u001b[0m44/2115 (epoch 0.104), train_loss = 3.25784315, grad/param norm = 4.9779e-01, time/batch = 0.4305s\u001b[0m\t\n",
            "\u001b[0m45/2115 (epoch 0.106), train_loss = 3.33075874, grad/param norm = 3.2093e-01, time/batch = 0.4139s\u001b[0m\t\n",
            "\u001b[0m46/2115 (epoch 0.109), train_loss = 3.30681468, grad/param norm = 2.6679e-01, time/batch = 0.4087s\u001b[0m\t\n",
            "\u001b[0m47/2115 (epoch 0.111), train_loss = 3.32420680, grad/param norm = 2.7067e-01, time/batch = 0.4265s\u001b[0m\t\n",
            "\u001b[0m48/2115 (epoch 0.113), train_loss = 3.36881499, grad/param norm = 2.6440e-01, time/batch = 0.4151s\u001b[0m\t\n",
            "\u001b[0m49/2115 (epoch 0.116), train_loss = 3.30183175, grad/param norm = 3.3810e-01, time/batch = 0.4294s\u001b[0m\t\n",
            "\u001b[0m50/2115 (epoch 0.118), train_loss = 3.37098402, grad/param norm = 5.4100e-01, time/batch = 0.4147s\u001b[0m\t\n",
            "\u001b[0m51/2115 (epoch 0.121), train_loss = 3.34450459, grad/param norm = 5.0587e-01, time/batch = 0.4375s\u001b[0m\t\n",
            "\u001b[0m52/2115 (epoch 0.123), train_loss = 3.30177772, grad/param norm = 3.2604e-01, time/batch = 0.4048s\u001b[0m\t\n",
            "\u001b[0m53/2115 (epoch 0.125), train_loss = 3.33516332, grad/param norm = 2.9571e-01, time/batch = 0.4049s\u001b[0m\t\n",
            "\u001b[0m54/2115 (epoch 0.128), train_loss = 3.28163103, grad/param norm = 1.9853e-01, time/batch = 0.4131s\u001b[0m\t\n",
            "\u001b[0m55/2115 (epoch 0.130), train_loss = 3.29128247, grad/param norm = 2.8884e-01, time/batch = 0.4039s\u001b[0m\t\n",
            "\u001b[0m56/2115 (epoch 0.132), train_loss = 3.34731815, grad/param norm = 5.6597e-01, time/batch = 0.4231s\u001b[0m\t\n",
            "\u001b[0m57/2115 (epoch 0.135), train_loss = 3.31382047, grad/param norm = 7.9505e-01, time/batch = 0.4368s\u001b[0m\t\n",
            "\u001b[0m58/2115 (epoch 0.137), train_loss = 3.27099894, grad/param norm = 5.3345e-01, time/batch = 0.4105s\u001b[0m\t\n",
            "\u001b[0m59/2115 (epoch 0.139), train_loss = 3.27964154, grad/param norm = 2.9955e-01, time/batch = 0.4317s\u001b[0m\t\n",
            "\u001b[0m60/2115 (epoch 0.142), train_loss = 3.28700893, grad/param norm = 2.5103e-01, time/batch = 0.7516s\u001b[0m\t\n",
            "\u001b[0m61/2115 (epoch 0.144), train_loss = 3.28298147, grad/param norm = 3.1690e-01, time/batch = 0.9922s\u001b[0m\t\n",
            "\u001b[0m62/2115 (epoch 0.147), train_loss = 3.33454827, grad/param norm = 4.8176e-01, time/batch = 0.8619s\u001b[0m\t\n",
            "\u001b[0m63/2115 (epoch 0.149), train_loss = 3.42687476, grad/param norm = 1.5867e+00, time/batch = 0.7309s\u001b[0m\t\n",
            "\u001b[0m64/2115 (epoch 0.151), train_loss = 3.34094024, grad/param norm = 5.0750e-01, time/batch = 0.4059s\u001b[0m\t\n",
            "\u001b[0m65/2115 (epoch 0.154), train_loss = 3.33734596, grad/param norm = 2.8535e-01, time/batch = 0.4421s\u001b[0m\t\n",
            "\u001b[0m66/2115 (epoch 0.156), train_loss = 3.22285970, grad/param norm = 2.9740e-01, time/batch = 0.4042s\u001b[0m\t\n",
            "\u001b[0m67/2115 (epoch 0.158), train_loss = 3.25642427, grad/param norm = 2.0619e-01, time/batch = 0.4377s\u001b[0m\t\n",
            "\u001b[0m68/2115 (epoch 0.161), train_loss = 3.24217004, grad/param norm = 2.6215e-01, time/batch = 0.4631s\u001b[0m\t\n",
            "\u001b[0m69/2115 (epoch 0.163), train_loss = 3.24995863, grad/param norm = 4.1906e-01, time/batch = 0.5059s\u001b[0m\t\n",
            "\u001b[0m70/2115 (epoch 0.165), train_loss = 3.24613273, grad/param norm = 5.2524e-01, time/batch = 0.4183s\u001b[0m\t\n",
            "\u001b[0m71/2115 (epoch 0.168), train_loss = 3.23701126, grad/param norm = 4.3555e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m72/2115 (epoch 0.170), train_loss = 3.24640413, grad/param norm = 4.8270e-01, time/batch = 0.4306s\u001b[0m\t\n",
            "\u001b[0m73/2115 (epoch 0.173), train_loss = 3.20488590, grad/param norm = 6.6542e-01, time/batch = 0.4162s\u001b[0m\t\n",
            "\u001b[0m74/2115 (epoch 0.175), train_loss = 3.24233486, grad/param norm = 1.2427e+00, time/batch = 0.4462s\u001b[0m\t\n",
            "\u001b[0m75/2115 (epoch 0.177), train_loss = 3.34454558, grad/param norm = 8.1842e-01, time/batch = 0.4064s\u001b[0m\t\n",
            "\u001b[0m76/2115 (epoch 0.180), train_loss = 3.19909525, grad/param norm = 3.8137e-01, time/batch = 0.4460s\u001b[0m\t\n",
            "\u001b[0m77/2115 (epoch 0.182), train_loss = 3.18182995, grad/param norm = 2.7819e-01, time/batch = 0.4273s\u001b[0m\t\n",
            "\u001b[0m78/2115 (epoch 0.184), train_loss = 3.18737724, grad/param norm = 1.8746e-01, time/batch = 0.4125s\u001b[0m\t\n",
            "\u001b[0m79/2115 (epoch 0.187), train_loss = 3.20169701, grad/param norm = 2.9521e-01, time/batch = 0.4391s\u001b[0m\t\n",
            "\u001b[0m80/2115 (epoch 0.189), train_loss = 3.14496863, grad/param norm = 3.0773e-01, time/batch = 0.4041s\u001b[0m\t\n",
            "\u001b[0m81/2115 (epoch 0.191), train_loss = 3.19034137, grad/param norm = 5.2855e-01, time/batch = 0.4463s\u001b[0m\t\n",
            "\u001b[0m82/2115 (epoch 0.194), train_loss = 3.20739712, grad/param norm = 1.2107e+00, time/batch = 0.4185s\u001b[0m\t\n",
            "\u001b[0m83/2115 (epoch 0.196), train_loss = 3.31247081, grad/param norm = 1.2504e+00, time/batch = 0.4190s\u001b[0m\t\n",
            "\u001b[0m84/2115 (epoch 0.199), train_loss = 3.19198682, grad/param norm = 7.0903e-01, time/batch = 0.4320s\u001b[0m\t\n",
            "\u001b[0m85/2115 (epoch 0.201), train_loss = 3.20656701, grad/param norm = 7.0288e-01, time/batch = 0.4111s\u001b[0m\t\n",
            "\u001b[0m86/2115 (epoch 0.203), train_loss = 3.15608981, grad/param norm = 6.4501e-01, time/batch = 0.5635s\u001b[0m\t\n",
            "\u001b[0m87/2115 (epoch 0.206), train_loss = 3.14339566, grad/param norm = 5.2868e-01, time/batch = 0.9123s\u001b[0m\t\n",
            "\u001b[0m88/2115 (epoch 0.208), train_loss = 3.09768087, grad/param norm = 4.2123e-01, time/batch = 0.8969s\u001b[0m\t\n",
            "\u001b[0m89/2115 (epoch 0.210), train_loss = 3.16539554, grad/param norm = 4.4588e-01, time/batch = 0.8949s\u001b[0m\t\n",
            "\u001b[0m90/2115 (epoch 0.213), train_loss = 3.09862011, grad/param norm = 5.2295e-01, time/batch = 0.5251s\u001b[0m\t\n",
            "\u001b[0m91/2115 (epoch 0.215), train_loss = 3.09048213, grad/param norm = 5.8459e-01, time/batch = 0.4315s\u001b[0m\t\n",
            "\u001b[0m92/2115 (epoch 0.217), train_loss = 3.07785415, grad/param norm = 6.8411e-01, time/batch = 0.4226s\u001b[0m\t\n",
            "\u001b[0m93/2115 (epoch 0.220), train_loss = 3.08527808, grad/param norm = 8.9168e-01, time/batch = 0.4129s\u001b[0m\t\n",
            "\u001b[0m94/2115 (epoch 0.222), train_loss = 3.09793075, grad/param norm = 8.6754e-01, time/batch = 0.4385s\u001b[0m\t\n",
            "\u001b[0m95/2115 (epoch 0.225), train_loss = 3.13378253, grad/param norm = 7.8541e-01, time/batch = 0.4107s\u001b[0m\t\n",
            "\u001b[0m96/2115 (epoch 0.227), train_loss = 3.10899339, grad/param norm = 9.9986e-01, time/batch = 0.4207s\u001b[0m\t\n",
            "\u001b[0m97/2115 (epoch 0.229), train_loss = 3.21832354, grad/param norm = 2.0254e+00, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m98/2115 (epoch 0.232), train_loss = 3.18827514, grad/param norm = 1.1597e+00, time/batch = 0.4151s\u001b[0m\t\n",
            "\u001b[0m99/2115 (epoch 0.234), train_loss = 3.09285689, grad/param norm = 3.7583e-01, time/batch = 0.4270s\u001b[0m\t\n",
            "\u001b[0m100/2115 (epoch 0.236), train_loss = 3.07821847, grad/param norm = 3.8043e-01, time/batch = 0.4083s\u001b[0m\t\n",
            "\u001b[0m101/2115 (epoch 0.239), train_loss = 3.05044595, grad/param norm = 5.4262e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m102/2115 (epoch 0.241), train_loss = 3.02238950, grad/param norm = 7.1935e-01, time/batch = 0.4479s\u001b[0m\t\n",
            "\u001b[0m103/2115 (epoch 0.243), train_loss = 3.07112218, grad/param norm = 8.4398e-01, time/batch = 0.4089s\u001b[0m\t\n",
            "\u001b[0m104/2115 (epoch 0.246), train_loss = 3.02526398, grad/param norm = 7.6002e-01, time/batch = 0.4402s\u001b[0m\t\n",
            "\u001b[0m105/2115 (epoch 0.248), train_loss = 2.98375756, grad/param norm = 6.6248e-01, time/batch = 0.4171s\u001b[0m\t\n",
            "\u001b[0m106/2115 (epoch 0.251), train_loss = 2.99396174, grad/param norm = 7.9373e-01, time/batch = 0.4058s\u001b[0m\t\n",
            "\u001b[0m107/2115 (epoch 0.253), train_loss = 3.01772288, grad/param norm = 8.3220e-01, time/batch = 0.4469s\u001b[0m\t\n",
            "\u001b[0m108/2115 (epoch 0.255), train_loss = 3.00508326, grad/param norm = 8.5894e-01, time/batch = 0.4162s\u001b[0m\t\n",
            "\u001b[0m109/2115 (epoch 0.258), train_loss = 2.96773194, grad/param norm = 7.7674e-01, time/batch = 0.4384s\u001b[0m\t\n",
            "\u001b[0m110/2115 (epoch 0.260), train_loss = 2.97045362, grad/param norm = 6.8385e-01, time/batch = 0.4144s\u001b[0m\t\n",
            "\u001b[0m111/2115 (epoch 0.262), train_loss = 2.94274945, grad/param norm = 6.2305e-01, time/batch = 0.4353s\u001b[0m\t\n",
            "\u001b[0m112/2115 (epoch 0.265), train_loss = 2.95670991, grad/param norm = 6.3522e-01, time/batch = 0.4238s\u001b[0m\t\n",
            "\u001b[0m113/2115 (epoch 0.267), train_loss = 2.95489105, grad/param norm = 7.0376e-01, time/batch = 0.7025s\u001b[0m\t\n",
            "\u001b[0m114/2115 (epoch 0.270), train_loss = 2.92510647, grad/param norm = 8.6010e-01, time/batch = 0.8954s\u001b[0m\t\n",
            "\u001b[0m115/2115 (epoch 0.272), train_loss = 2.95843108, grad/param norm = 9.2571e-01, time/batch = 0.8888s\u001b[0m\t\n",
            "\u001b[0m116/2115 (epoch 0.274), train_loss = 2.94344467, grad/param norm = 8.4551e-01, time/batch = 0.8227s\u001b[0m\t\n",
            "\u001b[0m117/2115 (epoch 0.277), train_loss = 2.94940864, grad/param norm = 7.5695e-01, time/batch = 0.5022s\u001b[0m\t\n",
            "\u001b[0m118/2115 (epoch 0.279), train_loss = 2.90698912, grad/param norm = 6.4347e-01, time/batch = 0.4206s\u001b[0m\t\n",
            "\u001b[0m119/2115 (epoch 0.281), train_loss = 2.85019348, grad/param norm = 6.3013e-01, time/batch = 0.4225s\u001b[0m\t\n",
            "\u001b[0m120/2115 (epoch 0.284), train_loss = 2.86486400, grad/param norm = 7.4161e-01, time/batch = 0.4452s\u001b[0m\t\n",
            "\u001b[0m121/2115 (epoch 0.286), train_loss = 2.86584311, grad/param norm = 8.7977e-01, time/batch = 0.4252s\u001b[0m\t\n",
            "\u001b[0m122/2115 (epoch 0.288), train_loss = 2.85840167, grad/param norm = 1.0635e+00, time/batch = 0.4368s\u001b[0m\t\n",
            "\u001b[0m123/2115 (epoch 0.291), train_loss = 2.89634527, grad/param norm = 1.0997e+00, time/batch = 0.4078s\u001b[0m\t\n",
            "\u001b[0m124/2115 (epoch 0.293), train_loss = 2.87399084, grad/param norm = 9.6137e-01, time/batch = 0.4408s\u001b[0m\t\n",
            "\u001b[0m125/2115 (epoch 0.296), train_loss = 2.84461099, grad/param norm = 7.5780e-01, time/batch = 0.4113s\u001b[0m\t\n",
            "\u001b[0m126/2115 (epoch 0.298), train_loss = 2.88319550, grad/param norm = 5.7880e-01, time/batch = 0.4235s\u001b[0m\t\n",
            "\u001b[0m127/2115 (epoch 0.300), train_loss = 2.83790894, grad/param norm = 4.0446e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m128/2115 (epoch 0.303), train_loss = 2.80132493, grad/param norm = 3.4536e-01, time/batch = 0.4137s\u001b[0m\t\n",
            "\u001b[0m129/2115 (epoch 0.305), train_loss = 2.83104764, grad/param norm = 4.1891e-01, time/batch = 0.4421s\u001b[0m\t\n",
            "\u001b[0m130/2115 (epoch 0.307), train_loss = 2.76925650, grad/param norm = 5.1282e-01, time/batch = 0.4346s\u001b[0m\t\n",
            "\u001b[0m131/2115 (epoch 0.310), train_loss = 2.82329223, grad/param norm = 8.2544e-01, time/batch = 0.4623s\u001b[0m\t\n",
            "\u001b[0m132/2115 (epoch 0.312), train_loss = 2.85139082, grad/param norm = 1.5271e+00, time/batch = 0.4117s\u001b[0m\t\n",
            "\u001b[0m133/2115 (epoch 0.314), train_loss = 2.92493309, grad/param norm = 1.6126e+00, time/batch = 0.4682s\u001b[0m\t\n",
            "\u001b[0m134/2115 (epoch 0.317), train_loss = 2.81723267, grad/param norm = 8.1515e-01, time/batch = 0.4410s\u001b[0m\t\n",
            "\u001b[0m135/2115 (epoch 0.319), train_loss = 2.74463577, grad/param norm = 5.8192e-01, time/batch = 0.4106s\u001b[0m\t\n",
            "\u001b[0m136/2115 (epoch 0.322), train_loss = 2.79113091, grad/param norm = 6.8544e-01, time/batch = 0.4324s\u001b[0m\t\n",
            "\u001b[0m137/2115 (epoch 0.324), train_loss = 2.80762039, grad/param norm = 9.9190e-01, time/batch = 0.4119s\u001b[0m\t\n",
            "\u001b[0m138/2115 (epoch 0.326), train_loss = 2.84752823, grad/param norm = 1.2154e+00, time/batch = 0.4394s\u001b[0m\t\n",
            "\u001b[0m139/2115 (epoch 0.329), train_loss = 2.78988560, grad/param norm = 8.3604e-01, time/batch = 0.4270s\u001b[0m\t\n",
            "\u001b[0m140/2115 (epoch 0.331), train_loss = 2.82422757, grad/param norm = 6.5683e-01, time/batch = 0.7046s\u001b[0m\t\n",
            "\u001b[0m141/2115 (epoch 0.333), train_loss = 2.77647984, grad/param norm = 5.9816e-01, time/batch = 0.8235s\u001b[0m\t\n",
            "\u001b[0m142/2115 (epoch 0.336), train_loss = 2.76551834, grad/param norm = 4.9630e-01, time/batch = 0.9063s\u001b[0m\t\n",
            "\u001b[0m143/2115 (epoch 0.338), train_loss = 2.68925351, grad/param norm = 4.4483e-01, time/batch = 0.8611s\u001b[0m\t\n",
            "\u001b[0m144/2115 (epoch 0.340), train_loss = 2.68725056, grad/param norm = 5.0040e-01, time/batch = 0.4152s\u001b[0m\t\n",
            "\u001b[0m145/2115 (epoch 0.343), train_loss = 2.72938514, grad/param norm = 7.9617e-01, time/batch = 0.4299s\u001b[0m\t\n",
            "\u001b[0m146/2115 (epoch 0.345), train_loss = 2.75873896, grad/param norm = 1.3228e+00, time/batch = 0.4166s\u001b[0m\t\n",
            "\u001b[0m147/2115 (epoch 0.348), train_loss = 2.77007693, grad/param norm = 1.1513e+00, time/batch = 0.4309s\u001b[0m\t\n",
            "\u001b[0m148/2115 (epoch 0.350), train_loss = 2.69847014, grad/param norm = 6.1959e-01, time/batch = 0.4228s\u001b[0m\t\n",
            "\u001b[0m149/2115 (epoch 0.352), train_loss = 2.74627412, grad/param norm = 6.2276e-01, time/batch = 0.4166s\u001b[0m\t\n",
            "\u001b[0m150/2115 (epoch 0.355), train_loss = 2.73010684, grad/param norm = 8.8102e-01, time/batch = 0.4389s\u001b[0m\t\n",
            "\u001b[0m151/2115 (epoch 0.357), train_loss = 2.77412128, grad/param norm = 1.1289e+00, time/batch = 0.4253s\u001b[0m\t\n",
            "\u001b[0m152/2115 (epoch 0.359), train_loss = 2.75830829, grad/param norm = 1.0266e+00, time/batch = 0.4498s\u001b[0m\t\n",
            "\u001b[0m153/2115 (epoch 0.362), train_loss = 2.66565938, grad/param norm = 5.1061e-01, time/batch = 0.4320s\u001b[0m\t\n",
            "\u001b[0m154/2115 (epoch 0.364), train_loss = 2.68766305, grad/param norm = 3.4005e-01, time/batch = 0.4328s\u001b[0m\t\n",
            "\u001b[0m155/2115 (epoch 0.366), train_loss = 2.66022934, grad/param norm = 3.7945e-01, time/batch = 0.4229s\u001b[0m\t\n",
            "\u001b[0m156/2115 (epoch 0.369), train_loss = 2.63620778, grad/param norm = 5.7091e-01, time/batch = 0.4120s\u001b[0m\t\n",
            "\u001b[0m157/2115 (epoch 0.371), train_loss = 2.64676701, grad/param norm = 8.7986e-01, time/batch = 0.4423s\u001b[0m\t\n",
            "\u001b[0m158/2115 (epoch 0.374), train_loss = 2.71109694, grad/param norm = 1.1010e+00, time/batch = 0.4269s\u001b[0m\t\n",
            "\u001b[0m159/2115 (epoch 0.376), train_loss = 2.69891883, grad/param norm = 1.1508e+00, time/batch = 0.4396s\u001b[0m\t\n",
            "\u001b[0m160/2115 (epoch 0.378), train_loss = 2.66954103, grad/param norm = 9.0391e-01, time/batch = 0.4234s\u001b[0m\t\n",
            "\u001b[0m161/2115 (epoch 0.381), train_loss = 2.63207934, grad/param norm = 5.3107e-01, time/batch = 0.4476s\u001b[0m\t\n",
            "\u001b[0m162/2115 (epoch 0.383), train_loss = 2.67103096, grad/param norm = 4.0328e-01, time/batch = 0.4237s\u001b[0m\t\n",
            "\u001b[0m163/2115 (epoch 0.385), train_loss = 2.63662891, grad/param norm = 5.5740e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m164/2115 (epoch 0.388), train_loss = 2.65052373, grad/param norm = 7.5369e-01, time/batch = 0.4485s\u001b[0m\t\n",
            "\u001b[0m165/2115 (epoch 0.390), train_loss = 2.68986122, grad/param norm = 8.9264e-01, time/batch = 0.4255s\u001b[0m\t\n",
            "\u001b[0m166/2115 (epoch 0.392), train_loss = 2.67473663, grad/param norm = 7.1320e-01, time/batch = 0.5485s\u001b[0m\t\n",
            "\u001b[0m167/2115 (epoch 0.395), train_loss = 2.62141534, grad/param norm = 4.2139e-01, time/batch = 0.9426s\u001b[0m\t\n",
            "\u001b[0m168/2115 (epoch 0.397), train_loss = 2.58207356, grad/param norm = 3.0503e-01, time/batch = 1.0471s\u001b[0m\t\n",
            "\u001b[0m169/2115 (epoch 0.400), train_loss = 2.58858939, grad/param norm = 2.2839e-01, time/batch = 0.9109s\u001b[0m\t\n",
            "\u001b[0m170/2115 (epoch 0.402), train_loss = 2.60815551, grad/param norm = 4.1948e-01, time/batch = 0.4965s\u001b[0m\t\n",
            "\u001b[0m171/2115 (epoch 0.404), train_loss = 2.65351911, grad/param norm = 1.0016e+00, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m172/2115 (epoch 0.407), train_loss = 2.69237709, grad/param norm = 1.5472e+00, time/batch = 0.4474s\u001b[0m\t\n",
            "\u001b[0m173/2115 (epoch 0.409), train_loss = 2.68104403, grad/param norm = 1.2879e+00, time/batch = 0.4113s\u001b[0m\t\n",
            "\u001b[0m174/2115 (epoch 0.411), train_loss = 2.60996963, grad/param norm = 7.0777e-01, time/batch = 0.4356s\u001b[0m\t\n",
            "\u001b[0m175/2115 (epoch 0.414), train_loss = 2.60246983, grad/param norm = 5.6602e-01, time/batch = 0.4118s\u001b[0m\t\n",
            "\u001b[0m176/2115 (epoch 0.416), train_loss = 2.64202388, grad/param norm = 4.7797e-01, time/batch = 0.4327s\u001b[0m\t\n",
            "\u001b[0m177/2115 (epoch 0.418), train_loss = 2.62718219, grad/param norm = 5.0849e-01, time/batch = 0.4179s\u001b[0m\t\n",
            "\u001b[0m178/2115 (epoch 0.421), train_loss = 2.60335444, grad/param norm = 5.5009e-01, time/batch = 0.4135s\u001b[0m\t\n",
            "\u001b[0m179/2115 (epoch 0.423), train_loss = 2.58967731, grad/param norm = 4.4932e-01, time/batch = 0.4303s\u001b[0m\t\n",
            "\u001b[0m180/2115 (epoch 0.426), train_loss = 2.52230660, grad/param norm = 4.6318e-01, time/batch = 0.4115s\u001b[0m\t\n",
            "\u001b[0m181/2115 (epoch 0.428), train_loss = 2.55771969, grad/param norm = 5.0155e-01, time/batch = 0.4378s\u001b[0m\t\n",
            "\u001b[0m182/2115 (epoch 0.430), train_loss = 2.63247526, grad/param norm = 6.0524e-01, time/batch = 0.4159s\u001b[0m\t\n",
            "\u001b[0m183/2115 (epoch 0.433), train_loss = 2.58423691, grad/param norm = 7.4522e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m184/2115 (epoch 0.435), train_loss = 2.59356101, grad/param norm = 7.1275e-01, time/batch = 0.4263s\u001b[0m\t\n",
            "\u001b[0m185/2115 (epoch 0.437), train_loss = 2.51092578, grad/param norm = 6.2481e-01, time/batch = 0.4177s\u001b[0m\t\n",
            "\u001b[0m186/2115 (epoch 0.440), train_loss = 2.58193746, grad/param norm = 7.4365e-01, time/batch = 0.4373s\u001b[0m\t\n",
            "\u001b[0m187/2115 (epoch 0.442), train_loss = 2.57647353, grad/param norm = 9.4985e-01, time/batch = 0.4126s\u001b[0m\t\n",
            "\u001b[0m188/2115 (epoch 0.444), train_loss = 2.61907351, grad/param norm = 1.1592e+00, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m189/2115 (epoch 0.447), train_loss = 2.59179933, grad/param norm = 8.7548e-01, time/batch = 0.4320s\u001b[0m\t\n",
            "\u001b[0m190/2115 (epoch 0.449), train_loss = 2.48201547, grad/param norm = 4.6463e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m191/2115 (epoch 0.452), train_loss = 2.50807268, grad/param norm = 3.1036e-01, time/batch = 0.4747s\u001b[0m\t\n",
            "\u001b[0m192/2115 (epoch 0.454), train_loss = 2.47010914, grad/param norm = 3.2614e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m193/2115 (epoch 0.456), train_loss = 2.51975330, grad/param norm = 6.0554e-01, time/batch = 0.6585s\u001b[0m\t\n",
            "\u001b[0m194/2115 (epoch 0.459), train_loss = 2.54416084, grad/param norm = 1.0617e+00, time/batch = 0.8530s\u001b[0m\t\n",
            "\u001b[0m195/2115 (epoch 0.461), train_loss = 2.60128508, grad/param norm = 1.2147e+00, time/batch = 1.0286s\u001b[0m\t\n",
            "\u001b[0m196/2115 (epoch 0.463), train_loss = 2.54807519, grad/param norm = 8.8373e-01, time/batch = 0.8925s\u001b[0m\t\n",
            "\u001b[0m197/2115 (epoch 0.466), train_loss = 2.49487902, grad/param norm = 4.4694e-01, time/batch = 0.4432s\u001b[0m\t\n",
            "\u001b[0m198/2115 (epoch 0.468), train_loss = 2.50648543, grad/param norm = 2.3355e-01, time/batch = 0.4250s\u001b[0m\t\n",
            "\u001b[0m199/2115 (epoch 0.470), train_loss = 2.52043138, grad/param norm = 2.2580e-01, time/batch = 0.4348s\u001b[0m\t\n",
            "\u001b[0m200/2115 (epoch 0.473), train_loss = 2.47241909, grad/param norm = 2.3136e-01, time/batch = 0.4207s\u001b[0m\t\n",
            "\u001b[0m201/2115 (epoch 0.475), train_loss = 2.52041473, grad/param norm = 2.4116e-01, time/batch = 0.4443s\u001b[0m\t\n",
            "\u001b[0m202/2115 (epoch 0.478), train_loss = 2.49676957, grad/param norm = 4.3180e-01, time/batch = 0.4268s\u001b[0m\t\n",
            "\u001b[0m203/2115 (epoch 0.480), train_loss = 2.53029762, grad/param norm = 1.1121e+00, time/batch = 0.4043s\u001b[0m\t\n",
            "\u001b[0m204/2115 (epoch 0.482), train_loss = 2.63400228, grad/param norm = 1.3649e+00, time/batch = 0.4299s\u001b[0m\t\n",
            "\u001b[0m205/2115 (epoch 0.485), train_loss = 2.52307777, grad/param norm = 6.8879e-01, time/batch = 0.4159s\u001b[0m\t\n",
            "\u001b[0m206/2115 (epoch 0.487), train_loss = 2.52149580, grad/param norm = 3.2416e-01, time/batch = 0.4370s\u001b[0m\t\n",
            "\u001b[0m207/2115 (epoch 0.489), train_loss = 2.46141103, grad/param norm = 2.7036e-01, time/batch = 0.4154s\u001b[0m\t\n",
            "\u001b[0m208/2115 (epoch 0.492), train_loss = 2.47730609, grad/param norm = 3.0304e-01, time/batch = 0.4085s\u001b[0m\t\n",
            "\u001b[0m209/2115 (epoch 0.494), train_loss = 2.54728997, grad/param norm = 3.8410e-01, time/batch = 0.4334s\u001b[0m\t\n",
            "\u001b[0m210/2115 (epoch 0.496), train_loss = 2.46726351, grad/param norm = 4.4349e-01, time/batch = 0.4132s\u001b[0m\t\n",
            "\u001b[0m211/2115 (epoch 0.499), train_loss = 2.48739814, grad/param norm = 4.9187e-01, time/batch = 0.4502s\u001b[0m\t\n",
            "\u001b[0m212/2115 (epoch 0.501), train_loss = 2.48722618, grad/param norm = 5.4036e-01, time/batch = 0.4204s\u001b[0m\t\n",
            "\u001b[0m213/2115 (epoch 0.504), train_loss = 2.51023675, grad/param norm = 7.0908e-01, time/batch = 0.4442s\u001b[0m\t\n",
            "\u001b[0m214/2115 (epoch 0.506), train_loss = 2.52300935, grad/param norm = 7.7984e-01, time/batch = 0.4275s\u001b[0m\t\n",
            "\u001b[0m215/2115 (epoch 0.508), train_loss = 2.45070952, grad/param norm = 6.6522e-01, time/batch = 0.4061s\u001b[0m\t\n",
            "\u001b[0m216/2115 (epoch 0.511), train_loss = 2.45177208, grad/param norm = 7.2316e-01, time/batch = 0.4402s\u001b[0m\t\n",
            "\u001b[0m217/2115 (epoch 0.513), train_loss = 2.51017703, grad/param norm = 7.4379e-01, time/batch = 0.4135s\u001b[0m\t\n",
            "\u001b[0m218/2115 (epoch 0.515), train_loss = 2.46447165, grad/param norm = 6.8818e-01, time/batch = 0.4336s\u001b[0m\t\n",
            "\u001b[0m219/2115 (epoch 0.518), train_loss = 2.46589481, grad/param norm = 7.1411e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m220/2115 (epoch 0.520), train_loss = 2.48342700, grad/param norm = 8.2284e-01, time/batch = 0.7717s\u001b[0m\t\n",
            "\u001b[0m221/2115 (epoch 0.522), train_loss = 2.47841560, grad/param norm = 8.6031e-01, time/batch = 0.9188s\u001b[0m\t\n",
            "\u001b[0m222/2115 (epoch 0.525), train_loss = 2.48863659, grad/param norm = 6.4867e-01, time/batch = 0.8872s\u001b[0m\t\n",
            "\u001b[0m223/2115 (epoch 0.527), train_loss = 2.42677319, grad/param norm = 4.1003e-01, time/batch = 0.7265s\u001b[0m\t\n",
            "\u001b[0m224/2115 (epoch 0.530), train_loss = 2.41304941, grad/param norm = 3.2775e-01, time/batch = 0.4373s\u001b[0m\t\n",
            "\u001b[0m225/2115 (epoch 0.532), train_loss = 2.40809192, grad/param norm = 3.7112e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m226/2115 (epoch 0.534), train_loss = 2.40047575, grad/param norm = 4.7976e-01, time/batch = 0.4314s\u001b[0m\t\n",
            "\u001b[0m227/2115 (epoch 0.537), train_loss = 2.49626921, grad/param norm = 5.0982e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m228/2115 (epoch 0.539), train_loss = 2.43752281, grad/param norm = 6.0529e-01, time/batch = 0.4134s\u001b[0m\t\n",
            "\u001b[0m229/2115 (epoch 0.541), train_loss = 2.47206719, grad/param norm = 6.6713e-01, time/batch = 0.4362s\u001b[0m\t\n",
            "\u001b[0m230/2115 (epoch 0.544), train_loss = 2.41449730, grad/param norm = 6.3031e-01, time/batch = 0.4156s\u001b[0m\t\n",
            "\u001b[0m231/2115 (epoch 0.546), train_loss = 2.47015348, grad/param norm = 7.0162e-01, time/batch = 0.4411s\u001b[0m\t\n",
            "\u001b[0m232/2115 (epoch 0.548), train_loss = 2.49340569, grad/param norm = 9.5649e-01, time/batch = 0.4115s\u001b[0m\t\n",
            "\u001b[0m233/2115 (epoch 0.551), train_loss = 2.48413489, grad/param norm = 9.8097e-01, time/batch = 0.4169s\u001b[0m\t\n",
            "\u001b[0m234/2115 (epoch 0.553), train_loss = 2.47696726, grad/param norm = 7.6156e-01, time/batch = 0.4352s\u001b[0m\t\n",
            "\u001b[0m235/2115 (epoch 0.556), train_loss = 2.42003128, grad/param norm = 4.7637e-01, time/batch = 0.4069s\u001b[0m\t\n",
            "\u001b[0m236/2115 (epoch 0.558), train_loss = 2.43784100, grad/param norm = 3.6415e-01, time/batch = 0.4429s\u001b[0m\t\n",
            "\u001b[0m237/2115 (epoch 0.560), train_loss = 2.41537521, grad/param norm = 3.0975e-01, time/batch = 0.4156s\u001b[0m\t\n",
            "\u001b[0m238/2115 (epoch 0.563), train_loss = 2.40617776, grad/param norm = 3.1120e-01, time/batch = 0.4125s\u001b[0m\t\n",
            "\u001b[0m239/2115 (epoch 0.565), train_loss = 2.39517289, grad/param norm = 3.6432e-01, time/batch = 0.4244s\u001b[0m\t\n",
            "\u001b[0m240/2115 (epoch 0.567), train_loss = 2.37927314, grad/param norm = 3.3272e-01, time/batch = 0.4158s\u001b[0m\t\n",
            "\u001b[0m241/2115 (epoch 0.570), train_loss = 2.38933547, grad/param norm = 4.0708e-01, time/batch = 0.4398s\u001b[0m\t\n",
            "\u001b[0m242/2115 (epoch 0.572), train_loss = 2.42222538, grad/param norm = 4.3470e-01, time/batch = 0.4132s\u001b[0m\t\n",
            "\u001b[0m243/2115 (epoch 0.574), train_loss = 2.38392840, grad/param norm = 4.4302e-01, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m244/2115 (epoch 0.577), train_loss = 2.40528393, grad/param norm = 6.7648e-01, time/batch = 0.4324s\u001b[0m\t\n",
            "\u001b[0m245/2115 (epoch 0.579), train_loss = 2.43108681, grad/param norm = 9.2463e-01, time/batch = 0.4183s\u001b[0m\t\n",
            "\u001b[0m246/2115 (epoch 0.582), train_loss = 2.42729577, grad/param norm = 9.5336e-01, time/batch = 0.4508s\u001b[0m\t\n",
            "\u001b[0m247/2115 (epoch 0.584), train_loss = 2.42606281, grad/param norm = 7.8303e-01, time/batch = 0.8519s\u001b[0m\t\n",
            "\u001b[0m248/2115 (epoch 0.586), train_loss = 2.40168978, grad/param norm = 5.2941e-01, time/batch = 0.9680s\u001b[0m\t\n",
            "\u001b[0m249/2115 (epoch 0.589), train_loss = 2.39930252, grad/param norm = 4.1299e-01, time/batch = 0.8060s\u001b[0m\t\n",
            "\u001b[0m250/2115 (epoch 0.591), train_loss = 2.38172405, grad/param norm = 4.4482e-01, time/batch = 0.6750s\u001b[0m\t\n",
            "\u001b[0m251/2115 (epoch 0.593), train_loss = 2.43753808, grad/param norm = 4.8519e-01, time/batch = 0.4169s\u001b[0m\t\n",
            "\u001b[0m252/2115 (epoch 0.596), train_loss = 2.34838996, grad/param norm = 5.4816e-01, time/batch = 0.4313s\u001b[0m\t\n",
            "\u001b[0m253/2115 (epoch 0.598), train_loss = 2.39963533, grad/param norm = 5.1364e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m254/2115 (epoch 0.600), train_loss = 2.38061618, grad/param norm = 4.6834e-01, time/batch = 0.4387s\u001b[0m\t\n",
            "\u001b[0m255/2115 (epoch 0.603), train_loss = 2.35714763, grad/param norm = 4.2197e-01, time/batch = 0.4150s\u001b[0m\t\n",
            "\u001b[0m256/2115 (epoch 0.605), train_loss = 2.37485831, grad/param norm = 2.9851e-01, time/batch = 0.4219s\u001b[0m\t\n",
            "\u001b[0m257/2115 (epoch 0.608), train_loss = 2.38860369, grad/param norm = 3.7676e-01, time/batch = 0.4520s\u001b[0m\t\n",
            "\u001b[0m258/2115 (epoch 0.610), train_loss = 2.37761358, grad/param norm = 6.1809e-01, time/batch = 0.4663s\u001b[0m\t\n",
            "\u001b[0m259/2115 (epoch 0.612), train_loss = 2.42149001, grad/param norm = 7.3962e-01, time/batch = 0.4545s\u001b[0m\t\n",
            "\u001b[0m260/2115 (epoch 0.615), train_loss = 2.37090996, grad/param norm = 6.3666e-01, time/batch = 0.4290s\u001b[0m\t\n",
            "\u001b[0m261/2115 (epoch 0.617), train_loss = 2.37786164, grad/param norm = 6.1267e-01, time/batch = 0.4511s\u001b[0m\t\n",
            "\u001b[0m262/2115 (epoch 0.619), train_loss = 2.33470656, grad/param norm = 4.8059e-01, time/batch = 0.4302s\u001b[0m\t\n",
            "\u001b[0m263/2115 (epoch 0.622), train_loss = 2.36840587, grad/param norm = 2.8172e-01, time/batch = 0.4145s\u001b[0m\t\n",
            "\u001b[0m264/2115 (epoch 0.624), train_loss = 2.34302888, grad/param norm = 1.9777e-01, time/batch = 0.4407s\u001b[0m\t\n",
            "\u001b[0m265/2115 (epoch 0.626), train_loss = 2.34194997, grad/param norm = 1.6319e-01, time/batch = 0.4115s\u001b[0m\t\n",
            "\u001b[0m266/2115 (epoch 0.629), train_loss = 2.35068807, grad/param norm = 1.9016e-01, time/batch = 0.4275s\u001b[0m\t\n",
            "\u001b[0m267/2115 (epoch 0.631), train_loss = 2.38052422, grad/param norm = 2.1229e-01, time/batch = 0.4127s\u001b[0m\t\n",
            "\u001b[0m268/2115 (epoch 0.634), train_loss = 2.33341303, grad/param norm = 3.1825e-01, time/batch = 0.4148s\u001b[0m\t\n",
            "\u001b[0m269/2115 (epoch 0.636), train_loss = 2.33891094, grad/param norm = 8.1599e-01, time/batch = 0.4364s\u001b[0m\t\n",
            "\u001b[0m270/2115 (epoch 0.638), train_loss = 2.45303282, grad/param norm = 1.2146e+00, time/batch = 0.4126s\u001b[0m\t\n",
            "\u001b[0m271/2115 (epoch 0.641), train_loss = 2.42000289, grad/param norm = 8.5542e-01, time/batch = 0.4495s\u001b[0m\t\n",
            "\u001b[0m272/2115 (epoch 0.643), train_loss = 2.41215719, grad/param norm = 5.0442e-01, time/batch = 0.4164s\u001b[0m\t\n",
            "\u001b[0m273/2115 (epoch 0.645), train_loss = 2.34212675, grad/param norm = 3.9937e-01, time/batch = 0.6799s\u001b[0m\t\n",
            "\u001b[0m274/2115 (epoch 0.648), train_loss = 2.35165064, grad/param norm = 2.6083e-01, time/batch = 0.9875s\u001b[0m\t\n",
            "\u001b[0m275/2115 (epoch 0.650), train_loss = 2.31555877, grad/param norm = 2.7123e-01, time/batch = 1.0000s\u001b[0m\t\n",
            "\u001b[0m276/2115 (epoch 0.652), train_loss = 2.34647695, grad/param norm = 4.6526e-01, time/batch = 0.7364s\u001b[0m\t\n",
            "\u001b[0m277/2115 (epoch 0.655), train_loss = 2.34991484, grad/param norm = 5.6312e-01, time/batch = 0.4413s\u001b[0m\t\n",
            "\u001b[0m278/2115 (epoch 0.657), train_loss = 2.36017062, grad/param norm = 4.7298e-01, time/batch = 0.4278s\u001b[0m\t\n",
            "\u001b[0m279/2115 (epoch 0.660), train_loss = 2.38552526, grad/param norm = 3.7170e-01, time/batch = 0.4485s\u001b[0m\t\n",
            "\u001b[0m280/2115 (epoch 0.662), train_loss = 2.36054093, grad/param norm = 2.9074e-01, time/batch = 0.4189s\u001b[0m\t\n",
            "\u001b[0m281/2115 (epoch 0.664), train_loss = 2.28524909, grad/param norm = 2.6526e-01, time/batch = 0.4313s\u001b[0m\t\n",
            "\u001b[0m282/2115 (epoch 0.667), train_loss = 2.35452199, grad/param norm = 3.1003e-01, time/batch = 0.4458s\u001b[0m\t\n",
            "\u001b[0m283/2115 (epoch 0.669), train_loss = 2.31638533, grad/param norm = 4.1225e-01, time/batch = 0.4229s\u001b[0m\t\n",
            "\u001b[0m284/2115 (epoch 0.671), train_loss = 2.35942849, grad/param norm = 5.5630e-01, time/batch = 0.4503s\u001b[0m\t\n",
            "\u001b[0m285/2115 (epoch 0.674), train_loss = 2.37046400, grad/param norm = 6.3152e-01, time/batch = 0.4320s\u001b[0m\t\n",
            "\u001b[0m286/2115 (epoch 0.676), train_loss = 2.33827742, grad/param norm = 4.6856e-01, time/batch = 0.4479s\u001b[0m\t\n",
            "\u001b[0m287/2115 (epoch 0.678), train_loss = 2.29472060, grad/param norm = 4.8195e-01, time/batch = 0.4507s\u001b[0m\t\n",
            "\u001b[0m288/2115 (epoch 0.681), train_loss = 2.36070104, grad/param norm = 5.0288e-01, time/batch = 0.4267s\u001b[0m\t\n",
            "\u001b[0m289/2115 (epoch 0.683), train_loss = 2.35010277, grad/param norm = 4.0711e-01, time/batch = 0.4440s\u001b[0m\t\n",
            "\u001b[0m290/2115 (epoch 0.686), train_loss = 2.32013622, grad/param norm = 3.6213e-01, time/batch = 0.4143s\u001b[0m\t\n",
            "\u001b[0m291/2115 (epoch 0.688), train_loss = 2.29007995, grad/param norm = 4.7577e-01, time/batch = 0.4475s\u001b[0m\t\n",
            "\u001b[0m292/2115 (epoch 0.690), train_loss = 2.34251635, grad/param norm = 5.8789e-01, time/batch = 0.4178s\u001b[0m\t\n",
            "\u001b[0m293/2115 (epoch 0.693), train_loss = 2.28472765, grad/param norm = 6.0234e-01, time/batch = 0.4369s\u001b[0m\t\n",
            "\u001b[0m294/2115 (epoch 0.695), train_loss = 2.33995473, grad/param norm = 5.4553e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m295/2115 (epoch 0.697), train_loss = 2.34533605, grad/param norm = 3.4785e-01, time/batch = 0.4177s\u001b[0m\t\n",
            "\u001b[0m296/2115 (epoch 0.700), train_loss = 2.32119288, grad/param norm = 2.5606e-01, time/batch = 0.4411s\u001b[0m\t\n",
            "\u001b[0m297/2115 (epoch 0.702), train_loss = 2.33747366, grad/param norm = 2.4986e-01, time/batch = 0.4229s\u001b[0m\t\n",
            "\u001b[0m298/2115 (epoch 0.704), train_loss = 2.27118159, grad/param norm = 2.7103e-01, time/batch = 0.4393s\u001b[0m\t\n",
            "\u001b[0m299/2115 (epoch 0.707), train_loss = 2.32632685, grad/param norm = 2.1785e-01, time/batch = 0.6314s\u001b[0m\t\n",
            "\u001b[0m300/2115 (epoch 0.709), train_loss = 2.20524262, grad/param norm = 1.7065e-01, time/batch = 0.9295s\u001b[0m\t\n",
            "\u001b[0m301/2115 (epoch 0.712), train_loss = 2.24034472, grad/param norm = 2.2174e-01, time/batch = 1.0084s\u001b[0m\t\n",
            "\u001b[0m302/2115 (epoch 0.714), train_loss = 2.30202670, grad/param norm = 3.9811e-01, time/batch = 0.7984s\u001b[0m\t\n",
            "\u001b[0m303/2115 (epoch 0.716), train_loss = 2.26759777, grad/param norm = 6.7944e-01, time/batch = 0.4210s\u001b[0m\t\n",
            "\u001b[0m304/2115 (epoch 0.719), train_loss = 2.29706265, grad/param norm = 8.1439e-01, time/batch = 0.4482s\u001b[0m\t\n",
            "\u001b[0m305/2115 (epoch 0.721), train_loss = 2.36459620, grad/param norm = 7.1162e-01, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m306/2115 (epoch 0.723), train_loss = 2.32685101, grad/param norm = 4.5919e-01, time/batch = 0.4353s\u001b[0m\t\n",
            "\u001b[0m307/2115 (epoch 0.726), train_loss = 2.22848395, grad/param norm = 2.7854e-01, time/batch = 0.4165s\u001b[0m\t\n",
            "\u001b[0m308/2115 (epoch 0.728), train_loss = 2.29062294, grad/param norm = 2.2181e-01, time/batch = 0.4267s\u001b[0m\t\n",
            "\u001b[0m309/2115 (epoch 0.730), train_loss = 2.32637523, grad/param norm = 2.5373e-01, time/batch = 0.4304s\u001b[0m\t\n",
            "\u001b[0m310/2115 (epoch 0.733), train_loss = 2.28095515, grad/param norm = 3.1145e-01, time/batch = 0.4721s\u001b[0m\t\n",
            "\u001b[0m311/2115 (epoch 0.735), train_loss = 2.26098658, grad/param norm = 3.3381e-01, time/batch = 0.4450s\u001b[0m\t\n",
            "\u001b[0m312/2115 (epoch 0.738), train_loss = 2.33762596, grad/param norm = 3.7204e-01, time/batch = 0.4113s\u001b[0m\t\n",
            "\u001b[0m313/2115 (epoch 0.740), train_loss = 2.32553102, grad/param norm = 9.1838e-01, time/batch = 0.4281s\u001b[0m\t\n",
            "\u001b[0m314/2115 (epoch 0.742), train_loss = 2.37940589, grad/param norm = 8.7624e-01, time/batch = 0.4168s\u001b[0m\t\n",
            "\u001b[0m315/2115 (epoch 0.745), train_loss = 2.26184533, grad/param norm = 5.3806e-01, time/batch = 0.4123s\u001b[0m\t\n",
            "\u001b[0m316/2115 (epoch 0.747), train_loss = 2.27403614, grad/param norm = 3.5523e-01, time/batch = 0.4269s\u001b[0m\t\n",
            "\u001b[0m317/2115 (epoch 0.749), train_loss = 2.26754830, grad/param norm = 2.2745e-01, time/batch = 0.4176s\u001b[0m\t\n",
            "\u001b[0m318/2115 (epoch 0.752), train_loss = 2.30273075, grad/param norm = 2.9980e-01, time/batch = 0.4487s\u001b[0m\t\n",
            "\u001b[0m319/2115 (epoch 0.754), train_loss = 2.26127076, grad/param norm = 3.5471e-01, time/batch = 0.4252s\u001b[0m\t\n",
            "\u001b[0m320/2115 (epoch 0.757), train_loss = 2.26006180, grad/param norm = 3.7924e-01, time/batch = 0.4194s\u001b[0m\t\n",
            "\u001b[0m321/2115 (epoch 0.759), train_loss = 2.23995366, grad/param norm = 3.4388e-01, time/batch = 0.4561s\u001b[0m\t\n",
            "\u001b[0m322/2115 (epoch 0.761), train_loss = 2.22883125, grad/param norm = 2.4471e-01, time/batch = 0.4519s\u001b[0m\t\n",
            "\u001b[0m323/2115 (epoch 0.764), train_loss = 2.25096843, grad/param norm = 2.3675e-01, time/batch = 0.4444s\u001b[0m\t\n",
            "\u001b[0m324/2115 (epoch 0.766), train_loss = 2.25618001, grad/param norm = 2.2405e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m325/2115 (epoch 0.768), train_loss = 2.24828207, grad/param norm = 2.5680e-01, time/batch = 0.5150s\u001b[0m\t\n",
            "\u001b[0m326/2115 (epoch 0.771), train_loss = 2.24942433, grad/param norm = 3.2138e-01, time/batch = 0.7550s\u001b[0m\t\n",
            "\u001b[0m327/2115 (epoch 0.773), train_loss = 2.21414466, grad/param norm = 4.6534e-01, time/batch = 0.9634s\u001b[0m\t\n",
            "\u001b[0m328/2115 (epoch 0.775), train_loss = 2.23340462, grad/param norm = 5.1752e-01, time/batch = 0.9012s\u001b[0m\t\n",
            "\u001b[0m329/2115 (epoch 0.778), train_loss = 2.29384755, grad/param norm = 5.7310e-01, time/batch = 0.6278s\u001b[0m\t\n",
            "\u001b[0m330/2115 (epoch 0.780), train_loss = 2.22924906, grad/param norm = 5.8027e-01, time/batch = 0.4241s\u001b[0m\t\n",
            "\u001b[0m331/2115 (epoch 0.783), train_loss = 2.27251958, grad/param norm = 5.6512e-01, time/batch = 0.4509s\u001b[0m\t\n",
            "\u001b[0m332/2115 (epoch 0.785), train_loss = 2.30110072, grad/param norm = 4.7028e-01, time/batch = 0.4168s\u001b[0m\t\n",
            "\u001b[0m333/2115 (epoch 0.787), train_loss = 2.25348031, grad/param norm = 3.5094e-01, time/batch = 0.4159s\u001b[0m\t\n",
            "\u001b[0m334/2115 (epoch 0.790), train_loss = 2.20340653, grad/param norm = 2.4580e-01, time/batch = 0.4325s\u001b[0m\t\n",
            "\u001b[0m335/2115 (epoch 0.792), train_loss = 2.22166668, grad/param norm = 2.5976e-01, time/batch = 0.4124s\u001b[0m\t\n",
            "\u001b[0m336/2115 (epoch 0.794), train_loss = 2.22346507, grad/param norm = 3.1485e-01, time/batch = 0.4499s\u001b[0m\t\n",
            "\u001b[0m337/2115 (epoch 0.797), train_loss = 2.23800537, grad/param norm = 4.0603e-01, time/batch = 0.4157s\u001b[0m\t\n",
            "\u001b[0m338/2115 (epoch 0.799), train_loss = 2.24314377, grad/param norm = 5.1162e-01, time/batch = 0.4190s\u001b[0m\t\n",
            "\u001b[0m339/2115 (epoch 0.801), train_loss = 2.30173550, grad/param norm = 5.4253e-01, time/batch = 0.4343s\u001b[0m\t\n",
            "\u001b[0m340/2115 (epoch 0.804), train_loss = 2.22243401, grad/param norm = 4.5817e-01, time/batch = 0.4141s\u001b[0m\t\n",
            "\u001b[0m341/2115 (epoch 0.806), train_loss = 2.27191109, grad/param norm = 3.9151e-01, time/batch = 0.4546s\u001b[0m\t\n",
            "\u001b[0m342/2115 (epoch 0.809), train_loss = 2.18387819, grad/param norm = 3.5252e-01, time/batch = 0.4132s\u001b[0m\t\n",
            "\u001b[0m343/2115 (epoch 0.811), train_loss = 2.23502690, grad/param norm = 4.5029e-01, time/batch = 0.4397s\u001b[0m\t\n",
            "\u001b[0m344/2115 (epoch 0.813), train_loss = 2.24057331, grad/param norm = 5.7136e-01, time/batch = 0.4109s\u001b[0m\t\n",
            "\u001b[0m345/2115 (epoch 0.816), train_loss = 2.18872898, grad/param norm = 5.3299e-01, time/batch = 0.4285s\u001b[0m\t\n",
            "\u001b[0m346/2115 (epoch 0.818), train_loss = 2.26300641, grad/param norm = 5.1337e-01, time/batch = 0.4405s\u001b[0m\t\n",
            "\u001b[0m347/2115 (epoch 0.820), train_loss = 2.24693178, grad/param norm = 5.0256e-01, time/batch = 0.4140s\u001b[0m\t\n",
            "\u001b[0m348/2115 (epoch 0.823), train_loss = 2.27600493, grad/param norm = 4.4399e-01, time/batch = 0.4481s\u001b[0m\t\n",
            "\u001b[0m349/2115 (epoch 0.825), train_loss = 2.18370714, grad/param norm = 3.4502e-01, time/batch = 0.4228s\u001b[0m\t\n",
            "\u001b[0m350/2115 (epoch 0.827), train_loss = 2.25725312, grad/param norm = 3.1277e-01, time/batch = 0.4456s\u001b[0m\t\n",
            "\u001b[0m351/2115 (epoch 0.830), train_loss = 2.25527720, grad/param norm = 3.4987e-01, time/batch = 0.4341s\u001b[0m\t\n",
            "\u001b[0m352/2115 (epoch 0.832), train_loss = 2.21943722, grad/param norm = 3.3311e-01, time/batch = 0.6061s\u001b[0m\t\n",
            "\u001b[0m353/2115 (epoch 0.835), train_loss = 2.19978323, grad/param norm = 4.4514e-01, time/batch = 0.9222s\u001b[0m\t\n",
            "\u001b[0m354/2115 (epoch 0.837), train_loss = 2.21222953, grad/param norm = 7.6321e-01, time/batch = 0.9365s\u001b[0m\t\n",
            "\u001b[0m355/2115 (epoch 0.839), train_loss = 2.22876174, grad/param norm = 6.4449e-01, time/batch = 0.8830s\u001b[0m\t\n",
            "\u001b[0m356/2115 (epoch 0.842), train_loss = 2.23678948, grad/param norm = 3.5201e-01, time/batch = 0.4545s\u001b[0m\t\n",
            "\u001b[0m357/2115 (epoch 0.844), train_loss = 2.21442952, grad/param norm = 3.0163e-01, time/batch = 0.4156s\u001b[0m\t\n",
            "\u001b[0m358/2115 (epoch 0.846), train_loss = 2.23632980, grad/param norm = 2.5082e-01, time/batch = 0.4112s\u001b[0m\t\n",
            "\u001b[0m359/2115 (epoch 0.849), train_loss = 2.16688051, grad/param norm = 1.9472e-01, time/batch = 0.4350s\u001b[0m\t\n",
            "\u001b[0m360/2115 (epoch 0.851), train_loss = 2.16260946, grad/param norm = 2.1367e-01, time/batch = 0.4274s\u001b[0m\t\n",
            "\u001b[0m361/2115 (epoch 0.853), train_loss = 2.18650652, grad/param norm = 2.0444e-01, time/batch = 0.4420s\u001b[0m\t\n",
            "\u001b[0m362/2115 (epoch 0.856), train_loss = 2.14703051, grad/param norm = 2.5267e-01, time/batch = 0.4190s\u001b[0m\t\n",
            "\u001b[0m363/2115 (epoch 0.858), train_loss = 2.18662338, grad/param norm = 2.7072e-01, time/batch = 0.4378s\u001b[0m\t\n",
            "\u001b[0m364/2115 (epoch 0.861), train_loss = 2.19375197, grad/param norm = 3.6432e-01, time/batch = 0.4230s\u001b[0m\t\n",
            "\u001b[0m365/2115 (epoch 0.863), train_loss = 2.19446972, grad/param norm = 4.7229e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m366/2115 (epoch 0.865), train_loss = 2.21940483, grad/param norm = 5.5171e-01, time/batch = 0.4297s\u001b[0m\t\n",
            "\u001b[0m367/2115 (epoch 0.868), train_loss = 2.22825224, grad/param norm = 6.3326e-01, time/batch = 0.4155s\u001b[0m\t\n",
            "\u001b[0m368/2115 (epoch 0.870), train_loss = 2.21557133, grad/param norm = 5.9670e-01, time/batch = 0.4366s\u001b[0m\t\n",
            "\u001b[0m369/2115 (epoch 0.872), train_loss = 2.19817134, grad/param norm = 4.2994e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m370/2115 (epoch 0.875), train_loss = 2.18993288, grad/param norm = 3.3003e-01, time/batch = 0.4280s\u001b[0m\t\n",
            "\u001b[0m371/2115 (epoch 0.877), train_loss = 2.28092084, grad/param norm = 2.2590e-01, time/batch = 0.4599s\u001b[0m\t\n",
            "\u001b[0m372/2115 (epoch 0.879), train_loss = 2.18000835, grad/param norm = 2.2826e-01, time/batch = 0.4396s\u001b[0m\t\n",
            "\u001b[0m373/2115 (epoch 0.882), train_loss = 2.19110647, grad/param norm = 2.1756e-01, time/batch = 0.4410s\u001b[0m\t\n",
            "\u001b[0m374/2115 (epoch 0.884), train_loss = 2.19002397, grad/param norm = 2.1511e-01, time/batch = 0.4179s\u001b[0m\t\n",
            "\u001b[0m375/2115 (epoch 0.887), train_loss = 2.16341765, grad/param norm = 3.4562e-01, time/batch = 0.4384s\u001b[0m\t\n",
            "\u001b[0m376/2115 (epoch 0.889), train_loss = 2.21664696, grad/param norm = 3.7984e-01, time/batch = 0.4205s\u001b[0m\t\n",
            "\u001b[0m377/2115 (epoch 0.891), train_loss = 2.19690816, grad/param norm = 3.3869e-01, time/batch = 0.4169s\u001b[0m\t\n",
            "\u001b[0m378/2115 (epoch 0.894), train_loss = 2.14553982, grad/param norm = 3.3227e-01, time/batch = 0.4394s\u001b[0m\t\n",
            "\u001b[0m379/2115 (epoch 0.896), train_loss = 2.17911439, grad/param norm = 3.0078e-01, time/batch = 0.8094s\u001b[0m\t\n",
            "\u001b[0m380/2115 (epoch 0.898), train_loss = 2.21264919, grad/param norm = 3.4448e-01, time/batch = 0.8081s\u001b[0m\t\n",
            "\u001b[0m381/2115 (epoch 0.901), train_loss = 2.17424891, grad/param norm = 4.2174e-01, time/batch = 0.9568s\u001b[0m\t\n",
            "\u001b[0m382/2115 (epoch 0.903), train_loss = 2.21841478, grad/param norm = 5.3469e-01, time/batch = 0.7419s\u001b[0m\t\n",
            "\u001b[0m383/2115 (epoch 0.905), train_loss = 2.22441842, grad/param norm = 4.9248e-01, time/batch = 0.5136s\u001b[0m\t\n",
            "\u001b[0m384/2115 (epoch 0.908), train_loss = 2.24237517, grad/param norm = 5.1215e-01, time/batch = 0.4272s\u001b[0m\t\n",
            "\u001b[0m385/2115 (epoch 0.910), train_loss = 2.17569191, grad/param norm = 4.1825e-01, time/batch = 0.4116s\u001b[0m\t\n",
            "\u001b[0m386/2115 (epoch 0.913), train_loss = 2.19348876, grad/param norm = 2.6564e-01, time/batch = 0.4281s\u001b[0m\t\n",
            "\u001b[0m387/2115 (epoch 0.915), train_loss = 2.18693608, grad/param norm = 2.8883e-01, time/batch = 0.4076s\u001b[0m\t\n",
            "\u001b[0m388/2115 (epoch 0.917), train_loss = 2.18065902, grad/param norm = 3.0508e-01, time/batch = 0.4364s\u001b[0m\t\n",
            "\u001b[0m389/2115 (epoch 0.920), train_loss = 2.21740746, grad/param norm = 2.9520e-01, time/batch = 0.4106s\u001b[0m\t\n",
            "\u001b[0m390/2115 (epoch 0.922), train_loss = 2.16272129, grad/param norm = 2.8938e-01, time/batch = 0.4105s\u001b[0m\t\n",
            "\u001b[0m391/2115 (epoch 0.924), train_loss = 2.15248212, grad/param norm = 2.8654e-01, time/batch = 0.4292s\u001b[0m\t\n",
            "\u001b[0m392/2115 (epoch 0.927), train_loss = 2.20957828, grad/param norm = 3.3077e-01, time/batch = 0.4160s\u001b[0m\t\n",
            "\u001b[0m393/2115 (epoch 0.929), train_loss = 2.20257544, grad/param norm = 4.2903e-01, time/batch = 0.4535s\u001b[0m\t\n",
            "\u001b[0m394/2115 (epoch 0.931), train_loss = 2.20091466, grad/param norm = 3.8292e-01, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m395/2115 (epoch 0.934), train_loss = 2.14585890, grad/param norm = 3.4191e-01, time/batch = 0.4331s\u001b[0m\t\n",
            "\u001b[0m396/2115 (epoch 0.936), train_loss = 2.18247961, grad/param norm = 3.9393e-01, time/batch = 0.4119s\u001b[0m\t\n",
            "\u001b[0m397/2115 (epoch 0.939), train_loss = 2.15616152, grad/param norm = 4.5147e-01, time/batch = 0.4256s\u001b[0m\t\n",
            "\u001b[0m398/2115 (epoch 0.941), train_loss = 2.12458213, grad/param norm = 4.2347e-01, time/batch = 0.4408s\u001b[0m\t\n",
            "\u001b[0m399/2115 (epoch 0.943), train_loss = 2.13770871, grad/param norm = 3.6908e-01, time/batch = 0.4229s\u001b[0m\t\n",
            "\u001b[0m400/2115 (epoch 0.946), train_loss = 2.11348750, grad/param norm = 2.9257e-01, time/batch = 0.4486s\u001b[0m\t\n",
            "\u001b[0m401/2115 (epoch 0.948), train_loss = 2.19649063, grad/param norm = 2.3434e-01, time/batch = 0.4207s\u001b[0m\t\n",
            "\u001b[0m402/2115 (epoch 0.950), train_loss = 2.13559387, grad/param norm = 1.7858e-01, time/batch = 0.4501s\u001b[0m\t\n",
            "\u001b[0m403/2115 (epoch 0.953), train_loss = 2.15414439, grad/param norm = 1.7534e-01, time/batch = 0.4119s\u001b[0m\t\n",
            "\u001b[0m404/2115 (epoch 0.955), train_loss = 2.17470804, grad/param norm = 2.4487e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m405/2115 (epoch 0.957), train_loss = 2.16820404, grad/param norm = 3.8225e-01, time/batch = 0.5612s\u001b[0m\t\n",
            "\u001b[0m406/2115 (epoch 0.960), train_loss = 2.18836226, grad/param norm = 3.9340e-01, time/batch = 0.7701s\u001b[0m\t\n",
            "\u001b[0m407/2115 (epoch 0.962), train_loss = 2.18927457, grad/param norm = 3.7578e-01, time/batch = 0.9949s\u001b[0m\t\n",
            "\u001b[0m408/2115 (epoch 0.965), train_loss = 2.12086399, grad/param norm = 3.2913e-01, time/batch = 0.7698s\u001b[0m\t\n",
            "\u001b[0m409/2115 (epoch 0.967), train_loss = 2.20727814, grad/param norm = 3.0463e-01, time/batch = 0.7109s\u001b[0m\t\n",
            "\u001b[0m410/2115 (epoch 0.969), train_loss = 2.16462845, grad/param norm = 2.9319e-01, time/batch = 0.4257s\u001b[0m\t\n",
            "\u001b[0m411/2115 (epoch 0.972), train_loss = 2.16467343, grad/param norm = 2.4154e-01, time/batch = 0.4409s\u001b[0m\t\n",
            "\u001b[0m412/2115 (epoch 0.974), train_loss = 2.12650788, grad/param norm = 1.7351e-01, time/batch = 0.4240s\u001b[0m\t\n",
            "\u001b[0m413/2115 (epoch 0.976), train_loss = 2.14912130, grad/param norm = 2.0356e-01, time/batch = 0.4301s\u001b[0m\t\n",
            "\u001b[0m414/2115 (epoch 0.979), train_loss = 2.13735871, grad/param norm = 2.4806e-01, time/batch = 0.4284s\u001b[0m\t\n",
            "\u001b[0m415/2115 (epoch 0.981), train_loss = 2.13099245, grad/param norm = 3.2308e-01, time/batch = 0.4161s\u001b[0m\t\n",
            "\u001b[0m416/2115 (epoch 0.983), train_loss = 2.15812952, grad/param norm = 3.7669e-01, time/batch = 0.4397s\u001b[0m\t\n",
            "\u001b[0m417/2115 (epoch 0.986), train_loss = 2.15915775, grad/param norm = 4.3665e-01, time/batch = 0.4210s\u001b[0m\t\n",
            "\u001b[0m418/2115 (epoch 0.988), train_loss = 2.12785408, grad/param norm = 5.6863e-01, time/batch = 0.4376s\u001b[0m\t\n",
            "\u001b[0m419/2115 (epoch 0.991), train_loss = 2.17566609, grad/param norm = 5.4277e-01, time/batch = 0.4267s\u001b[0m\t\n",
            "\u001b[0m420/2115 (epoch 0.993), train_loss = 2.14913929, grad/param norm = 3.9522e-01, time/batch = 0.4431s\u001b[0m\t\n",
            "\u001b[0m421/2115 (epoch 0.995), train_loss = 2.14477340, grad/param norm = 3.2979e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m422/2115 (epoch 0.998), train_loss = 2.14874544, grad/param norm = 2.9679e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m423/2115 (epoch 1.000), train_loss = 2.16690959, grad/param norm = 2.8094e-01, time/batch = 0.4388s\u001b[0m\t\n",
            "\u001b[0m424/2115 (epoch 1.002), train_loss = 2.22151443, grad/param norm = 2.8352e-01, time/batch = 0.4163s\u001b[0m\t\n",
            "\u001b[0m425/2115 (epoch 1.005), train_loss = 2.16554657, grad/param norm = 3.1331e-01, time/batch = 0.4354s\u001b[0m\t\n",
            "\u001b[0m426/2115 (epoch 1.007), train_loss = 2.09710767, grad/param norm = 2.7824e-01, time/batch = 0.4292s\u001b[0m\t\n",
            "\u001b[0m427/2115 (epoch 1.009), train_loss = 2.14544964, grad/param norm = 2.9948e-01, time/batch = 0.4215s\u001b[0m\t\n",
            "\u001b[0m428/2115 (epoch 1.012), train_loss = 2.15738543, grad/param norm = 3.4024e-01, time/batch = 0.4270s\u001b[0m\t\n",
            "\u001b[0m429/2115 (epoch 1.014), train_loss = 2.14145318, grad/param norm = 3.3756e-01, time/batch = 0.4109s\u001b[0m\t\n",
            "\u001b[0m430/2115 (epoch 1.017), train_loss = 2.17970452, grad/param norm = 3.3465e-01, time/batch = 0.4355s\u001b[0m\t\n",
            "\u001b[0m431/2115 (epoch 1.019), train_loss = 2.17564015, grad/param norm = 4.3002e-01, time/batch = 0.4274s\u001b[0m\t\n",
            "\u001b[0m432/2115 (epoch 1.021), train_loss = 2.13162932, grad/param norm = 5.3020e-01, time/batch = 0.5841s\u001b[0m\t\n",
            "\u001b[0m433/2115 (epoch 1.024), train_loss = 2.08145699, grad/param norm = 4.7671e-01, time/batch = 0.9863s\u001b[0m\t\n",
            "\u001b[0m434/2115 (epoch 1.026), train_loss = 2.16913881, grad/param norm = 3.0291e-01, time/batch = 0.8529s\u001b[0m\t\n",
            "\u001b[0m435/2115 (epoch 1.028), train_loss = 2.11149985, grad/param norm = 2.0090e-01, time/batch = 0.8957s\u001b[0m\t\n",
            "\u001b[0m436/2115 (epoch 1.031), train_loss = 2.10002779, grad/param norm = 1.8473e-01, time/batch = 0.5154s\u001b[0m\t\n",
            "\u001b[0m437/2115 (epoch 1.033), train_loss = 2.07249914, grad/param norm = 2.1456e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m438/2115 (epoch 1.035), train_loss = 2.08742849, grad/param norm = 3.0039e-01, time/batch = 0.4376s\u001b[0m\t\n",
            "\u001b[0m439/2115 (epoch 1.038), train_loss = 2.07647025, grad/param norm = 3.3551e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m440/2115 (epoch 1.040), train_loss = 2.15590864, grad/param norm = 2.9828e-01, time/batch = 0.4219s\u001b[0m\t\n",
            "\u001b[0m441/2115 (epoch 1.043), train_loss = 2.09404251, grad/param norm = 2.3555e-01, time/batch = 0.4287s\u001b[0m\t\n",
            "\u001b[0m442/2115 (epoch 1.045), train_loss = 2.08348297, grad/param norm = 2.0375e-01, time/batch = 0.4187s\u001b[0m\t\n",
            "\u001b[0m443/2115 (epoch 1.047), train_loss = 2.09532336, grad/param norm = 2.3320e-01, time/batch = 0.4437s\u001b[0m\t\n",
            "\u001b[0m444/2115 (epoch 1.050), train_loss = 2.07894945, grad/param norm = 2.6199e-01, time/batch = 0.4165s\u001b[0m\t\n",
            "\u001b[0m445/2115 (epoch 1.052), train_loss = 2.08292195, grad/param norm = 3.3175e-01, time/batch = 0.4408s\u001b[0m\t\n",
            "\u001b[0m446/2115 (epoch 1.054), train_loss = 2.08192267, grad/param norm = 3.3637e-01, time/batch = 0.4802s\u001b[0m\t\n",
            "\u001b[0m447/2115 (epoch 1.057), train_loss = 2.08703413, grad/param norm = 3.3729e-01, time/batch = 0.5651s\u001b[0m\t\n",
            "\u001b[0m448/2115 (epoch 1.059), train_loss = 2.06020414, grad/param norm = 3.7647e-01, time/batch = 0.4098s\u001b[0m\t\n",
            "\u001b[0m449/2115 (epoch 1.061), train_loss = 2.11562580, grad/param norm = 4.2622e-01, time/batch = 0.4102s\u001b[0m\t\n",
            "\u001b[0m450/2115 (epoch 1.064), train_loss = 2.07297472, grad/param norm = 5.9822e-01, time/batch = 0.4362s\u001b[0m\t\n",
            "\u001b[0m451/2115 (epoch 1.066), train_loss = 2.14512375, grad/param norm = 4.7131e-01, time/batch = 0.4135s\u001b[0m\t\n",
            "\u001b[0m452/2115 (epoch 1.069), train_loss = 2.07765288, grad/param norm = 2.8870e-01, time/batch = 0.4395s\u001b[0m\t\n",
            "\u001b[0m453/2115 (epoch 1.071), train_loss = 2.09223010, grad/param norm = 3.0944e-01, time/batch = 0.4064s\u001b[0m\t\n",
            "\u001b[0m454/2115 (epoch 1.073), train_loss = 2.07724368, grad/param norm = 2.9656e-01, time/batch = 0.4217s\u001b[0m\t\n",
            "\u001b[0m455/2115 (epoch 1.076), train_loss = 2.14665797, grad/param norm = 3.1125e-01, time/batch = 0.4305s\u001b[0m\t\n",
            "\u001b[0m456/2115 (epoch 1.078), train_loss = 2.03415971, grad/param norm = 2.8865e-01, time/batch = 0.4078s\u001b[0m\t\n",
            "\u001b[0m457/2115 (epoch 1.080), train_loss = 2.06858068, grad/param norm = 2.3920e-01, time/batch = 0.4394s\u001b[0m\t\n",
            "\u001b[0m458/2115 (epoch 1.083), train_loss = 2.08148656, grad/param norm = 2.4171e-01, time/batch = 0.4377s\u001b[0m\t\n",
            "\u001b[0m459/2115 (epoch 1.085), train_loss = 2.07894361, grad/param norm = 3.1874e-01, time/batch = 0.9051s\u001b[0m\t\n",
            "\u001b[0m460/2115 (epoch 1.087), train_loss = 2.18920994, grad/param norm = 3.9233e-01, time/batch = 0.9762s\u001b[0m\t\n",
            "\u001b[0m461/2115 (epoch 1.090), train_loss = 2.13427097, grad/param norm = 3.6318e-01, time/batch = 0.7576s\u001b[0m\t\n",
            "\u001b[0m462/2115 (epoch 1.092), train_loss = 2.08012885, grad/param norm = 3.2936e-01, time/batch = 0.6474s\u001b[0m\t\n",
            "\u001b[0m463/2115 (epoch 1.095), train_loss = 2.11957112, grad/param norm = 3.6959e-01, time/batch = 0.4435s\u001b[0m\t\n",
            "\u001b[0m464/2115 (epoch 1.097), train_loss = 2.13114374, grad/param norm = 4.0834e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m465/2115 (epoch 1.099), train_loss = 2.10736102, grad/param norm = 3.7047e-01, time/batch = 0.4413s\u001b[0m\t\n",
            "\u001b[0m466/2115 (epoch 1.102), train_loss = 2.07831958, grad/param norm = 2.8744e-01, time/batch = 0.4193s\u001b[0m\t\n",
            "\u001b[0m467/2115 (epoch 1.104), train_loss = 2.04424992, grad/param norm = 2.3416e-01, time/batch = 0.4365s\u001b[0m\t\n",
            "\u001b[0m468/2115 (epoch 1.106), train_loss = 2.09655569, grad/param norm = 2.2952e-01, time/batch = 0.4303s\u001b[0m\t\n",
            "\u001b[0m469/2115 (epoch 1.109), train_loss = 2.05656956, grad/param norm = 2.0396e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m470/2115 (epoch 1.111), train_loss = 2.03161465, grad/param norm = 1.9257e-01, time/batch = 0.4349s\u001b[0m\t\n",
            "\u001b[0m471/2115 (epoch 1.113), train_loss = 2.04660160, grad/param norm = 1.6363e-01, time/batch = 0.4274s\u001b[0m\t\n",
            "\u001b[0m472/2115 (epoch 1.116), train_loss = 2.04597874, grad/param norm = 2.0447e-01, time/batch = 0.4150s\u001b[0m\t\n",
            "\u001b[0m473/2115 (epoch 1.118), train_loss = 2.02572800, grad/param norm = 3.2058e-01, time/batch = 0.4342s\u001b[0m\t\n",
            "\u001b[0m474/2115 (epoch 1.121), train_loss = 2.06691549, grad/param norm = 3.2306e-01, time/batch = 0.4296s\u001b[0m\t\n",
            "\u001b[0m475/2115 (epoch 1.123), train_loss = 2.01114985, grad/param norm = 2.8035e-01, time/batch = 0.4364s\u001b[0m\t\n",
            "\u001b[0m476/2115 (epoch 1.125), train_loss = 2.02388784, grad/param norm = 2.7503e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m477/2115 (epoch 1.128), train_loss = 2.08895412, grad/param norm = 2.8363e-01, time/batch = 0.4286s\u001b[0m\t\n",
            "\u001b[0m478/2115 (epoch 1.130), train_loss = 2.05960324, grad/param norm = 3.5988e-01, time/batch = 0.4303s\u001b[0m\t\n",
            "\u001b[0m479/2115 (epoch 1.132), train_loss = 2.04068609, grad/param norm = 3.1404e-01, time/batch = 0.4189s\u001b[0m\t\n",
            "\u001b[0m480/2115 (epoch 1.135), train_loss = 2.01833333, grad/param norm = 2.4717e-01, time/batch = 0.4427s\u001b[0m\t\n",
            "\u001b[0m481/2115 (epoch 1.137), train_loss = 2.07727992, grad/param norm = 3.4742e-01, time/batch = 0.4378s\u001b[0m\t\n",
            "\u001b[0m482/2115 (epoch 1.139), train_loss = 2.08758566, grad/param norm = 4.7053e-01, time/batch = 0.4430s\u001b[0m\t\n",
            "\u001b[0m483/2115 (epoch 1.142), train_loss = 2.08825119, grad/param norm = 5.0124e-01, time/batch = 0.4298s\u001b[0m\t\n",
            "\u001b[0m484/2115 (epoch 1.144), train_loss = 2.12939553, grad/param norm = 4.2346e-01, time/batch = 0.4379s\u001b[0m\t\n",
            "\u001b[0m485/2115 (epoch 1.147), train_loss = 2.11684420, grad/param norm = 3.0343e-01, time/batch = 0.7428s\u001b[0m\t\n",
            "\u001b[0m486/2115 (epoch 1.149), train_loss = 2.07360707, grad/param norm = 2.8196e-01, time/batch = 1.0280s\u001b[0m\t\n",
            "\u001b[0m487/2115 (epoch 1.151), train_loss = 2.12148967, grad/param norm = 2.8496e-01, time/batch = 0.8804s\u001b[0m\t\n",
            "\u001b[0m488/2115 (epoch 1.154), train_loss = 2.04578363, grad/param norm = 2.9422e-01, time/batch = 0.6894s\u001b[0m\t\n",
            "\u001b[0m489/2115 (epoch 1.156), train_loss = 2.05867522, grad/param norm = 2.0963e-01, time/batch = 0.4346s\u001b[0m\t\n",
            "\u001b[0m490/2115 (epoch 1.158), train_loss = 2.02944050, grad/param norm = 1.5933e-01, time/batch = 0.4669s\u001b[0m\t\n",
            "\u001b[0m491/2115 (epoch 1.161), train_loss = 2.04931793, grad/param norm = 1.7866e-01, time/batch = 0.4202s\u001b[0m\t\n",
            "\u001b[0m492/2115 (epoch 1.163), train_loss = 2.08660465, grad/param norm = 2.5348e-01, time/batch = 0.4194s\u001b[0m\t\n",
            "\u001b[0m493/2115 (epoch 1.165), train_loss = 2.06837940, grad/param norm = 2.6024e-01, time/batch = 0.4421s\u001b[0m\t\n",
            "\u001b[0m494/2115 (epoch 1.168), train_loss = 2.02876984, grad/param norm = 2.2157e-01, time/batch = 0.4154s\u001b[0m\t\n",
            "\u001b[0m495/2115 (epoch 1.170), train_loss = 2.03449025, grad/param norm = 2.8110e-01, time/batch = 0.4402s\u001b[0m\t\n",
            "\u001b[0m496/2115 (epoch 1.173), train_loss = 2.06791991, grad/param norm = 3.0224e-01, time/batch = 0.4140s\u001b[0m\t\n",
            "\u001b[0m497/2115 (epoch 1.175), train_loss = 2.02007913, grad/param norm = 3.1228e-01, time/batch = 0.4440s\u001b[0m\t\n",
            "\u001b[0m498/2115 (epoch 1.177), train_loss = 2.03181023, grad/param norm = 2.8973e-01, time/batch = 0.4183s\u001b[0m\t\n",
            "\u001b[0m499/2115 (epoch 1.180), train_loss = 2.01158862, grad/param norm = 2.5339e-01, time/batch = 0.4089s\u001b[0m\t\n",
            "\u001b[0m500/2115 (epoch 1.182), train_loss = 2.03018467, grad/param norm = 2.6614e-01, time/batch = 0.4323s\u001b[0m\t\n",
            "\u001b[0m501/2115 (epoch 1.184), train_loss = 2.04534832, grad/param norm = 2.8044e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m502/2115 (epoch 1.187), train_loss = 2.04277538, grad/param norm = 3.1978e-01, time/batch = 0.4505s\u001b[0m\t\n",
            "\u001b[0m503/2115 (epoch 1.189), train_loss = 2.06678410, grad/param norm = 4.3517e-01, time/batch = 0.4109s\u001b[0m\t\n",
            "\u001b[0m504/2115 (epoch 1.191), train_loss = 2.12047546, grad/param norm = 4.2303e-01, time/batch = 0.4173s\u001b[0m\t\n",
            "\u001b[0m505/2115 (epoch 1.194), train_loss = 1.97724172, grad/param norm = 3.5026e-01, time/batch = 0.4291s\u001b[0m\t\n",
            "\u001b[0m506/2115 (epoch 1.196), train_loss = 2.03241580, grad/param norm = 2.5139e-01, time/batch = 0.4124s\u001b[0m\t\n",
            "\u001b[0m507/2115 (epoch 1.199), train_loss = 2.02634879, grad/param norm = 2.0473e-01, time/batch = 0.4415s\u001b[0m\t\n",
            "\u001b[0m508/2115 (epoch 1.201), train_loss = 2.04426378, grad/param norm = 2.1249e-01, time/batch = 0.4160s\u001b[0m\t\n",
            "\u001b[0m509/2115 (epoch 1.203), train_loss = 1.99713331, grad/param norm = 1.9811e-01, time/batch = 0.4134s\u001b[0m\t\n",
            "\u001b[0m510/2115 (epoch 1.206), train_loss = 1.99128850, grad/param norm = 2.1530e-01, time/batch = 0.5204s\u001b[0m\t\n",
            "\u001b[0m511/2115 (epoch 1.208), train_loss = 1.96855486, grad/param norm = 2.7609e-01, time/batch = 0.9500s\u001b[0m\t\n",
            "\u001b[0m512/2115 (epoch 1.210), train_loss = 2.11556733, grad/param norm = 4.7045e-01, time/batch = 1.0943s\u001b[0m\t\n",
            "\u001b[0m513/2115 (epoch 1.213), train_loss = 2.04669377, grad/param norm = 4.5451e-01, time/batch = 0.8042s\u001b[0m\t\n",
            "\u001b[0m514/2115 (epoch 1.215), train_loss = 2.00320785, grad/param norm = 2.8795e-01, time/batch = 0.6663s\u001b[0m\t\n",
            "\u001b[0m515/2115 (epoch 1.217), train_loss = 1.98805343, grad/param norm = 2.3782e-01, time/batch = 0.4302s\u001b[0m\t\n",
            "\u001b[0m516/2115 (epoch 1.220), train_loss = 2.01187137, grad/param norm = 2.1765e-01, time/batch = 0.4180s\u001b[0m\t\n",
            "\u001b[0m517/2115 (epoch 1.222), train_loss = 2.00605322, grad/param norm = 2.5180e-01, time/batch = 0.4414s\u001b[0m\t\n",
            "\u001b[0m518/2115 (epoch 1.225), train_loss = 2.03994003, grad/param norm = 2.4677e-01, time/batch = 0.4230s\u001b[0m\t\n",
            "\u001b[0m519/2115 (epoch 1.227), train_loss = 2.05913951, grad/param norm = 3.3174e-01, time/batch = 0.4164s\u001b[0m\t\n",
            "\u001b[0m520/2115 (epoch 1.229), train_loss = 2.04512641, grad/param norm = 2.9925e-01, time/batch = 0.4542s\u001b[0m\t\n",
            "\u001b[0m521/2115 (epoch 1.232), train_loss = 2.01077494, grad/param norm = 2.4791e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m522/2115 (epoch 1.234), train_loss = 1.98585604, grad/param norm = 3.1332e-01, time/batch = 0.4281s\u001b[0m\t\n",
            "\u001b[0m523/2115 (epoch 1.236), train_loss = 2.06017819, grad/param norm = 3.3176e-01, time/batch = 0.4151s\u001b[0m\t\n",
            "\u001b[0m524/2115 (epoch 1.239), train_loss = 2.01958900, grad/param norm = 3.2052e-01, time/batch = 0.4319s\u001b[0m\t\n",
            "\u001b[0m525/2115 (epoch 1.241), train_loss = 2.02589961, grad/param norm = 2.5547e-01, time/batch = 0.4354s\u001b[0m\t\n",
            "\u001b[0m526/2115 (epoch 1.243), train_loss = 2.03701219, grad/param norm = 2.1728e-01, time/batch = 0.4238s\u001b[0m\t\n",
            "\u001b[0m527/2115 (epoch 1.246), train_loss = 1.99583682, grad/param norm = 1.8349e-01, time/batch = 0.4309s\u001b[0m\t\n",
            "\u001b[0m528/2115 (epoch 1.248), train_loss = 1.99804180, grad/param norm = 2.3765e-01, time/batch = 0.4209s\u001b[0m\t\n",
            "\u001b[0m529/2115 (epoch 1.251), train_loss = 2.02077974, grad/param norm = 2.7243e-01, time/batch = 0.4311s\u001b[0m\t\n",
            "\u001b[0m530/2115 (epoch 1.253), train_loss = 1.98190350, grad/param norm = 2.8103e-01, time/batch = 0.4209s\u001b[0m\t\n",
            "\u001b[0m531/2115 (epoch 1.255), train_loss = 1.98851327, grad/param norm = 2.4686e-01, time/batch = 0.4437s\u001b[0m\t\n",
            "\u001b[0m532/2115 (epoch 1.258), train_loss = 2.02955945, grad/param norm = 2.3807e-01, time/batch = 0.4115s\u001b[0m\t\n",
            "\u001b[0m533/2115 (epoch 1.260), train_loss = 1.95124182, grad/param norm = 2.5193e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m534/2115 (epoch 1.262), train_loss = 1.94853432, grad/param norm = 2.7197e-01, time/batch = 0.4381s\u001b[0m\t\n",
            "\u001b[0m535/2115 (epoch 1.265), train_loss = 2.01537909, grad/param norm = 2.6029e-01, time/batch = 0.4105s\u001b[0m\t\n",
            "\u001b[0m536/2115 (epoch 1.267), train_loss = 2.01458144, grad/param norm = 2.3256e-01, time/batch = 0.4336s\u001b[0m\t\n",
            "\u001b[0m537/2115 (epoch 1.270), train_loss = 2.01092373, grad/param norm = 2.1041e-01, time/batch = 0.5539s\u001b[0m\t\n",
            "\u001b[0m538/2115 (epoch 1.272), train_loss = 2.03247414, grad/param norm = 2.0591e-01, time/batch = 0.9492s\u001b[0m\t\n",
            "\u001b[0m539/2115 (epoch 1.274), train_loss = 2.03974050, grad/param norm = 2.4141e-01, time/batch = 0.9410s\u001b[0m\t\n",
            "\u001b[0m540/2115 (epoch 1.277), train_loss = 2.01109452, grad/param norm = 2.5531e-01, time/batch = 0.9296s\u001b[0m\t\n",
            "\u001b[0m541/2115 (epoch 1.279), train_loss = 2.04364991, grad/param norm = 2.3111e-01, time/batch = 0.4214s\u001b[0m\t\n",
            "\u001b[0m542/2115 (epoch 1.281), train_loss = 1.99423580, grad/param norm = 2.1641e-01, time/batch = 0.4338s\u001b[0m\t\n",
            "\u001b[0m543/2115 (epoch 1.284), train_loss = 1.91922205, grad/param norm = 2.5407e-01, time/batch = 0.4133s\u001b[0m\t\n",
            "\u001b[0m544/2115 (epoch 1.286), train_loss = 2.01299140, grad/param norm = 3.6936e-01, time/batch = 0.4291s\u001b[0m\t\n",
            "\u001b[0m545/2115 (epoch 1.288), train_loss = 2.03719071, grad/param norm = 5.0565e-01, time/batch = 0.4398s\u001b[0m\t\n",
            "\u001b[0m546/2115 (epoch 1.291), train_loss = 1.99455203, grad/param norm = 4.0986e-01, time/batch = 0.4137s\u001b[0m\t\n",
            "\u001b[0m547/2115 (epoch 1.293), train_loss = 1.96196652, grad/param norm = 2.2428e-01, time/batch = 0.4425s\u001b[0m\t\n",
            "\u001b[0m548/2115 (epoch 1.296), train_loss = 1.98300257, grad/param norm = 2.0576e-01, time/batch = 0.4103s\u001b[0m\t\n",
            "\u001b[0m549/2115 (epoch 1.298), train_loss = 2.04929339, grad/param norm = 3.5180e-01, time/batch = 0.4395s\u001b[0m\t\n",
            "\u001b[0m550/2115 (epoch 1.300), train_loss = 1.99727584, grad/param norm = 3.6750e-01, time/batch = 0.4169s\u001b[0m\t\n",
            "\u001b[0m551/2115 (epoch 1.303), train_loss = 2.02164905, grad/param norm = 3.4445e-01, time/batch = 0.4279s\u001b[0m\t\n",
            "\u001b[0m552/2115 (epoch 1.305), train_loss = 2.04936761, grad/param norm = 4.8337e-01, time/batch = 0.4695s\u001b[0m\t\n",
            "\u001b[0m553/2115 (epoch 1.307), train_loss = 1.98296710, grad/param norm = 3.7346e-01, time/batch = 0.4118s\u001b[0m\t\n",
            "\u001b[0m554/2115 (epoch 1.310), train_loss = 1.94373194, grad/param norm = 2.4654e-01, time/batch = 0.4416s\u001b[0m\t\n",
            "\u001b[0m555/2115 (epoch 1.312), train_loss = 1.98481033, grad/param norm = 2.2433e-01, time/batch = 0.4099s\u001b[0m\t\n",
            "\u001b[0m556/2115 (epoch 1.314), train_loss = 2.02561606, grad/param norm = 1.9545e-01, time/batch = 0.4167s\u001b[0m\t\n",
            "\u001b[0m557/2115 (epoch 1.317), train_loss = 1.96675184, grad/param norm = 1.8739e-01, time/batch = 0.4307s\u001b[0m\t\n",
            "\u001b[0m558/2115 (epoch 1.319), train_loss = 1.93968059, grad/param norm = 2.0113e-01, time/batch = 0.4248s\u001b[0m\t\n",
            "\u001b[0m559/2115 (epoch 1.322), train_loss = 1.98206016, grad/param norm = 1.7504e-01, time/batch = 0.4637s\u001b[0m\t\n",
            "\u001b[0m560/2115 (epoch 1.324), train_loss = 2.03327896, grad/param norm = 2.0851e-01, time/batch = 0.4109s\u001b[0m\t\n",
            "\u001b[0m561/2115 (epoch 1.326), train_loss = 2.02910367, grad/param norm = 2.2319e-01, time/batch = 0.4517s\u001b[0m\t\n",
            "\u001b[0m562/2115 (epoch 1.329), train_loss = 2.02931525, grad/param norm = 2.6409e-01, time/batch = 0.4184s\u001b[0m\t\n",
            "\u001b[0m563/2115 (epoch 1.331), train_loss = 2.04613984, grad/param norm = 2.7267e-01, time/batch = 0.4587s\u001b[0m\t\n",
            "\u001b[0m564/2115 (epoch 1.333), train_loss = 2.03241847, grad/param norm = 2.8530e-01, time/batch = 0.7408s\u001b[0m\t\n",
            "\u001b[0m565/2115 (epoch 1.336), train_loss = 2.02498436, grad/param norm = 2.3594e-01, time/batch = 1.0618s\u001b[0m\t\n",
            "\u001b[0m566/2115 (epoch 1.338), train_loss = 1.93992060, grad/param norm = 1.8173e-01, time/batch = 0.8700s\u001b[0m\t\n",
            "\u001b[0m567/2115 (epoch 1.340), train_loss = 1.95706978, grad/param norm = 1.7473e-01, time/batch = 0.6668s\u001b[0m\t\n",
            "\u001b[0m568/2115 (epoch 1.343), train_loss = 1.97707719, grad/param norm = 1.8381e-01, time/batch = 0.4160s\u001b[0m\t\n",
            "\u001b[0m569/2115 (epoch 1.345), train_loss = 1.97310409, grad/param norm = 1.8568e-01, time/batch = 0.4149s\u001b[0m\t\n",
            "\u001b[0m570/2115 (epoch 1.348), train_loss = 1.97221986, grad/param norm = 2.5365e-01, time/batch = 0.4648s\u001b[0m\t\n",
            "\u001b[0m571/2115 (epoch 1.350), train_loss = 1.97465594, grad/param norm = 2.7769e-01, time/batch = 0.4375s\u001b[0m\t\n",
            "\u001b[0m572/2115 (epoch 1.352), train_loss = 2.01161258, grad/param norm = 3.1529e-01, time/batch = 0.4547s\u001b[0m\t\n",
            "\u001b[0m573/2115 (epoch 1.355), train_loss = 1.99610666, grad/param norm = 2.7519e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m574/2115 (epoch 1.357), train_loss = 1.95748958, grad/param norm = 3.0130e-01, time/batch = 0.4354s\u001b[0m\t\n",
            "\u001b[0m575/2115 (epoch 1.359), train_loss = 2.00358630, grad/param norm = 2.7814e-01, time/batch = 0.4136s\u001b[0m\t\n",
            "\u001b[0m576/2115 (epoch 1.362), train_loss = 1.97389992, grad/param norm = 2.9408e-01, time/batch = 0.4145s\u001b[0m\t\n",
            "\u001b[0m577/2115 (epoch 1.364), train_loss = 2.00543991, grad/param norm = 4.2671e-01, time/batch = 0.4459s\u001b[0m\t\n",
            "\u001b[0m578/2115 (epoch 1.366), train_loss = 2.01554117, grad/param norm = 4.1342e-01, time/batch = 0.4174s\u001b[0m\t\n",
            "\u001b[0m579/2115 (epoch 1.369), train_loss = 2.01487192, grad/param norm = 3.3577e-01, time/batch = 0.4317s\u001b[0m\t\n",
            "\u001b[0m580/2115 (epoch 1.371), train_loss = 1.95992846, grad/param norm = 2.5841e-01, time/batch = 0.4169s\u001b[0m\t\n",
            "\u001b[0m581/2115 (epoch 1.374), train_loss = 1.98121975, grad/param norm = 2.0723e-01, time/batch = 0.4495s\u001b[0m\t\n",
            "\u001b[0m582/2115 (epoch 1.376), train_loss = 1.95911967, grad/param norm = 2.1075e-01, time/batch = 0.4400s\u001b[0m\t\n",
            "\u001b[0m583/2115 (epoch 1.378), train_loss = 1.97016479, grad/param norm = 2.3387e-01, time/batch = 0.4171s\u001b[0m\t\n",
            "\u001b[0m584/2115 (epoch 1.381), train_loss = 1.91673125, grad/param norm = 2.3459e-01, time/batch = 0.4418s\u001b[0m\t\n",
            "\u001b[0m585/2115 (epoch 1.383), train_loss = 2.06390525, grad/param norm = 2.3463e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m586/2115 (epoch 1.385), train_loss = 1.98077293, grad/param norm = 2.4721e-01, time/batch = 0.4450s\u001b[0m\t\n",
            "\u001b[0m587/2115 (epoch 1.388), train_loss = 1.95338319, grad/param norm = 2.1280e-01, time/batch = 0.4158s\u001b[0m\t\n",
            "\u001b[0m588/2115 (epoch 1.390), train_loss = 1.99364869, grad/param norm = 1.7896e-01, time/batch = 0.4562s\u001b[0m\t\n",
            "\u001b[0m589/2115 (epoch 1.392), train_loss = 1.98233163, grad/param norm = 1.9009e-01, time/batch = 0.4344s\u001b[0m\t\n",
            "\u001b[0m590/2115 (epoch 1.395), train_loss = 1.95868377, grad/param norm = 1.8582e-01, time/batch = 0.6196s\u001b[0m\t\n",
            "\u001b[0m591/2115 (epoch 1.397), train_loss = 1.91983368, grad/param norm = 2.3508e-01, time/batch = 0.9239s\u001b[0m\t\n",
            "\u001b[0m592/2115 (epoch 1.400), train_loss = 1.97762225, grad/param norm = 2.9231e-01, time/batch = 1.0057s\u001b[0m\t\n",
            "\u001b[0m593/2115 (epoch 1.402), train_loss = 2.02491716, grad/param norm = 3.7724e-01, time/batch = 0.7217s\u001b[0m\t\n",
            "\u001b[0m594/2115 (epoch 1.404), train_loss = 2.00488228, grad/param norm = 3.4143e-01, time/batch = 0.4379s\u001b[0m\t\n",
            "\u001b[0m595/2115 (epoch 1.407), train_loss = 2.01114315, grad/param norm = 2.4384e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m596/2115 (epoch 1.409), train_loss = 1.98492347, grad/param norm = 2.3201e-01, time/batch = 0.4288s\u001b[0m\t\n",
            "\u001b[0m597/2115 (epoch 1.411), train_loss = 1.94918386, grad/param norm = 2.0664e-01, time/batch = 0.4286s\u001b[0m\t\n",
            "\u001b[0m598/2115 (epoch 1.414), train_loss = 1.97172399, grad/param norm = 2.1513e-01, time/batch = 0.4075s\u001b[0m\t\n",
            "\u001b[0m599/2115 (epoch 1.416), train_loss = 2.00480865, grad/param norm = 2.5898e-01, time/batch = 0.4367s\u001b[0m\t\n",
            "\u001b[0m600/2115 (epoch 1.418), train_loss = 1.98706454, grad/param norm = 2.9430e-01, time/batch = 0.4137s\u001b[0m\t\n",
            "\u001b[0m601/2115 (epoch 1.421), train_loss = 1.94626821, grad/param norm = 3.0219e-01, time/batch = 0.4685s\u001b[0m\t\n",
            "\u001b[0m602/2115 (epoch 1.423), train_loss = 1.93530591, grad/param norm = 2.8278e-01, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m603/2115 (epoch 1.426), train_loss = 1.92708734, grad/param norm = 1.8900e-01, time/batch = 0.4254s\u001b[0m\t\n",
            "\u001b[0m604/2115 (epoch 1.428), train_loss = 1.93017266, grad/param norm = 1.6675e-01, time/batch = 0.4362s\u001b[0m\t\n",
            "\u001b[0m605/2115 (epoch 1.430), train_loss = 1.98390851, grad/param norm = 2.0521e-01, time/batch = 0.4222s\u001b[0m\t\n",
            "\u001b[0m606/2115 (epoch 1.433), train_loss = 1.98282967, grad/param norm = 2.8277e-01, time/batch = 0.4478s\u001b[0m\t\n",
            "\u001b[0m607/2115 (epoch 1.435), train_loss = 1.97508391, grad/param norm = 2.5647e-01, time/batch = 0.4272s\u001b[0m\t\n",
            "\u001b[0m608/2115 (epoch 1.437), train_loss = 1.87332651, grad/param norm = 2.1037e-01, time/batch = 0.4169s\u001b[0m\t\n",
            "\u001b[0m609/2115 (epoch 1.440), train_loss = 1.92597767, grad/param norm = 2.2403e-01, time/batch = 0.4273s\u001b[0m\t\n",
            "\u001b[0m610/2115 (epoch 1.442), train_loss = 1.92921215, grad/param norm = 2.9447e-01, time/batch = 0.4155s\u001b[0m\t\n",
            "\u001b[0m611/2115 (epoch 1.444), train_loss = 1.99140672, grad/param norm = 3.5818e-01, time/batch = 0.4500s\u001b[0m\t\n",
            "\u001b[0m612/2115 (epoch 1.447), train_loss = 1.96886050, grad/param norm = 3.2556e-01, time/batch = 0.4324s\u001b[0m\t\n",
            "\u001b[0m613/2115 (epoch 1.449), train_loss = 1.87780891, grad/param norm = 2.5022e-01, time/batch = 0.4613s\u001b[0m\t\n",
            "\u001b[0m614/2115 (epoch 1.452), train_loss = 1.96555021, grad/param norm = 2.6306e-01, time/batch = 0.4095s\u001b[0m\t\n",
            "\u001b[0m615/2115 (epoch 1.454), train_loss = 1.87544823, grad/param norm = 2.6203e-01, time/batch = 0.4196s\u001b[0m\t\n",
            "\u001b[0m616/2115 (epoch 1.456), train_loss = 1.90911999, grad/param norm = 2.5495e-01, time/batch = 0.5190s\u001b[0m\t\n",
            "\u001b[0m617/2115 (epoch 1.459), train_loss = 1.90664401, grad/param norm = 2.5078e-01, time/batch = 0.8438s\u001b[0m\t\n",
            "\u001b[0m618/2115 (epoch 1.461), train_loss = 1.93150369, grad/param norm = 2.5722e-01, time/batch = 0.9722s\u001b[0m\t\n",
            "\u001b[0m619/2115 (epoch 1.463), train_loss = 1.92627742, grad/param norm = 3.2442e-01, time/batch = 0.8327s\u001b[0m\t\n",
            "\u001b[0m620/2115 (epoch 1.466), train_loss = 1.92834945, grad/param norm = 3.2759e-01, time/batch = 0.6901s\u001b[0m\t\n",
            "\u001b[0m621/2115 (epoch 1.468), train_loss = 1.96478648, grad/param norm = 2.5712e-01, time/batch = 0.4423s\u001b[0m\t\n",
            "\u001b[0m622/2115 (epoch 1.470), train_loss = 1.98592846, grad/param norm = 2.0269e-01, time/batch = 0.4225s\u001b[0m\t\n",
            "\u001b[0m623/2115 (epoch 1.473), train_loss = 1.94914795, grad/param norm = 1.8109e-01, time/batch = 0.4178s\u001b[0m\t\n",
            "\u001b[0m624/2115 (epoch 1.475), train_loss = 1.96328106, grad/param norm = 1.7757e-01, time/batch = 0.4421s\u001b[0m\t\n",
            "\u001b[0m625/2115 (epoch 1.478), train_loss = 1.93908355, grad/param norm = 1.9266e-01, time/batch = 0.4230s\u001b[0m\t\n",
            "\u001b[0m626/2115 (epoch 1.480), train_loss = 1.94971653, grad/param norm = 2.0549e-01, time/batch = 0.4487s\u001b[0m\t\n",
            "\u001b[0m627/2115 (epoch 1.482), train_loss = 1.92031398, grad/param norm = 2.3994e-01, time/batch = 0.4290s\u001b[0m\t\n",
            "\u001b[0m628/2115 (epoch 1.485), train_loss = 1.93544355, grad/param norm = 2.2551e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m629/2115 (epoch 1.487), train_loss = 1.95644828, grad/param norm = 1.9817e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m630/2115 (epoch 1.489), train_loss = 1.95770208, grad/param norm = 2.3700e-01, time/batch = 0.4160s\u001b[0m\t\n",
            "\u001b[0m631/2115 (epoch 1.492), train_loss = 1.96301774, grad/param norm = 2.9622e-01, time/batch = 0.4463s\u001b[0m\t\n",
            "\u001b[0m632/2115 (epoch 1.494), train_loss = 2.03371828, grad/param norm = 3.1486e-01, time/batch = 0.4181s\u001b[0m\t\n",
            "\u001b[0m633/2115 (epoch 1.496), train_loss = 1.97385458, grad/param norm = 2.8595e-01, time/batch = 0.4875s\u001b[0m\t\n",
            "\u001b[0m634/2115 (epoch 1.499), train_loss = 1.93962270, grad/param norm = 2.4380e-01, time/batch = 0.4843s\u001b[0m\t\n",
            "\u001b[0m635/2115 (epoch 1.501), train_loss = 1.91320202, grad/param norm = 3.0152e-01, time/batch = 0.4157s\u001b[0m\t\n",
            "\u001b[0m636/2115 (epoch 1.504), train_loss = 1.94426687, grad/param norm = 2.8661e-01, time/batch = 0.4378s\u001b[0m\t\n",
            "\u001b[0m637/2115 (epoch 1.506), train_loss = 1.95497489, grad/param norm = 1.9011e-01, time/batch = 0.4146s\u001b[0m\t\n",
            "\u001b[0m638/2115 (epoch 1.508), train_loss = 1.85771528, grad/param norm = 2.1469e-01, time/batch = 0.4380s\u001b[0m\t\n",
            "\u001b[0m639/2115 (epoch 1.511), train_loss = 1.91836337, grad/param norm = 2.0574e-01, time/batch = 0.4278s\u001b[0m\t\n",
            "\u001b[0m640/2115 (epoch 1.513), train_loss = 1.94421505, grad/param norm = 1.9664e-01, time/batch = 0.4449s\u001b[0m\t\n",
            "\u001b[0m641/2115 (epoch 1.515), train_loss = 1.90315944, grad/param norm = 1.9139e-01, time/batch = 0.4242s\u001b[0m\t\n",
            "\u001b[0m642/2115 (epoch 1.518), train_loss = 1.90951269, grad/param norm = 2.2148e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m643/2115 (epoch 1.520), train_loss = 1.92854888, grad/param norm = 2.5598e-01, time/batch = 0.6782s\u001b[0m\t\n",
            "\u001b[0m644/2115 (epoch 1.522), train_loss = 1.92270113, grad/param norm = 2.7875e-01, time/batch = 0.9055s\u001b[0m\t\n",
            "\u001b[0m645/2115 (epoch 1.525), train_loss = 1.98509813, grad/param norm = 2.9823e-01, time/batch = 0.9446s\u001b[0m\t\n",
            "\u001b[0m646/2115 (epoch 1.527), train_loss = 1.96193273, grad/param norm = 3.5999e-01, time/batch = 0.7089s\u001b[0m\t\n",
            "\u001b[0m647/2115 (epoch 1.530), train_loss = 1.92232656, grad/param norm = 3.1724e-01, time/batch = 0.5672s\u001b[0m\t\n",
            "\u001b[0m648/2115 (epoch 1.532), train_loss = 1.90489723, grad/param norm = 3.1226e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m649/2115 (epoch 1.534), train_loss = 1.89586877, grad/param norm = 2.5984e-01, time/batch = 0.4379s\u001b[0m\t\n",
            "\u001b[0m650/2115 (epoch 1.537), train_loss = 1.99388340, grad/param norm = 2.1653e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m651/2115 (epoch 1.539), train_loss = 1.89099998, grad/param norm = 1.8621e-01, time/batch = 0.4497s\u001b[0m\t\n",
            "\u001b[0m652/2115 (epoch 1.541), train_loss = 1.92901086, grad/param norm = 2.3061e-01, time/batch = 0.4294s\u001b[0m\t\n",
            "\u001b[0m653/2115 (epoch 1.544), train_loss = 1.90766053, grad/param norm = 2.4020e-01, time/batch = 0.4418s\u001b[0m\t\n",
            "\u001b[0m654/2115 (epoch 1.546), train_loss = 1.95665811, grad/param norm = 2.1200e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m655/2115 (epoch 1.548), train_loss = 1.91009747, grad/param norm = 1.7539e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m656/2115 (epoch 1.551), train_loss = 1.90677361, grad/param norm = 2.0161e-01, time/batch = 0.4398s\u001b[0m\t\n",
            "\u001b[0m657/2115 (epoch 1.553), train_loss = 1.94539914, grad/param norm = 2.2804e-01, time/batch = 0.4254s\u001b[0m\t\n",
            "\u001b[0m658/2115 (epoch 1.556), train_loss = 1.91652076, grad/param norm = 2.2712e-01, time/batch = 0.4422s\u001b[0m\t\n",
            "\u001b[0m659/2115 (epoch 1.558), train_loss = 1.93830947, grad/param norm = 2.3696e-01, time/batch = 0.4134s\u001b[0m\t\n",
            "\u001b[0m660/2115 (epoch 1.560), train_loss = 1.93397445, grad/param norm = 2.3299e-01, time/batch = 0.4138s\u001b[0m\t\n",
            "\u001b[0m661/2115 (epoch 1.563), train_loss = 1.89323825, grad/param norm = 2.0856e-01, time/batch = 0.4268s\u001b[0m\t\n",
            "\u001b[0m662/2115 (epoch 1.565), train_loss = 1.90953110, grad/param norm = 2.1513e-01, time/batch = 0.4171s\u001b[0m\t\n",
            "\u001b[0m663/2115 (epoch 1.567), train_loss = 1.90532589, grad/param norm = 2.0443e-01, time/batch = 0.4405s\u001b[0m\t\n",
            "\u001b[0m664/2115 (epoch 1.570), train_loss = 1.92885604, grad/param norm = 2.1328e-01, time/batch = 0.4166s\u001b[0m\t\n",
            "\u001b[0m665/2115 (epoch 1.572), train_loss = 1.88389205, grad/param norm = 2.4238e-01, time/batch = 0.4382s\u001b[0m\t\n",
            "\u001b[0m666/2115 (epoch 1.574), train_loss = 1.90435408, grad/param norm = 2.3867e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m667/2115 (epoch 1.577), train_loss = 1.92367692, grad/param norm = 2.6780e-01, time/batch = 0.4171s\u001b[0m\t\n",
            "\u001b[0m668/2115 (epoch 1.579), train_loss = 1.88417646, grad/param norm = 2.5209e-01, time/batch = 0.4282s\u001b[0m\t\n",
            "\u001b[0m669/2115 (epoch 1.582), train_loss = 1.88120622, grad/param norm = 2.7315e-01, time/batch = 0.4263s\u001b[0m\t\n",
            "\u001b[0m670/2115 (epoch 1.584), train_loss = 1.91612667, grad/param norm = 2.3241e-01, time/batch = 0.7444s\u001b[0m\t\n",
            "\u001b[0m671/2115 (epoch 1.586), train_loss = 1.89912048, grad/param norm = 1.9869e-01, time/batch = 1.0320s\u001b[0m\t\n",
            "\u001b[0m672/2115 (epoch 1.589), train_loss = 1.93572520, grad/param norm = 1.7728e-01, time/batch = 0.8994s\u001b[0m\t\n",
            "\u001b[0m673/2115 (epoch 1.591), train_loss = 1.84471327, grad/param norm = 2.0661e-01, time/batch = 0.7323s\u001b[0m\t\n",
            "\u001b[0m674/2115 (epoch 1.593), train_loss = 1.92430626, grad/param norm = 1.6306e-01, time/batch = 0.4124s\u001b[0m\t\n",
            "\u001b[0m675/2115 (epoch 1.596), train_loss = 1.83367908, grad/param norm = 1.6779e-01, time/batch = 0.4133s\u001b[0m\t\n",
            "\u001b[0m676/2115 (epoch 1.598), train_loss = 1.91940364, grad/param norm = 1.8002e-01, time/batch = 0.4337s\u001b[0m\t\n",
            "\u001b[0m677/2115 (epoch 1.600), train_loss = 1.90575041, grad/param norm = 2.0112e-01, time/batch = 0.4174s\u001b[0m\t\n",
            "\u001b[0m678/2115 (epoch 1.603), train_loss = 1.87089292, grad/param norm = 2.2870e-01, time/batch = 0.4385s\u001b[0m\t\n",
            "\u001b[0m679/2115 (epoch 1.605), train_loss = 1.91948217, grad/param norm = 2.5282e-01, time/batch = 0.4278s\u001b[0m\t\n",
            "\u001b[0m680/2115 (epoch 1.608), train_loss = 1.91936519, grad/param norm = 2.8352e-01, time/batch = 0.4187s\u001b[0m\t\n",
            "\u001b[0m681/2115 (epoch 1.610), train_loss = 1.89527548, grad/param norm = 2.9480e-01, time/batch = 0.4385s\u001b[0m\t\n",
            "\u001b[0m682/2115 (epoch 1.612), train_loss = 1.93827340, grad/param norm = 3.0694e-01, time/batch = 0.4229s\u001b[0m\t\n",
            "\u001b[0m683/2115 (epoch 1.615), train_loss = 1.90763987, grad/param norm = 3.1822e-01, time/batch = 0.4474s\u001b[0m\t\n",
            "\u001b[0m684/2115 (epoch 1.617), train_loss = 1.88216106, grad/param norm = 3.0812e-01, time/batch = 0.4140s\u001b[0m\t\n",
            "\u001b[0m685/2115 (epoch 1.619), train_loss = 1.85714683, grad/param norm = 2.3714e-01, time/batch = 0.4335s\u001b[0m\t\n",
            "\u001b[0m686/2115 (epoch 1.622), train_loss = 1.86132015, grad/param norm = 2.2747e-01, time/batch = 0.4207s\u001b[0m\t\n",
            "\u001b[0m687/2115 (epoch 1.624), train_loss = 1.88341919, grad/param norm = 2.5694e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m688/2115 (epoch 1.626), train_loss = 1.86204399, grad/param norm = 2.4720e-01, time/batch = 0.4340s\u001b[0m\t\n",
            "\u001b[0m689/2115 (epoch 1.629), train_loss = 1.92899661, grad/param norm = 2.0975e-01, time/batch = 0.4159s\u001b[0m\t\n",
            "\u001b[0m690/2115 (epoch 1.631), train_loss = 1.93406292, grad/param norm = 1.6371e-01, time/batch = 0.4451s\u001b[0m\t\n",
            "\u001b[0m691/2115 (epoch 1.634), train_loss = 1.87558208, grad/param norm = 1.5742e-01, time/batch = 0.4238s\u001b[0m\t\n",
            "\u001b[0m692/2115 (epoch 1.636), train_loss = 1.84409034, grad/param norm = 1.6483e-01, time/batch = 0.4191s\u001b[0m\t\n",
            "\u001b[0m693/2115 (epoch 1.638), train_loss = 1.86526996, grad/param norm = 1.6999e-01, time/batch = 0.4462s\u001b[0m\t\n",
            "\u001b[0m694/2115 (epoch 1.641), train_loss = 1.87949836, grad/param norm = 1.7959e-01, time/batch = 0.4271s\u001b[0m\t\n",
            "\u001b[0m695/2115 (epoch 1.643), train_loss = 1.95568009, grad/param norm = 2.5536e-01, time/batch = 0.4478s\u001b[0m\t\n",
            "\u001b[0m696/2115 (epoch 1.645), train_loss = 1.86938890, grad/param norm = 2.9386e-01, time/batch = 0.5629s\u001b[0m\t\n",
            "\u001b[0m697/2115 (epoch 1.648), train_loss = 1.94406572, grad/param norm = 2.4997e-01, time/batch = 1.0231s\u001b[0m\t\n",
            "\u001b[0m698/2115 (epoch 1.650), train_loss = 1.87845577, grad/param norm = 2.0168e-01, time/batch = 1.0211s\u001b[0m\t\n",
            "\u001b[0m699/2115 (epoch 1.652), train_loss = 1.91375233, grad/param norm = 1.8034e-01, time/batch = 0.8494s\u001b[0m\t\n",
            "\u001b[0m700/2115 (epoch 1.655), train_loss = 1.89520838, grad/param norm = 2.1129e-01, time/batch = 0.4154s\u001b[0m\t\n",
            "\u001b[0m701/2115 (epoch 1.657), train_loss = 1.90989061, grad/param norm = 2.1482e-01, time/batch = 0.4522s\u001b[0m\t\n",
            "\u001b[0m702/2115 (epoch 1.660), train_loss = 1.88665563, grad/param norm = 1.8607e-01, time/batch = 0.4239s\u001b[0m\t\n",
            "\u001b[0m703/2115 (epoch 1.662), train_loss = 1.92800476, grad/param norm = 2.0159e-01, time/batch = 0.4395s\u001b[0m\t\n",
            "\u001b[0m704/2115 (epoch 1.664), train_loss = 1.87606308, grad/param norm = 2.7951e-01, time/batch = 0.4188s\u001b[0m\t\n",
            "\u001b[0m705/2115 (epoch 1.667), train_loss = 1.97264163, grad/param norm = 3.2645e-01, time/batch = 0.4350s\u001b[0m\t\n",
            "\u001b[0m706/2115 (epoch 1.669), train_loss = 1.86896109, grad/param norm = 2.9257e-01, time/batch = 0.4142s\u001b[0m\t\n",
            "\u001b[0m707/2115 (epoch 1.671), train_loss = 1.89256207, grad/param norm = 2.8110e-01, time/batch = 0.4328s\u001b[0m\t\n",
            "\u001b[0m708/2115 (epoch 1.674), train_loss = 1.88386065, grad/param norm = 2.6250e-01, time/batch = 0.4359s\u001b[0m\t\n",
            "\u001b[0m709/2115 (epoch 1.676), train_loss = 1.87003555, grad/param norm = 1.9450e-01, time/batch = 0.4104s\u001b[0m\t\n",
            "\u001b[0m710/2115 (epoch 1.678), train_loss = 1.84413615, grad/param norm = 1.8615e-01, time/batch = 0.4449s\u001b[0m\t\n",
            "\u001b[0m711/2115 (epoch 1.681), train_loss = 1.90877590, grad/param norm = 2.2736e-01, time/batch = 0.4203s\u001b[0m\t\n",
            "\u001b[0m712/2115 (epoch 1.683), train_loss = 1.90293592, grad/param norm = 1.9092e-01, time/batch = 0.4171s\u001b[0m\t\n",
            "\u001b[0m713/2115 (epoch 1.686), train_loss = 1.86143087, grad/param norm = 1.8205e-01, time/batch = 0.4415s\u001b[0m\t\n",
            "\u001b[0m714/2115 (epoch 1.688), train_loss = 1.86713456, grad/param norm = 1.9771e-01, time/batch = 0.4153s\u001b[0m\t\n",
            "\u001b[0m715/2115 (epoch 1.690), train_loss = 1.86635843, grad/param norm = 2.0285e-01, time/batch = 0.4384s\u001b[0m\t\n",
            "\u001b[0m716/2115 (epoch 1.693), train_loss = 1.84617061, grad/param norm = 2.0128e-01, time/batch = 0.4162s\u001b[0m\t\n",
            "\u001b[0m717/2115 (epoch 1.695), train_loss = 1.92363842, grad/param norm = 2.0185e-01, time/batch = 0.4260s\u001b[0m\t\n",
            "\u001b[0m718/2115 (epoch 1.697), train_loss = 1.92031727, grad/param norm = 1.9344e-01, time/batch = 0.4341s\u001b[0m\t\n",
            "\u001b[0m719/2115 (epoch 1.700), train_loss = 1.88811430, grad/param norm = 2.0024e-01, time/batch = 0.4193s\u001b[0m\t\n",
            "\u001b[0m720/2115 (epoch 1.702), train_loss = 1.91293812, grad/param norm = 1.8107e-01, time/batch = 0.4270s\u001b[0m\t\n",
            "\u001b[0m721/2115 (epoch 1.704), train_loss = 1.86082492, grad/param norm = 2.1682e-01, time/batch = 0.4305s\u001b[0m\t\n",
            "\u001b[0m722/2115 (epoch 1.707), train_loss = 1.92664224, grad/param norm = 2.4149e-01, time/batch = 0.5720s\u001b[0m\t\n",
            "\u001b[0m723/2115 (epoch 1.709), train_loss = 1.80545060, grad/param norm = 2.6334e-01, time/batch = 1.0501s\u001b[0m\t\n",
            "\u001b[0m724/2115 (epoch 1.712), train_loss = 1.81295357, grad/param norm = 2.2330e-01, time/batch = 0.9449s\u001b[0m\t\n",
            "\u001b[0m725/2115 (epoch 1.714), train_loss = 1.86435109, grad/param norm = 1.7909e-01, time/batch = 0.7874s\u001b[0m\t\n",
            "\u001b[0m726/2115 (epoch 1.716), train_loss = 1.81266149, grad/param norm = 1.8994e-01, time/batch = 0.4843s\u001b[0m\t\n",
            "\u001b[0m727/2115 (epoch 1.719), train_loss = 1.79508487, grad/param norm = 2.1504e-01, time/batch = 0.4185s\u001b[0m\t\n",
            "\u001b[0m728/2115 (epoch 1.721), train_loss = 1.91101234, grad/param norm = 2.2962e-01, time/batch = 0.4472s\u001b[0m\t\n",
            "\u001b[0m729/2115 (epoch 1.723), train_loss = 1.90757242, grad/param norm = 2.4762e-01, time/batch = 0.4278s\u001b[0m\t\n",
            "\u001b[0m730/2115 (epoch 1.726), train_loss = 1.82109995, grad/param norm = 2.4387e-01, time/batch = 0.4361s\u001b[0m\t\n",
            "\u001b[0m731/2115 (epoch 1.728), train_loss = 1.87481328, grad/param norm = 2.0690e-01, time/batch = 0.4613s\u001b[0m\t\n",
            "\u001b[0m732/2115 (epoch 1.730), train_loss = 1.92705489, grad/param norm = 1.6822e-01, time/batch = 0.4108s\u001b[0m\t\n",
            "\u001b[0m733/2115 (epoch 1.733), train_loss = 1.86638658, grad/param norm = 1.5491e-01, time/batch = 0.4435s\u001b[0m\t\n",
            "\u001b[0m734/2115 (epoch 1.735), train_loss = 1.85120329, grad/param norm = 1.9403e-01, time/batch = 0.4280s\u001b[0m\t\n",
            "\u001b[0m735/2115 (epoch 1.738), train_loss = 1.94071642, grad/param norm = 2.6942e-01, time/batch = 0.4370s\u001b[0m\t\n",
            "\u001b[0m736/2115 (epoch 1.740), train_loss = 1.92161827, grad/param norm = 2.8096e-01, time/batch = 0.4179s\u001b[0m\t\n",
            "\u001b[0m737/2115 (epoch 1.742), train_loss = 1.91263556, grad/param norm = 3.1764e-01, time/batch = 0.4468s\u001b[0m\t\n",
            "\u001b[0m738/2115 (epoch 1.745), train_loss = 1.83078075, grad/param norm = 2.4427e-01, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m739/2115 (epoch 1.747), train_loss = 1.87517191, grad/param norm = 1.7857e-01, time/batch = 0.4166s\u001b[0m\t\n",
            "\u001b[0m740/2115 (epoch 1.749), train_loss = 1.86034188, grad/param norm = 2.0437e-01, time/batch = 0.4418s\u001b[0m\t\n",
            "\u001b[0m741/2115 (epoch 1.752), train_loss = 1.89744388, grad/param norm = 2.2970e-01, time/batch = 0.4438s\u001b[0m\t\n",
            "\u001b[0m742/2115 (epoch 1.754), train_loss = 1.84510342, grad/param norm = 2.3157e-01, time/batch = 0.4618s\u001b[0m\t\n",
            "\u001b[0m743/2115 (epoch 1.757), train_loss = 1.83479404, grad/param norm = 2.3999e-01, time/batch = 0.4345s\u001b[0m\t\n",
            "\u001b[0m744/2115 (epoch 1.759), train_loss = 1.81287536, grad/param norm = 1.9946e-01, time/batch = 0.4437s\u001b[0m\t\n",
            "\u001b[0m745/2115 (epoch 1.761), train_loss = 1.81866350, grad/param norm = 1.5502e-01, time/batch = 0.4126s\u001b[0m\t\n",
            "\u001b[0m746/2115 (epoch 1.764), train_loss = 1.83808010, grad/param norm = 1.5184e-01, time/batch = 0.4144s\u001b[0m\t\n",
            "\u001b[0m747/2115 (epoch 1.766), train_loss = 1.81556188, grad/param norm = 1.6240e-01, time/batch = 0.4363s\u001b[0m\t\n",
            "\u001b[0m748/2115 (epoch 1.768), train_loss = 1.82253634, grad/param norm = 1.7193e-01, time/batch = 0.4458s\u001b[0m\t\n",
            "\u001b[0m749/2115 (epoch 1.771), train_loss = 1.82995589, grad/param norm = 1.6265e-01, time/batch = 1.0257s\u001b[0m\t\n",
            "\u001b[0m750/2115 (epoch 1.773), train_loss = 1.82027293, grad/param norm = 1.7405e-01, time/batch = 0.9129s\u001b[0m\t\n",
            "\u001b[0m751/2115 (epoch 1.775), train_loss = 1.80998690, grad/param norm = 1.8826e-01, time/batch = 0.8509s\u001b[0m\t\n",
            "\u001b[0m752/2115 (epoch 1.778), train_loss = 1.88234241, grad/param norm = 2.2321e-01, time/batch = 0.5767s\u001b[0m\t\n",
            "\u001b[0m753/2115 (epoch 1.780), train_loss = 1.82605238, grad/param norm = 2.2736e-01, time/batch = 0.4336s\u001b[0m\t\n",
            "\u001b[0m754/2115 (epoch 1.783), train_loss = 1.82595780, grad/param norm = 2.0712e-01, time/batch = 0.4289s\u001b[0m\t\n",
            "\u001b[0m755/2115 (epoch 1.785), train_loss = 1.87529637, grad/param norm = 1.8934e-01, time/batch = 0.4294s\u001b[0m\t\n",
            "\u001b[0m756/2115 (epoch 1.787), train_loss = 1.84479394, grad/param norm = 1.6875e-01, time/batch = 0.4182s\u001b[0m\t\n",
            "\u001b[0m757/2115 (epoch 1.790), train_loss = 1.79706873, grad/param norm = 1.9720e-01, time/batch = 0.5065s\u001b[0m\t\n",
            "\u001b[0m758/2115 (epoch 1.792), train_loss = 1.83256001, grad/param norm = 2.3931e-01, time/batch = 0.4282s\u001b[0m\t\n",
            "\u001b[0m759/2115 (epoch 1.794), train_loss = 1.84639987, grad/param norm = 3.2833e-01, time/batch = 0.4169s\u001b[0m\t\n",
            "\u001b[0m760/2115 (epoch 1.797), train_loss = 1.89553347, grad/param norm = 3.8344e-01, time/batch = 0.4492s\u001b[0m\t\n",
            "\u001b[0m761/2115 (epoch 1.799), train_loss = 1.87859056, grad/param norm = 4.0794e-01, time/batch = 0.4262s\u001b[0m\t\n",
            "\u001b[0m762/2115 (epoch 1.801), train_loss = 1.95884402, grad/param norm = 2.8075e-01, time/batch = 0.4370s\u001b[0m\t\n",
            "\u001b[0m763/2115 (epoch 1.804), train_loss = 1.79887269, grad/param norm = 2.0862e-01, time/batch = 0.4275s\u001b[0m\t\n",
            "\u001b[0m764/2115 (epoch 1.806), train_loss = 1.86700733, grad/param norm = 2.0469e-01, time/batch = 0.4371s\u001b[0m\t\n",
            "\u001b[0m765/2115 (epoch 1.809), train_loss = 1.77856826, grad/param norm = 1.9693e-01, time/batch = 0.4376s\u001b[0m\t\n",
            "\u001b[0m766/2115 (epoch 1.811), train_loss = 1.85948729, grad/param norm = 2.0741e-01, time/batch = 0.4340s\u001b[0m\t\n",
            "\u001b[0m767/2115 (epoch 1.813), train_loss = 1.83452692, grad/param norm = 1.8735e-01, time/batch = 0.4426s\u001b[0m\t\n",
            "\u001b[0m768/2115 (epoch 1.816), train_loss = 1.76820437, grad/param norm = 1.5222e-01, time/batch = 0.4284s\u001b[0m\t\n",
            "\u001b[0m769/2115 (epoch 1.818), train_loss = 1.85875779, grad/param norm = 1.5326e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m770/2115 (epoch 1.820), train_loss = 1.81776758, grad/param norm = 1.4583e-01, time/batch = 0.4289s\u001b[0m\t\n",
            "\u001b[0m771/2115 (epoch 1.823), train_loss = 1.88163847, grad/param norm = 1.7847e-01, time/batch = 0.4456s\u001b[0m\t\n",
            "\u001b[0m772/2115 (epoch 1.825), train_loss = 1.80259239, grad/param norm = 1.7173e-01, time/batch = 0.4255s\u001b[0m\t\n",
            "\u001b[0m773/2115 (epoch 1.827), train_loss = 1.90531723, grad/param norm = 2.0374e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m774/2115 (epoch 1.830), train_loss = 1.87427719, grad/param norm = 2.5065e-01, time/batch = 0.4360s\u001b[0m\t\n",
            "\u001b[0m775/2115 (epoch 1.832), train_loss = 1.87765344, grad/param norm = 2.7960e-01, time/batch = 0.7996s\u001b[0m\t\n",
            "\u001b[0m776/2115 (epoch 1.835), train_loss = 1.82883080, grad/param norm = 2.4305e-01, time/batch = 1.0542s\u001b[0m\t\n",
            "\u001b[0m777/2115 (epoch 1.837), train_loss = 1.77354831, grad/param norm = 2.1573e-01, time/batch = 0.8899s\u001b[0m\t\n",
            "\u001b[0m778/2115 (epoch 1.839), train_loss = 1.77475121, grad/param norm = 2.0295e-01, time/batch = 0.6870s\u001b[0m\t\n",
            "\u001b[0m779/2115 (epoch 1.842), train_loss = 1.84153005, grad/param norm = 2.0571e-01, time/batch = 0.4598s\u001b[0m\t\n",
            "\u001b[0m780/2115 (epoch 1.844), train_loss = 1.85692011, grad/param norm = 1.9074e-01, time/batch = 0.4504s\u001b[0m\t\n",
            "\u001b[0m781/2115 (epoch 1.846), train_loss = 1.86694752, grad/param norm = 1.7336e-01, time/batch = 0.4349s\u001b[0m\t\n",
            "\u001b[0m782/2115 (epoch 1.849), train_loss = 1.77975990, grad/param norm = 1.4005e-01, time/batch = 0.4327s\u001b[0m\t\n",
            "\u001b[0m783/2115 (epoch 1.851), train_loss = 1.79096053, grad/param norm = 1.6489e-01, time/batch = 0.4143s\u001b[0m\t\n",
            "\u001b[0m784/2115 (epoch 1.853), train_loss = 1.84649439, grad/param norm = 1.6117e-01, time/batch = 0.4373s\u001b[0m\t\n",
            "\u001b[0m785/2115 (epoch 1.856), train_loss = 1.80597730, grad/param norm = 2.0906e-01, time/batch = 0.4158s\u001b[0m\t\n",
            "\u001b[0m786/2115 (epoch 1.858), train_loss = 1.81639426, grad/param norm = 1.8676e-01, time/batch = 0.4220s\u001b[0m\t\n",
            "\u001b[0m787/2115 (epoch 1.861), train_loss = 1.84579802, grad/param norm = 2.1092e-01, time/batch = 0.4603s\u001b[0m\t\n",
            "\u001b[0m788/2115 (epoch 1.863), train_loss = 1.83755745, grad/param norm = 2.0872e-01, time/batch = 0.4281s\u001b[0m\t\n",
            "\u001b[0m789/2115 (epoch 1.865), train_loss = 1.83061543, grad/param norm = 2.0756e-01, time/batch = 0.5007s\u001b[0m\t\n",
            "\u001b[0m790/2115 (epoch 1.868), train_loss = 1.80992252, grad/param norm = 2.0935e-01, time/batch = 0.4205s\u001b[0m\t\n",
            "\u001b[0m791/2115 (epoch 1.870), train_loss = 1.80020156, grad/param norm = 2.5766e-01, time/batch = 0.5226s\u001b[0m\t\n",
            "\u001b[0m792/2115 (epoch 1.872), train_loss = 1.84763808, grad/param norm = 3.1144e-01, time/batch = 0.4161s\u001b[0m\t\n",
            "\u001b[0m793/2115 (epoch 1.875), train_loss = 1.83211384, grad/param norm = 3.0151e-01, time/batch = 0.4496s\u001b[0m\t\n",
            "\u001b[0m794/2115 (epoch 1.877), train_loss = 1.94934342, grad/param norm = 2.1244e-01, time/batch = 0.4147s\u001b[0m\t\n",
            "\u001b[0m795/2115 (epoch 1.879), train_loss = 1.80260995, grad/param norm = 1.9147e-01, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m796/2115 (epoch 1.882), train_loss = 1.84207245, grad/param norm = 1.8991e-01, time/batch = 0.4357s\u001b[0m\t\n",
            "\u001b[0m797/2115 (epoch 1.884), train_loss = 1.84748289, grad/param norm = 2.2260e-01, time/batch = 0.4755s\u001b[0m\t\n",
            "\u001b[0m798/2115 (epoch 1.887), train_loss = 1.82483008, grad/param norm = 1.9409e-01, time/batch = 0.4695s\u001b[0m\t\n",
            "\u001b[0m799/2115 (epoch 1.889), train_loss = 1.82986536, grad/param norm = 1.6510e-01, time/batch = 0.4181s\u001b[0m\t\n",
            "\u001b[0m800/2115 (epoch 1.891), train_loss = 1.83697890, grad/param norm = 1.4657e-01, time/batch = 0.4169s\u001b[0m\t\n",
            "\u001b[0m801/2115 (epoch 1.894), train_loss = 1.78439063, grad/param norm = 1.5061e-01, time/batch = 0.7745s\u001b[0m\t\n",
            "\u001b[0m802/2115 (epoch 1.896), train_loss = 1.80542736, grad/param norm = 1.7391e-01, time/batch = 1.0923s\u001b[0m\t\n",
            "\u001b[0m803/2115 (epoch 1.898), train_loss = 1.82670698, grad/param norm = 2.1934e-01, time/batch = 0.8532s\u001b[0m\t\n",
            "\u001b[0m804/2115 (epoch 1.901), train_loss = 1.83563270, grad/param norm = 2.2764e-01, time/batch = 0.6459s\u001b[0m\t\n",
            "\u001b[0m805/2115 (epoch 1.903), train_loss = 1.83909284, grad/param norm = 2.1953e-01, time/batch = 0.4150s\u001b[0m\t\n",
            "\u001b[0m806/2115 (epoch 1.905), train_loss = 1.86134550, grad/param norm = 2.0285e-01, time/batch = 0.4382s\u001b[0m\t\n",
            "\u001b[0m807/2115 (epoch 1.908), train_loss = 1.83687814, grad/param norm = 1.8112e-01, time/batch = 0.4143s\u001b[0m\t\n",
            "\u001b[0m808/2115 (epoch 1.910), train_loss = 1.83013928, grad/param norm = 2.0017e-01, time/batch = 0.4217s\u001b[0m\t\n",
            "\u001b[0m809/2115 (epoch 1.913), train_loss = 1.88110277, grad/param norm = 2.5832e-01, time/batch = 0.4546s\u001b[0m\t\n",
            "\u001b[0m810/2115 (epoch 1.915), train_loss = 1.85118743, grad/param norm = 2.6820e-01, time/batch = 0.4113s\u001b[0m\t\n",
            "\u001b[0m811/2115 (epoch 1.917), train_loss = 1.84834626, grad/param norm = 2.3513e-01, time/batch = 0.4505s\u001b[0m\t\n",
            "\u001b[0m812/2115 (epoch 1.920), train_loss = 1.87671657, grad/param norm = 2.0977e-01, time/batch = 0.4398s\u001b[0m\t\n",
            "\u001b[0m813/2115 (epoch 1.922), train_loss = 1.80572403, grad/param norm = 1.6438e-01, time/batch = 0.4411s\u001b[0m\t\n",
            "\u001b[0m814/2115 (epoch 1.924), train_loss = 1.79427342, grad/param norm = 1.4064e-01, time/batch = 0.4154s\u001b[0m\t\n",
            "\u001b[0m815/2115 (epoch 1.927), train_loss = 1.83839436, grad/param norm = 1.6066e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m816/2115 (epoch 1.929), train_loss = 1.83078388, grad/param norm = 1.8463e-01, time/batch = 0.4686s\u001b[0m\t\n",
            "\u001b[0m817/2115 (epoch 1.931), train_loss = 1.85084092, grad/param norm = 1.5909e-01, time/batch = 0.4478s\u001b[0m\t\n",
            "\u001b[0m818/2115 (epoch 1.934), train_loss = 1.77553285, grad/param norm = 1.5119e-01, time/batch = 0.4434s\u001b[0m\t\n",
            "\u001b[0m819/2115 (epoch 1.936), train_loss = 1.85072368, grad/param norm = 1.9407e-01, time/batch = 0.4144s\u001b[0m\t\n",
            "\u001b[0m820/2115 (epoch 1.939), train_loss = 1.78053614, grad/param norm = 2.1301e-01, time/batch = 0.4919s\u001b[0m\t\n",
            "\u001b[0m821/2115 (epoch 1.941), train_loss = 1.77757006, grad/param norm = 2.5153e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m822/2115 (epoch 1.943), train_loss = 1.83014982, grad/param norm = 2.8585e-01, time/batch = 0.4143s\u001b[0m\t\n",
            "\u001b[0m823/2115 (epoch 1.946), train_loss = 1.78498443, grad/param norm = 2.7452e-01, time/batch = 0.4444s\u001b[0m\t\n",
            "\u001b[0m824/2115 (epoch 1.948), train_loss = 1.87594191, grad/param norm = 2.3709e-01, time/batch = 0.4145s\u001b[0m\t\n",
            "\u001b[0m825/2115 (epoch 1.950), train_loss = 1.83083556, grad/param norm = 2.4000e-01, time/batch = 0.4421s\u001b[0m\t\n",
            "\u001b[0m826/2115 (epoch 1.953), train_loss = 1.82429384, grad/param norm = 2.2094e-01, time/batch = 0.4197s\u001b[0m\t\n",
            "\u001b[0m827/2115 (epoch 1.955), train_loss = 1.84504298, grad/param norm = 1.9758e-01, time/batch = 0.6849s\u001b[0m\t\n",
            "\u001b[0m828/2115 (epoch 1.957), train_loss = 1.82604090, grad/param norm = 1.8011e-01, time/batch = 0.8527s\u001b[0m\t\n",
            "\u001b[0m829/2115 (epoch 1.960), train_loss = 1.83234565, grad/param norm = 1.7233e-01, time/batch = 1.0829s\u001b[0m\t\n",
            "\u001b[0m830/2115 (epoch 1.962), train_loss = 1.84643318, grad/param norm = 1.7999e-01, time/batch = 0.9999s\u001b[0m\t\n",
            "\u001b[0m831/2115 (epoch 1.965), train_loss = 1.79195674, grad/param norm = 2.2707e-01, time/batch = 0.4175s\u001b[0m\t\n",
            "\u001b[0m832/2115 (epoch 1.967), train_loss = 1.88087412, grad/param norm = 2.7150e-01, time/batch = 0.4198s\u001b[0m\t\n",
            "\u001b[0m833/2115 (epoch 1.969), train_loss = 1.82911090, grad/param norm = 2.6841e-01, time/batch = 0.4418s\u001b[0m\t\n",
            "\u001b[0m834/2115 (epoch 1.972), train_loss = 1.85933031, grad/param norm = 2.3764e-01, time/batch = 0.4153s\u001b[0m\t\n",
            "\u001b[0m835/2115 (epoch 1.974), train_loss = 1.79009242, grad/param norm = 1.9713e-01, time/batch = 0.4516s\u001b[0m\t\n",
            "\u001b[0m836/2115 (epoch 1.976), train_loss = 1.82870772, grad/param norm = 1.8669e-01, time/batch = 0.4155s\u001b[0m\t\n",
            "\u001b[0m837/2115 (epoch 1.979), train_loss = 1.78392576, grad/param norm = 1.8384e-01, time/batch = 0.4205s\u001b[0m\t\n",
            "\u001b[0m838/2115 (epoch 1.981), train_loss = 1.78213678, grad/param norm = 1.5368e-01, time/batch = 0.4434s\u001b[0m\t\n",
            "\u001b[0m839/2115 (epoch 1.983), train_loss = 1.80993291, grad/param norm = 1.4243e-01, time/batch = 0.4272s\u001b[0m\t\n",
            "\u001b[0m840/2115 (epoch 1.986), train_loss = 1.79377563, grad/param norm = 1.4149e-01, time/batch = 0.4416s\u001b[0m\t\n",
            "\u001b[0m841/2115 (epoch 1.988), train_loss = 1.76550057, grad/param norm = 1.3164e-01, time/batch = 0.4306s\u001b[0m\t\n",
            "\u001b[0m842/2115 (epoch 1.991), train_loss = 1.79640417, grad/param norm = 1.5706e-01, time/batch = 0.4449s\u001b[0m\t\n",
            "\u001b[0m843/2115 (epoch 1.993), train_loss = 1.82686806, grad/param norm = 1.8577e-01, time/batch = 0.4201s\u001b[0m\t\n",
            "\u001b[0m844/2115 (epoch 1.995), train_loss = 1.81304421, grad/param norm = 2.0328e-01, time/batch = 0.4157s\u001b[0m\t\n",
            "\u001b[0m845/2115 (epoch 1.998), train_loss = 1.83634144, grad/param norm = 2.1048e-01, time/batch = 0.4340s\u001b[0m\t\n",
            "\u001b[0m846/2115 (epoch 2.000), train_loss = 1.86717706, grad/param norm = 2.3817e-01, time/batch = 0.4152s\u001b[0m\t\n",
            "\u001b[0m847/2115 (epoch 2.002), train_loss = 1.95729649, grad/param norm = 2.4764e-01, time/batch = 0.4350s\u001b[0m\t\n",
            "\u001b[0m848/2115 (epoch 2.005), train_loss = 1.81657465, grad/param norm = 2.3049e-01, time/batch = 0.4285s\u001b[0m\t\n",
            "\u001b[0m849/2115 (epoch 2.007), train_loss = 1.78721461, grad/param norm = 2.0068e-01, time/batch = 0.4423s\u001b[0m\t\n",
            "\u001b[0m850/2115 (epoch 2.009), train_loss = 1.80017276, grad/param norm = 1.6063e-01, time/batch = 0.4164s\u001b[0m\t\n",
            "\u001b[0m851/2115 (epoch 2.012), train_loss = 1.83384541, grad/param norm = 1.7513e-01, time/batch = 0.4340s\u001b[0m\t\n",
            "\u001b[0m852/2115 (epoch 2.014), train_loss = 1.83314877, grad/param norm = 2.1368e-01, time/batch = 0.4863s\u001b[0m\t\n",
            "\u001b[0m853/2115 (epoch 2.017), train_loss = 1.84884596, grad/param norm = 1.9092e-01, time/batch = 0.8374s\u001b[0m\t\n",
            "\u001b[0m854/2115 (epoch 2.019), train_loss = 1.83096513, grad/param norm = 1.5189e-01, time/batch = 1.0576s\u001b[0m\t\n",
            "\u001b[0m855/2115 (epoch 2.021), train_loss = 1.77210948, grad/param norm = 1.7302e-01, time/batch = 0.8397s\u001b[0m\t\n",
            "\u001b[0m856/2115 (epoch 2.024), train_loss = 1.69551978, grad/param norm = 1.7130e-01, time/batch = 0.6555s\u001b[0m\t\n",
            "\u001b[0m857/2115 (epoch 2.026), train_loss = 1.85174254, grad/param norm = 1.8032e-01, time/batch = 0.4275s\u001b[0m\t\n",
            "\u001b[0m858/2115 (epoch 2.028), train_loss = 1.76093846, grad/param norm = 1.8930e-01, time/batch = 0.4332s\u001b[0m\t\n",
            "\u001b[0m859/2115 (epoch 2.031), train_loss = 1.80085320, grad/param norm = 1.7458e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m860/2115 (epoch 2.033), train_loss = 1.75611297, grad/param norm = 1.7002e-01, time/batch = 0.4338s\u001b[0m\t\n",
            "\u001b[0m861/2115 (epoch 2.035), train_loss = 1.76527670, grad/param norm = 1.7828e-01, time/batch = 0.4319s\u001b[0m\t\n",
            "\u001b[0m862/2115 (epoch 2.038), train_loss = 1.73969260, grad/param norm = 1.8162e-01, time/batch = 0.4405s\u001b[0m\t\n",
            "\u001b[0m863/2115 (epoch 2.040), train_loss = 1.85176927, grad/param norm = 1.7892e-01, time/batch = 0.4205s\u001b[0m\t\n",
            "\u001b[0m864/2115 (epoch 2.043), train_loss = 1.77495748, grad/param norm = 1.5784e-01, time/batch = 0.4089s\u001b[0m\t\n",
            "\u001b[0m865/2115 (epoch 2.045), train_loss = 1.77139806, grad/param norm = 1.4907e-01, time/batch = 0.4281s\u001b[0m\t\n",
            "\u001b[0m866/2115 (epoch 2.047), train_loss = 1.77358294, grad/param norm = 1.5960e-01, time/batch = 0.4125s\u001b[0m\t\n",
            "\u001b[0m867/2115 (epoch 2.050), train_loss = 1.73864421, grad/param norm = 2.0554e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m868/2115 (epoch 2.052), train_loss = 1.77723928, grad/param norm = 2.8267e-01, time/batch = 0.4150s\u001b[0m\t\n",
            "\u001b[0m869/2115 (epoch 2.054), train_loss = 1.78575337, grad/param norm = 2.6341e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m870/2115 (epoch 2.057), train_loss = 1.75605716, grad/param norm = 1.9743e-01, time/batch = 0.4450s\u001b[0m\t\n",
            "\u001b[0m871/2115 (epoch 2.059), train_loss = 1.71578168, grad/param norm = 1.7216e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m872/2115 (epoch 2.061), train_loss = 1.78009027, grad/param norm = 1.7590e-01, time/batch = 0.4438s\u001b[0m\t\n",
            "\u001b[0m873/2115 (epoch 2.064), train_loss = 1.71066708, grad/param norm = 1.4226e-01, time/batch = 0.4181s\u001b[0m\t\n",
            "\u001b[0m874/2115 (epoch 2.066), train_loss = 1.75831730, grad/param norm = 1.5516e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m875/2115 (epoch 2.069), train_loss = 1.78073599, grad/param norm = 1.5226e-01, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m876/2115 (epoch 2.071), train_loss = 1.78742310, grad/param norm = 1.7053e-01, time/batch = 0.4161s\u001b[0m\t\n",
            "\u001b[0m877/2115 (epoch 2.073), train_loss = 1.78067402, grad/param norm = 2.0469e-01, time/batch = 0.4540s\u001b[0m\t\n",
            "\u001b[0m878/2115 (epoch 2.076), train_loss = 1.82643686, grad/param norm = 2.5848e-01, time/batch = 0.4235s\u001b[0m\t\n",
            "\u001b[0m879/2115 (epoch 2.078), train_loss = 1.75780226, grad/param norm = 2.5172e-01, time/batch = 0.5676s\u001b[0m\t\n",
            "\u001b[0m880/2115 (epoch 2.080), train_loss = 1.78109259, grad/param norm = 2.4118e-01, time/batch = 0.9020s\u001b[0m\t\n",
            "\u001b[0m881/2115 (epoch 2.083), train_loss = 1.81881232, grad/param norm = 2.4322e-01, time/batch = 1.0063s\u001b[0m\t\n",
            "\u001b[0m882/2115 (epoch 2.085), train_loss = 1.77609911, grad/param norm = 2.3122e-01, time/batch = 0.9025s\u001b[0m\t\n",
            "\u001b[0m883/2115 (epoch 2.087), train_loss = 1.88540236, grad/param norm = 2.0507e-01, time/batch = 0.4805s\u001b[0m\t\n",
            "\u001b[0m884/2115 (epoch 2.090), train_loss = 1.84598652, grad/param norm = 1.9270e-01, time/batch = 0.4339s\u001b[0m\t\n",
            "\u001b[0m885/2115 (epoch 2.092), train_loss = 1.79549029, grad/param norm = 1.6434e-01, time/batch = 0.4496s\u001b[0m\t\n",
            "\u001b[0m886/2115 (epoch 2.095), train_loss = 1.80784187, grad/param norm = 1.6692e-01, time/batch = 0.4197s\u001b[0m\t\n",
            "\u001b[0m887/2115 (epoch 2.097), train_loss = 1.82452761, grad/param norm = 1.6574e-01, time/batch = 0.4357s\u001b[0m\t\n",
            "\u001b[0m888/2115 (epoch 2.099), train_loss = 1.82394086, grad/param norm = 1.5206e-01, time/batch = 0.4076s\u001b[0m\t\n",
            "\u001b[0m889/2115 (epoch 2.102), train_loss = 1.76348723, grad/param norm = 1.4710e-01, time/batch = 0.4370s\u001b[0m\t\n",
            "\u001b[0m890/2115 (epoch 2.104), train_loss = 1.72448809, grad/param norm = 1.5364e-01, time/batch = 0.4143s\u001b[0m\t\n",
            "\u001b[0m891/2115 (epoch 2.106), train_loss = 1.79938664, grad/param norm = 1.6911e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m892/2115 (epoch 2.109), train_loss = 1.78858100, grad/param norm = 1.4864e-01, time/batch = 0.4313s\u001b[0m\t\n",
            "\u001b[0m893/2115 (epoch 2.111), train_loss = 1.75041158, grad/param norm = 1.6308e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m894/2115 (epoch 2.113), train_loss = 1.75654540, grad/param norm = 1.8342e-01, time/batch = 0.4331s\u001b[0m\t\n",
            "\u001b[0m895/2115 (epoch 2.116), train_loss = 1.77175429, grad/param norm = 2.0836e-01, time/batch = 0.4199s\u001b[0m\t\n",
            "\u001b[0m896/2115 (epoch 2.118), train_loss = 1.72398080, grad/param norm = 2.2351e-01, time/batch = 0.4204s\u001b[0m\t\n",
            "\u001b[0m897/2115 (epoch 2.121), train_loss = 1.75967665, grad/param norm = 1.9133e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m898/2115 (epoch 2.123), train_loss = 1.71089140, grad/param norm = 1.5922e-01, time/batch = 0.4166s\u001b[0m\t\n",
            "\u001b[0m899/2115 (epoch 2.125), train_loss = 1.71960539, grad/param norm = 1.6159e-01, time/batch = 0.4404s\u001b[0m\t\n",
            "\u001b[0m900/2115 (epoch 2.128), train_loss = 1.78481122, grad/param norm = 1.9082e-01, time/batch = 0.4197s\u001b[0m\t\n",
            "\u001b[0m901/2115 (epoch 2.130), train_loss = 1.76581218, grad/param norm = 2.4729e-01, time/batch = 0.4487s\u001b[0m\t\n",
            "\u001b[0m902/2115 (epoch 2.132), train_loss = 1.74152074, grad/param norm = 2.0691e-01, time/batch = 0.4270s\u001b[0m\t\n",
            "\u001b[0m903/2115 (epoch 2.135), train_loss = 1.71637010, grad/param norm = 1.5383e-01, time/batch = 0.4261s\u001b[0m\t\n",
            "\u001b[0m904/2115 (epoch 2.137), train_loss = 1.79124663, grad/param norm = 1.6056e-01, time/batch = 0.4437s\u001b[0m\t\n",
            "\u001b[0m905/2115 (epoch 2.139), train_loss = 1.77021451, grad/param norm = 1.9693e-01, time/batch = 0.4255s\u001b[0m\t\n",
            "\u001b[0m906/2115 (epoch 2.142), train_loss = 1.77501057, grad/param norm = 2.3519e-01, time/batch = 0.6526s\u001b[0m\t\n",
            "\u001b[0m907/2115 (epoch 2.144), train_loss = 1.85327666, grad/param norm = 2.9463e-01, time/batch = 0.9862s\u001b[0m\t\n",
            "\u001b[0m908/2115 (epoch 2.147), train_loss = 1.83896820, grad/param norm = 2.6801e-01, time/batch = 0.9028s\u001b[0m\t\n",
            "\u001b[0m909/2115 (epoch 2.149), train_loss = 1.78391806, grad/param norm = 1.9421e-01, time/batch = 0.8867s\u001b[0m\t\n",
            "\u001b[0m910/2115 (epoch 2.151), train_loss = 1.82487203, grad/param norm = 1.9356e-01, time/batch = 0.4514s\u001b[0m\t\n",
            "\u001b[0m911/2115 (epoch 2.154), train_loss = 1.72937344, grad/param norm = 1.8890e-01, time/batch = 0.4135s\u001b[0m\t\n",
            "\u001b[0m912/2115 (epoch 2.156), train_loss = 1.80064780, grad/param norm = 1.7217e-01, time/batch = 0.4338s\u001b[0m\t\n",
            "\u001b[0m913/2115 (epoch 2.158), train_loss = 1.72576824, grad/param norm = 1.4096e-01, time/batch = 0.4197s\u001b[0m\t\n",
            "\u001b[0m914/2115 (epoch 2.161), train_loss = 1.79099107, grad/param norm = 1.6346e-01, time/batch = 0.4430s\u001b[0m\t\n",
            "\u001b[0m915/2115 (epoch 2.163), train_loss = 1.82230497, grad/param norm = 1.9319e-01, time/batch = 0.4217s\u001b[0m\t\n",
            "\u001b[0m916/2115 (epoch 2.165), train_loss = 1.80494121, grad/param norm = 2.2767e-01, time/batch = 0.4098s\u001b[0m\t\n",
            "\u001b[0m917/2115 (epoch 2.168), train_loss = 1.77242535, grad/param norm = 1.8685e-01, time/batch = 0.4478s\u001b[0m\t\n",
            "\u001b[0m918/2115 (epoch 2.170), train_loss = 1.72517070, grad/param norm = 1.5791e-01, time/batch = 0.4233s\u001b[0m\t\n",
            "\u001b[0m919/2115 (epoch 2.173), train_loss = 1.78705648, grad/param norm = 1.6387e-01, time/batch = 0.4350s\u001b[0m\t\n",
            "\u001b[0m920/2115 (epoch 2.175), train_loss = 1.72988772, grad/param norm = 1.5745e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m921/2115 (epoch 2.177), train_loss = 1.72844126, grad/param norm = 1.5864e-01, time/batch = 0.4438s\u001b[0m\t\n",
            "\u001b[0m922/2115 (epoch 2.180), train_loss = 1.75723430, grad/param norm = 1.5696e-01, time/batch = 0.4130s\u001b[0m\t\n",
            "\u001b[0m923/2115 (epoch 2.182), train_loss = 1.76205051, grad/param norm = 1.6091e-01, time/batch = 0.4252s\u001b[0m\t\n",
            "\u001b[0m924/2115 (epoch 2.184), train_loss = 1.77500592, grad/param norm = 1.8478e-01, time/batch = 0.4426s\u001b[0m\t\n",
            "\u001b[0m925/2115 (epoch 2.187), train_loss = 1.76156749, grad/param norm = 1.8633e-01, time/batch = 0.4151s\u001b[0m\t\n",
            "\u001b[0m926/2115 (epoch 2.189), train_loss = 1.77943405, grad/param norm = 1.6132e-01, time/batch = 0.4414s\u001b[0m\t\n",
            "\u001b[0m927/2115 (epoch 2.191), train_loss = 1.78426489, grad/param norm = 1.5010e-01, time/batch = 0.4247s\u001b[0m\t\n",
            "\u001b[0m928/2115 (epoch 2.194), train_loss = 1.64887410, grad/param norm = 1.5734e-01, time/batch = 0.4167s\u001b[0m\t\n",
            "\u001b[0m929/2115 (epoch 2.196), train_loss = 1.73383398, grad/param norm = 1.7446e-01, time/batch = 0.4392s\u001b[0m\t\n",
            "\u001b[0m930/2115 (epoch 2.199), train_loss = 1.76028812, grad/param norm = 1.6885e-01, time/batch = 0.4205s\u001b[0m\t\n",
            "\u001b[0m931/2115 (epoch 2.201), train_loss = 1.77915364, grad/param norm = 1.7451e-01, time/batch = 0.4348s\u001b[0m\t\n",
            "\u001b[0m932/2115 (epoch 2.203), train_loss = 1.75694323, grad/param norm = 1.6067e-01, time/batch = 0.4159s\u001b[0m\t\n",
            "\u001b[0m933/2115 (epoch 2.206), train_loss = 1.72152012, grad/param norm = 1.4926e-01, time/batch = 0.8912s\u001b[0m\t\n",
            "\u001b[0m934/2115 (epoch 2.208), train_loss = 1.69722500, grad/param norm = 1.4240e-01, time/batch = 1.0126s\u001b[0m\t\n",
            "\u001b[0m935/2115 (epoch 2.210), train_loss = 1.82404712, grad/param norm = 1.6743e-01, time/batch = 0.8574s\u001b[0m\t\n",
            "\u001b[0m936/2115 (epoch 2.213), train_loss = 1.74994362, grad/param norm = 2.0991e-01, time/batch = 0.6282s\u001b[0m\t\n",
            "\u001b[0m937/2115 (epoch 2.215), train_loss = 1.73172924, grad/param norm = 2.2917e-01, time/batch = 0.4462s\u001b[0m\t\n",
            "\u001b[0m938/2115 (epoch 2.217), train_loss = 1.72021683, grad/param norm = 2.3158e-01, time/batch = 0.4223s\u001b[0m\t\n",
            "\u001b[0m939/2115 (epoch 2.220), train_loss = 1.77881169, grad/param norm = 2.4091e-01, time/batch = 0.4359s\u001b[0m\t\n",
            "\u001b[0m940/2115 (epoch 2.222), train_loss = 1.75720671, grad/param norm = 2.5114e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m941/2115 (epoch 2.225), train_loss = 1.78564977, grad/param norm = 2.6179e-01, time/batch = 0.4460s\u001b[0m\t\n",
            "\u001b[0m942/2115 (epoch 2.227), train_loss = 1.81222886, grad/param norm = 1.8781e-01, time/batch = 0.4243s\u001b[0m\t\n",
            "\u001b[0m943/2115 (epoch 2.229), train_loss = 1.75855872, grad/param norm = 1.4265e-01, time/batch = 0.4428s\u001b[0m\t\n",
            "\u001b[0m944/2115 (epoch 2.232), train_loss = 1.77403613, grad/param norm = 1.4111e-01, time/batch = 0.4704s\u001b[0m\t\n",
            "\u001b[0m945/2115 (epoch 2.234), train_loss = 1.68929591, grad/param norm = 1.5974e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m946/2115 (epoch 2.236), train_loss = 1.78938757, grad/param norm = 1.7642e-01, time/batch = 0.4382s\u001b[0m\t\n",
            "\u001b[0m947/2115 (epoch 2.239), train_loss = 1.73364958, grad/param norm = 1.8657e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m948/2115 (epoch 2.241), train_loss = 1.75275856, grad/param norm = 1.8776e-01, time/batch = 0.4182s\u001b[0m\t\n",
            "\u001b[0m949/2115 (epoch 2.243), train_loss = 1.77672568, grad/param norm = 1.8621e-01, time/batch = 0.4350s\u001b[0m\t\n",
            "\u001b[0m950/2115 (epoch 2.246), train_loss = 1.72920972, grad/param norm = 1.6725e-01, time/batch = 0.4267s\u001b[0m\t\n",
            "\u001b[0m951/2115 (epoch 2.248), train_loss = 1.76226557, grad/param norm = 1.5148e-01, time/batch = 0.4447s\u001b[0m\t\n",
            "\u001b[0m952/2115 (epoch 2.251), train_loss = 1.76322748, grad/param norm = 1.5543e-01, time/batch = 0.4251s\u001b[0m\t\n",
            "\u001b[0m953/2115 (epoch 2.253), train_loss = 1.71571038, grad/param norm = 1.5028e-01, time/batch = 0.4334s\u001b[0m\t\n",
            "\u001b[0m954/2115 (epoch 2.255), train_loss = 1.70456964, grad/param norm = 1.2603e-01, time/batch = 0.4141s\u001b[0m\t\n",
            "\u001b[0m955/2115 (epoch 2.258), train_loss = 1.78343286, grad/param norm = 1.3146e-01, time/batch = 0.4156s\u001b[0m\t\n",
            "\u001b[0m956/2115 (epoch 2.260), train_loss = 1.66688157, grad/param norm = 1.5759e-01, time/batch = 0.4315s\u001b[0m\t\n",
            "\u001b[0m957/2115 (epoch 2.262), train_loss = 1.66127272, grad/param norm = 1.5982e-01, time/batch = 0.4272s\u001b[0m\t\n",
            "\u001b[0m958/2115 (epoch 2.265), train_loss = 1.74581991, grad/param norm = 1.5973e-01, time/batch = 0.4331s\u001b[0m\t\n",
            "\u001b[0m959/2115 (epoch 2.267), train_loss = 1.75770211, grad/param norm = 1.7624e-01, time/batch = 0.5285s\u001b[0m\t\n",
            "\u001b[0m960/2115 (epoch 2.270), train_loss = 1.75528526, grad/param norm = 1.8331e-01, time/batch = 0.9033s\u001b[0m\t\n",
            "\u001b[0m961/2115 (epoch 2.272), train_loss = 1.78195790, grad/param norm = 1.6902e-01, time/batch = 0.9800s\u001b[0m\t\n",
            "\u001b[0m962/2115 (epoch 2.274), train_loss = 1.78304259, grad/param norm = 1.7172e-01, time/batch = 0.8761s\u001b[0m\t\n",
            "\u001b[0m963/2115 (epoch 2.277), train_loss = 1.75416509, grad/param norm = 1.8106e-01, time/batch = 0.4504s\u001b[0m\t\n",
            "\u001b[0m964/2115 (epoch 2.279), train_loss = 1.78563162, grad/param norm = 1.7300e-01, time/batch = 0.4247s\u001b[0m\t\n",
            "\u001b[0m965/2115 (epoch 2.281), train_loss = 1.75756082, grad/param norm = 1.7289e-01, time/batch = 0.4156s\u001b[0m\t\n",
            "\u001b[0m966/2115 (epoch 2.284), train_loss = 1.65247072, grad/param norm = 1.8231e-01, time/batch = 0.4467s\u001b[0m\t\n",
            "\u001b[0m967/2115 (epoch 2.286), train_loss = 1.75171481, grad/param norm = 1.9823e-01, time/batch = 0.4189s\u001b[0m\t\n",
            "\u001b[0m968/2115 (epoch 2.288), train_loss = 1.75907965, grad/param norm = 1.9647e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m969/2115 (epoch 2.291), train_loss = 1.70708769, grad/param norm = 1.9704e-01, time/batch = 0.4359s\u001b[0m\t\n",
            "\u001b[0m970/2115 (epoch 2.293), train_loss = 1.71311594, grad/param norm = 1.8984e-01, time/batch = 0.5851s\u001b[0m\t\n",
            "\u001b[0m971/2115 (epoch 2.296), train_loss = 1.77019327, grad/param norm = 2.2681e-01, time/batch = 0.4547s\u001b[0m\t\n",
            "\u001b[0m972/2115 (epoch 2.298), train_loss = 1.82208816, grad/param norm = 2.6225e-01, time/batch = 0.4224s\u001b[0m\t\n",
            "\u001b[0m973/2115 (epoch 2.300), train_loss = 1.75894547, grad/param norm = 3.1204e-01, time/batch = 0.4440s\u001b[0m\t\n",
            "\u001b[0m974/2115 (epoch 2.303), train_loss = 1.79694964, grad/param norm = 2.4688e-01, time/batch = 0.4187s\u001b[0m\t\n",
            "\u001b[0m975/2115 (epoch 2.305), train_loss = 1.78148229, grad/param norm = 1.8960e-01, time/batch = 0.4361s\u001b[0m\t\n",
            "\u001b[0m976/2115 (epoch 2.307), train_loss = 1.68952528, grad/param norm = 1.7048e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m977/2115 (epoch 2.310), train_loss = 1.67200244, grad/param norm = 1.8591e-01, time/batch = 0.4104s\u001b[0m\t\n",
            "\u001b[0m978/2115 (epoch 2.312), train_loss = 1.74551947, grad/param norm = 1.3061e-01, time/batch = 0.4586s\u001b[0m\t\n",
            "\u001b[0m979/2115 (epoch 2.314), train_loss = 1.79188852, grad/param norm = 1.6965e-01, time/batch = 0.4154s\u001b[0m\t\n",
            "\u001b[0m980/2115 (epoch 2.317), train_loss = 1.74096047, grad/param norm = 1.6590e-01, time/batch = 0.4484s\u001b[0m\t\n",
            "\u001b[0m981/2115 (epoch 2.319), train_loss = 1.69697450, grad/param norm = 1.4757e-01, time/batch = 0.4222s\u001b[0m\t\n",
            "\u001b[0m982/2115 (epoch 2.322), train_loss = 1.73500720, grad/param norm = 1.2024e-01, time/batch = 0.4163s\u001b[0m\t\n",
            "\u001b[0m983/2115 (epoch 2.324), train_loss = 1.78752388, grad/param norm = 1.4471e-01, time/batch = 0.4284s\u001b[0m\t\n",
            "\u001b[0m984/2115 (epoch 2.326), train_loss = 1.79496181, grad/param norm = 1.4028e-01, time/batch = 0.4250s\u001b[0m\t\n",
            "\u001b[0m985/2115 (epoch 2.329), train_loss = 1.78654512, grad/param norm = 1.4934e-01, time/batch = 0.5416s\u001b[0m\t\n",
            "\u001b[0m986/2115 (epoch 2.331), train_loss = 1.77475184, grad/param norm = 1.7099e-01, time/batch = 0.8175s\u001b[0m\t\n",
            "\u001b[0m987/2115 (epoch 2.333), train_loss = 1.79539604, grad/param norm = 1.9963e-01, time/batch = 0.9645s\u001b[0m\t\n",
            "\u001b[0m988/2115 (epoch 2.336), train_loss = 1.78564165, grad/param norm = 1.9119e-01, time/batch = 0.8723s\u001b[0m\t\n",
            "\u001b[0m989/2115 (epoch 2.338), train_loss = 1.72569650, grad/param norm = 1.9316e-01, time/batch = 0.5994s\u001b[0m\t\n",
            "\u001b[0m990/2115 (epoch 2.340), train_loss = 1.70877160, grad/param norm = 1.7157e-01, time/batch = 0.4336s\u001b[0m\t\n",
            "\u001b[0m991/2115 (epoch 2.343), train_loss = 1.73897741, grad/param norm = 1.7462e-01, time/batch = 0.4449s\u001b[0m\t\n",
            "\u001b[0m992/2115 (epoch 2.345), train_loss = 1.73700153, grad/param norm = 1.4136e-01, time/batch = 0.4157s\u001b[0m\t\n",
            "\u001b[0m993/2115 (epoch 2.348), train_loss = 1.72518674, grad/param norm = 1.4623e-01, time/batch = 0.4435s\u001b[0m\t\n",
            "\u001b[0m994/2115 (epoch 2.350), train_loss = 1.70617377, grad/param norm = 1.4297e-01, time/batch = 0.4257s\u001b[0m\t\n",
            "\u001b[0m995/2115 (epoch 2.352), train_loss = 1.72953004, grad/param norm = 1.4275e-01, time/batch = 0.4254s\u001b[0m\t\n",
            "\u001b[0m996/2115 (epoch 2.355), train_loss = 1.73160072, grad/param norm = 1.4274e-01, time/batch = 0.4380s\u001b[0m\t\n",
            "\u001b[0m997/2115 (epoch 2.357), train_loss = 1.68615023, grad/param norm = 1.6254e-01, time/batch = 0.4213s\u001b[0m\t\n",
            "\u001b[0m998/2115 (epoch 2.359), train_loss = 1.76481827, grad/param norm = 1.9354e-01, time/batch = 0.4413s\u001b[0m\t\n",
            "\u001b[0m999/2115 (epoch 2.362), train_loss = 1.73615990, grad/param norm = 2.0855e-01, time/batch = 0.4147s\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch2.36_1.7680.t7\u001b[0m\t\n",
            "\u001b[0m1000/2115 (epoch 2.364), train_loss = 1.72991061, grad/param norm = 1.9743e-01, time/batch = 0.4280s\u001b[0m\t\n",
            "\u001b[0m1001/2115 (epoch 2.366), train_loss = 1.87302413, grad/param norm = 1.9216e-01, time/batch = 0.9241s\u001b[0m\t\n",
            "\u001b[0m1002/2115 (epoch 2.369), train_loss = 1.75890392, grad/param norm = 1.9284e-01, time/batch = 0.9173s\u001b[0m\t\n",
            "\u001b[0m1003/2115 (epoch 2.371), train_loss = 1.73764712, grad/param norm = 2.2126e-01, time/batch = 0.8530s\u001b[0m\t\n",
            "\u001b[0m1004/2115 (epoch 2.374), train_loss = 1.73718113, grad/param norm = 2.0943e-01, time/batch = 0.4664s\u001b[0m\t\n",
            "\u001b[0m1005/2115 (epoch 2.376), train_loss = 1.75056044, grad/param norm = 2.0099e-01, time/batch = 0.4315s\u001b[0m\t\n",
            "\u001b[0m1006/2115 (epoch 2.378), train_loss = 1.74380531, grad/param norm = 1.9370e-01, time/batch = 0.4446s\u001b[0m\t\n",
            "\u001b[0m1007/2115 (epoch 2.381), train_loss = 1.69822094, grad/param norm = 1.6832e-01, time/batch = 0.4207s\u001b[0m\t\n",
            "\u001b[0m1008/2115 (epoch 2.383), train_loss = 1.84481005, grad/param norm = 1.5812e-01, time/batch = 0.4188s\u001b[0m\t\n",
            "\u001b[0m1009/2115 (epoch 2.385), train_loss = 1.74706404, grad/param norm = 1.4571e-01, time/batch = 0.4351s\u001b[0m\t\n",
            "\u001b[0m1010/2115 (epoch 2.388), train_loss = 1.72251905, grad/param norm = 1.2906e-01, time/batch = 0.4156s\u001b[0m\t\n",
            "\u001b[0m1011/2115 (epoch 2.390), train_loss = 1.77727745, grad/param norm = 1.3780e-01, time/batch = 0.4453s\u001b[0m\t\n",
            "\u001b[0m1012/2115 (epoch 2.392), train_loss = 1.75987609, grad/param norm = 1.4086e-01, time/batch = 0.4206s\u001b[0m\t\n",
            "\u001b[0m1013/2115 (epoch 2.395), train_loss = 1.71778184, grad/param norm = 1.3274e-01, time/batch = 0.4337s\u001b[0m\t\n",
            "\u001b[0m1014/2115 (epoch 2.397), train_loss = 1.68235915, grad/param norm = 1.5903e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m1015/2115 (epoch 2.400), train_loss = 1.74889480, grad/param norm = 1.7937e-01, time/batch = 0.4202s\u001b[0m\t\n",
            "\u001b[0m1016/2115 (epoch 2.402), train_loss = 1.78978408, grad/param norm = 1.9711e-01, time/batch = 0.4374s\u001b[0m\t\n",
            "\u001b[0m1017/2115 (epoch 2.404), train_loss = 1.77929305, grad/param norm = 2.2067e-01, time/batch = 0.4151s\u001b[0m\t\n",
            "\u001b[0m1018/2115 (epoch 2.407), train_loss = 1.78422265, grad/param norm = 1.8242e-01, time/batch = 0.4371s\u001b[0m\t\n",
            "\u001b[0m1019/2115 (epoch 2.409), train_loss = 1.75223663, grad/param norm = 1.7158e-01, time/batch = 0.4333s\u001b[0m\t\n",
            "\u001b[0m1020/2115 (epoch 2.411), train_loss = 1.71781471, grad/param norm = 1.6911e-01, time/batch = 0.4517s\u001b[0m\t\n",
            "\u001b[0m1021/2115 (epoch 2.414), train_loss = 1.75011408, grad/param norm = 1.5577e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m1022/2115 (epoch 2.416), train_loss = 1.77051705, grad/param norm = 1.4854e-01, time/batch = 0.4136s\u001b[0m\t\n",
            "\u001b[0m1023/2115 (epoch 2.418), train_loss = 1.73579835, grad/param norm = 1.4836e-01, time/batch = 0.4444s\u001b[0m\t\n",
            "\u001b[0m1024/2115 (epoch 2.421), train_loss = 1.69147207, grad/param norm = 1.5110e-01, time/batch = 0.4265s\u001b[0m\t\n",
            "\u001b[0m1025/2115 (epoch 2.423), train_loss = 1.67525931, grad/param norm = 1.7036e-01, time/batch = 0.4671s\u001b[0m\t\n",
            "\u001b[0m1026/2115 (epoch 2.426), train_loss = 1.71849223, grad/param norm = 2.3164e-01, time/batch = 0.4297s\u001b[0m\t\n",
            "\u001b[0m1027/2115 (epoch 2.428), train_loss = 1.74351825, grad/param norm = 2.3957e-01, time/batch = 0.8398s\u001b[0m\t\n",
            "\u001b[0m1028/2115 (epoch 2.430), train_loss = 1.73559187, grad/param norm = 2.1468e-01, time/batch = 1.0371s\u001b[0m\t\n",
            "\u001b[0m1029/2115 (epoch 2.433), train_loss = 1.75111799, grad/param norm = 1.6219e-01, time/batch = 0.8359s\u001b[0m\t\n",
            "\u001b[0m1030/2115 (epoch 2.435), train_loss = 1.73204373, grad/param norm = 1.4287e-01, time/batch = 0.6366s\u001b[0m\t\n",
            "\u001b[0m1031/2115 (epoch 2.437), train_loss = 1.62716896, grad/param norm = 1.3565e-01, time/batch = 0.4500s\u001b[0m\t\n",
            "\u001b[0m1032/2115 (epoch 2.440), train_loss = 1.68956388, grad/param norm = 1.4696e-01, time/batch = 0.4137s\u001b[0m\t\n",
            "\u001b[0m1033/2115 (epoch 2.442), train_loss = 1.72195694, grad/param norm = 1.5242e-01, time/batch = 0.4424s\u001b[0m\t\n",
            "\u001b[0m1034/2115 (epoch 2.444), train_loss = 1.73361204, grad/param norm = 1.6901e-01, time/batch = 0.4201s\u001b[0m\t\n",
            "\u001b[0m1035/2115 (epoch 2.447), train_loss = 1.75152297, grad/param norm = 1.8083e-01, time/batch = 0.4175s\u001b[0m\t\n",
            "\u001b[0m1036/2115 (epoch 2.449), train_loss = 1.64602216, grad/param norm = 1.6436e-01, time/batch = 0.4440s\u001b[0m\t\n",
            "\u001b[0m1037/2115 (epoch 2.452), train_loss = 1.74555775, grad/param norm = 1.8246e-01, time/batch = 0.4162s\u001b[0m\t\n",
            "\u001b[0m1038/2115 (epoch 2.454), train_loss = 1.65456075, grad/param norm = 1.5214e-01, time/batch = 0.4394s\u001b[0m\t\n",
            "\u001b[0m1039/2115 (epoch 2.456), train_loss = 1.67369623, grad/param norm = 1.4329e-01, time/batch = 0.4176s\u001b[0m\t\n",
            "\u001b[0m1040/2115 (epoch 2.459), train_loss = 1.69572024, grad/param norm = 1.6166e-01, time/batch = 0.4176s\u001b[0m\t\n",
            "\u001b[0m1041/2115 (epoch 2.461), train_loss = 1.70743288, grad/param norm = 1.7460e-01, time/batch = 0.4470s\u001b[0m\t\n",
            "\u001b[0m1042/2115 (epoch 2.463), train_loss = 1.68476662, grad/param norm = 2.1784e-01, time/batch = 0.4235s\u001b[0m\t\n",
            "\u001b[0m1043/2115 (epoch 2.466), train_loss = 1.69068190, grad/param norm = 2.2006e-01, time/batch = 0.4370s\u001b[0m\t\n",
            "\u001b[0m1044/2115 (epoch 2.468), train_loss = 1.72691081, grad/param norm = 1.6856e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m1045/2115 (epoch 2.470), train_loss = 1.75807700, grad/param norm = 1.4822e-01, time/batch = 0.4255s\u001b[0m\t\n",
            "\u001b[0m1046/2115 (epoch 2.473), train_loss = 1.74551804, grad/param norm = 1.5468e-01, time/batch = 0.4141s\u001b[0m\t\n",
            "\u001b[0m1047/2115 (epoch 2.475), train_loss = 1.76157492, grad/param norm = 1.7457e-01, time/batch = 0.4155s\u001b[0m\t\n",
            "\u001b[0m1048/2115 (epoch 2.478), train_loss = 1.71208151, grad/param norm = 1.8215e-01, time/batch = 0.4380s\u001b[0m\t\n",
            "\u001b[0m1049/2115 (epoch 2.480), train_loss = 1.74294687, grad/param norm = 2.0622e-01, time/batch = 0.4167s\u001b[0m\t\n",
            "\u001b[0m1050/2115 (epoch 2.482), train_loss = 1.68845456, grad/param norm = 1.8386e-01, time/batch = 0.4527s\u001b[0m\t\n",
            "\u001b[0m1051/2115 (epoch 2.485), train_loss = 1.71093684, grad/param norm = 1.7032e-01, time/batch = 0.4239s\u001b[0m\t\n",
            "\u001b[0m1052/2115 (epoch 2.487), train_loss = 1.72369093, grad/param norm = 1.6458e-01, time/batch = 0.4407s\u001b[0m\t\n",
            "\u001b[0m1053/2115 (epoch 2.489), train_loss = 1.74651842, grad/param norm = 1.7754e-01, time/batch = 0.8249s\u001b[0m\t\n",
            "\u001b[0m1054/2115 (epoch 2.492), train_loss = 1.74800249, grad/param norm = 1.6493e-01, time/batch = 1.0065s\u001b[0m\t\n",
            "\u001b[0m1055/2115 (epoch 2.494), train_loss = 1.78623918, grad/param norm = 1.4872e-01, time/batch = 1.0478s\u001b[0m\t\n",
            "\u001b[0m1056/2115 (epoch 2.496), train_loss = 1.72344694, grad/param norm = 1.5118e-01, time/batch = 0.6106s\u001b[0m\t\n",
            "\u001b[0m1057/2115 (epoch 2.499), train_loss = 1.71531777, grad/param norm = 1.4269e-01, time/batch = 0.4180s\u001b[0m\t\n",
            "\u001b[0m1058/2115 (epoch 2.501), train_loss = 1.67702074, grad/param norm = 1.5353e-01, time/batch = 0.4495s\u001b[0m\t\n",
            "\u001b[0m1059/2115 (epoch 2.504), train_loss = 1.70860775, grad/param norm = 1.4660e-01, time/batch = 0.4232s\u001b[0m\t\n",
            "\u001b[0m1060/2115 (epoch 2.506), train_loss = 1.73100277, grad/param norm = 1.7826e-01, time/batch = 0.4159s\u001b[0m\t\n",
            "\u001b[0m1061/2115 (epoch 2.508), train_loss = 1.66435830, grad/param norm = 1.7296e-01, time/batch = 0.4398s\u001b[0m\t\n",
            "\u001b[0m1062/2115 (epoch 2.511), train_loss = 1.69748784, grad/param norm = 1.4087e-01, time/batch = 0.4416s\u001b[0m\t\n",
            "\u001b[0m1063/2115 (epoch 2.513), train_loss = 1.71664131, grad/param norm = 1.3970e-01, time/batch = 0.4352s\u001b[0m\t\n",
            "\u001b[0m1064/2115 (epoch 2.515), train_loss = 1.67829682, grad/param norm = 1.5539e-01, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m1065/2115 (epoch 2.518), train_loss = 1.69393685, grad/param norm = 2.0862e-01, time/batch = 0.4335s\u001b[0m\t\n",
            "\u001b[0m1066/2115 (epoch 2.520), train_loss = 1.71716953, grad/param norm = 1.9355e-01, time/batch = 0.4176s\u001b[0m\t\n",
            "\u001b[0m1067/2115 (epoch 2.522), train_loss = 1.72037492, grad/param norm = 1.7471e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m1068/2115 (epoch 2.525), train_loss = 1.74518222, grad/param norm = 1.7176e-01, time/batch = 0.4381s\u001b[0m\t\n",
            "\u001b[0m1069/2115 (epoch 2.527), train_loss = 1.71846991, grad/param norm = 2.2311e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m1070/2115 (epoch 2.530), train_loss = 1.70731737, grad/param norm = 2.0173e-01, time/batch = 0.4320s\u001b[0m\t\n",
            "\u001b[0m1071/2115 (epoch 2.532), train_loss = 1.66279091, grad/param norm = 1.8357e-01, time/batch = 0.4282s\u001b[0m\t\n",
            "\u001b[0m1072/2115 (epoch 2.534), train_loss = 1.67958950, grad/param norm = 1.7015e-01, time/batch = 0.4262s\u001b[0m\t\n",
            "\u001b[0m1073/2115 (epoch 2.537), train_loss = 1.76941037, grad/param norm = 1.4962e-01, time/batch = 0.4224s\u001b[0m\t\n",
            "\u001b[0m1074/2115 (epoch 2.539), train_loss = 1.68595804, grad/param norm = 1.3103e-01, time/batch = 0.4201s\u001b[0m\t\n",
            "\u001b[0m1075/2115 (epoch 2.541), train_loss = 1.71097791, grad/param norm = 1.3060e-01, time/batch = 0.4266s\u001b[0m\t\n",
            "\u001b[0m1076/2115 (epoch 2.544), train_loss = 1.70747810, grad/param norm = 1.3174e-01, time/batch = 0.4215s\u001b[0m\t\n",
            "\u001b[0m1077/2115 (epoch 2.546), train_loss = 1.74459880, grad/param norm = 1.5168e-01, time/batch = 0.4532s\u001b[0m\t\n",
            "\u001b[0m1078/2115 (epoch 2.548), train_loss = 1.69752453, grad/param norm = 1.5397e-01, time/batch = 0.4259s\u001b[0m\t\n",
            "\u001b[0m1079/2115 (epoch 2.551), train_loss = 1.71356903, grad/param norm = 1.5396e-01, time/batch = 0.5728s\u001b[0m\t\n",
            "\u001b[0m1080/2115 (epoch 2.553), train_loss = 1.74714592, grad/param norm = 1.6268e-01, time/batch = 0.7749s\u001b[0m\t\n",
            "\u001b[0m1081/2115 (epoch 2.556), train_loss = 1.72660467, grad/param norm = 1.7284e-01, time/batch = 1.0384s\u001b[0m\t\n",
            "\u001b[0m1082/2115 (epoch 2.558), train_loss = 1.70930294, grad/param norm = 1.8816e-01, time/batch = 0.9326s\u001b[0m\t\n",
            "\u001b[0m1083/2115 (epoch 2.560), train_loss = 1.73466175, grad/param norm = 1.7635e-01, time/batch = 0.4619s\u001b[0m\t\n",
            "\u001b[0m1084/2115 (epoch 2.563), train_loss = 1.66855588, grad/param norm = 1.6470e-01, time/batch = 0.4092s\u001b[0m\t\n",
            "\u001b[0m1085/2115 (epoch 2.565), train_loss = 1.71434369, grad/param norm = 1.8085e-01, time/batch = 0.4179s\u001b[0m\t\n",
            "\u001b[0m1086/2115 (epoch 2.567), train_loss = 1.69556281, grad/param norm = 1.7111e-01, time/batch = 0.4394s\u001b[0m\t\n",
            "\u001b[0m1087/2115 (epoch 2.570), train_loss = 1.73816158, grad/param norm = 1.5088e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m1088/2115 (epoch 2.572), train_loss = 1.65697246, grad/param norm = 1.5175e-01, time/batch = 0.4521s\u001b[0m\t\n",
            "\u001b[0m1089/2115 (epoch 2.574), train_loss = 1.69340374, grad/param norm = 1.4130e-01, time/batch = 0.4109s\u001b[0m\t\n",
            "\u001b[0m1090/2115 (epoch 2.577), train_loss = 1.71169420, grad/param norm = 1.6411e-01, time/batch = 0.4417s\u001b[0m\t\n",
            "\u001b[0m1091/2115 (epoch 2.579), train_loss = 1.66359954, grad/param norm = 1.6111e-01, time/batch = 0.4275s\u001b[0m\t\n",
            "\u001b[0m1092/2115 (epoch 2.582), train_loss = 1.67683224, grad/param norm = 1.8278e-01, time/batch = 0.4199s\u001b[0m\t\n",
            "\u001b[0m1093/2115 (epoch 2.584), train_loss = 1.71462002, grad/param norm = 1.8010e-01, time/batch = 0.4367s\u001b[0m\t\n",
            "\u001b[0m1094/2115 (epoch 2.586), train_loss = 1.69131236, grad/param norm = 1.4007e-01, time/batch = 0.4131s\u001b[0m\t\n",
            "\u001b[0m1095/2115 (epoch 2.589), train_loss = 1.75220991, grad/param norm = 1.8238e-01, time/batch = 0.4429s\u001b[0m\t\n",
            "\u001b[0m1096/2115 (epoch 2.591), train_loss = 1.66212297, grad/param norm = 1.9677e-01, time/batch = 0.4153s\u001b[0m\t\n",
            "\u001b[0m1097/2115 (epoch 2.593), train_loss = 1.72698882, grad/param norm = 1.7374e-01, time/batch = 0.4267s\u001b[0m\t\n",
            "\u001b[0m1098/2115 (epoch 2.596), train_loss = 1.64857300, grad/param norm = 1.4550e-01, time/batch = 0.4437s\u001b[0m\t\n",
            "\u001b[0m1099/2115 (epoch 2.598), train_loss = 1.71732246, grad/param norm = 1.4000e-01, time/batch = 0.4385s\u001b[0m\t\n",
            "\u001b[0m1100/2115 (epoch 2.600), train_loss = 1.70171536, grad/param norm = 1.6253e-01, time/batch = 0.4461s\u001b[0m\t\n",
            "\u001b[0m1101/2115 (epoch 2.603), train_loss = 1.68702333, grad/param norm = 1.7404e-01, time/batch = 0.4181s\u001b[0m\t\n",
            "\u001b[0m1102/2115 (epoch 2.605), train_loss = 1.71162423, grad/param norm = 1.5362e-01, time/batch = 0.4410s\u001b[0m\t\n",
            "\u001b[0m1103/2115 (epoch 2.608), train_loss = 1.66600045, grad/param norm = 1.8084e-01, time/batch = 0.4108s\u001b[0m\t\n",
            "\u001b[0m1104/2115 (epoch 2.610), train_loss = 1.68562658, grad/param norm = 1.9741e-01, time/batch = 0.4243s\u001b[0m\t\n",
            "\u001b[0m1105/2115 (epoch 2.612), train_loss = 1.71617944, grad/param norm = 2.0377e-01, time/batch = 0.4333s\u001b[0m\t\n",
            "\u001b[0m1106/2115 (epoch 2.615), train_loss = 1.68955635, grad/param norm = 2.0781e-01, time/batch = 0.8576s\u001b[0m\t\n",
            "\u001b[0m1107/2115 (epoch 2.617), train_loss = 1.65777392, grad/param norm = 1.7666e-01, time/batch = 0.9363s\u001b[0m\t\n",
            "\u001b[0m1108/2115 (epoch 2.619), train_loss = 1.65926310, grad/param norm = 1.3434e-01, time/batch = 0.9408s\u001b[0m\t\n",
            "\u001b[0m1109/2115 (epoch 2.622), train_loss = 1.63521887, grad/param norm = 1.4513e-01, time/batch = 0.6659s\u001b[0m\t\n",
            "\u001b[0m1110/2115 (epoch 2.624), train_loss = 1.70601435, grad/param norm = 2.2757e-01, time/batch = 0.4323s\u001b[0m\t\n",
            "\u001b[0m1111/2115 (epoch 2.626), train_loss = 1.65744828, grad/param norm = 1.7613e-01, time/batch = 0.4398s\u001b[0m\t\n",
            "\u001b[0m1112/2115 (epoch 2.629), train_loss = 1.73505345, grad/param norm = 1.4968e-01, time/batch = 0.4432s\u001b[0m\t\n",
            "\u001b[0m1113/2115 (epoch 2.631), train_loss = 1.72752371, grad/param norm = 1.1678e-01, time/batch = 0.4585s\u001b[0m\t\n",
            "\u001b[0m1114/2115 (epoch 2.634), train_loss = 1.69005800, grad/param norm = 1.2104e-01, time/batch = 0.4293s\u001b[0m\t\n",
            "\u001b[0m1115/2115 (epoch 2.636), train_loss = 1.66697695, grad/param norm = 1.3872e-01, time/batch = 0.4413s\u001b[0m\t\n",
            "\u001b[0m1116/2115 (epoch 2.638), train_loss = 1.66510755, grad/param norm = 1.2608e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m1117/2115 (epoch 2.641), train_loss = 1.67629495, grad/param norm = 1.1646e-01, time/batch = 0.4325s\u001b[0m\t\n",
            "\u001b[0m1118/2115 (epoch 2.643), train_loss = 1.71718604, grad/param norm = 1.4887e-01, time/batch = 0.4321s\u001b[0m\t\n",
            "\u001b[0m1119/2115 (epoch 2.645), train_loss = 1.64868790, grad/param norm = 1.9198e-01, time/batch = 0.4541s\u001b[0m\t\n",
            "\u001b[0m1120/2115 (epoch 2.648), train_loss = 1.75655000, grad/param norm = 2.1022e-01, time/batch = 0.4536s\u001b[0m\t\n",
            "\u001b[0m1121/2115 (epoch 2.650), train_loss = 1.70510631, grad/param norm = 1.7350e-01, time/batch = 0.4299s\u001b[0m\t\n",
            "\u001b[0m1122/2115 (epoch 2.652), train_loss = 1.72961697, grad/param norm = 1.4926e-01, time/batch = 0.4435s\u001b[0m\t\n",
            "\u001b[0m1123/2115 (epoch 2.655), train_loss = 1.72104301, grad/param norm = 1.6395e-01, time/batch = 0.4232s\u001b[0m\t\n",
            "\u001b[0m1124/2115 (epoch 2.657), train_loss = 1.71717354, grad/param norm = 1.6205e-01, time/batch = 0.4348s\u001b[0m\t\n",
            "\u001b[0m1125/2115 (epoch 2.660), train_loss = 1.68376699, grad/param norm = 1.4936e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m1126/2115 (epoch 2.662), train_loss = 1.71987305, grad/param norm = 1.5286e-01, time/batch = 0.4111s\u001b[0m\t\n",
            "\u001b[0m1127/2115 (epoch 2.664), train_loss = 1.68174601, grad/param norm = 1.5648e-01, time/batch = 0.4387s\u001b[0m\t\n",
            "\u001b[0m1128/2115 (epoch 2.667), train_loss = 1.74122926, grad/param norm = 1.6217e-01, time/batch = 0.4174s\u001b[0m\t\n",
            "\u001b[0m1129/2115 (epoch 2.669), train_loss = 1.66306822, grad/param norm = 1.4165e-01, time/batch = 0.4339s\u001b[0m\t\n",
            "\u001b[0m1130/2115 (epoch 2.671), train_loss = 1.65742456, grad/param norm = 1.3492e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m1131/2115 (epoch 2.674), train_loss = 1.68231738, grad/param norm = 1.6491e-01, time/batch = 0.4261s\u001b[0m\t\n",
            "\u001b[0m1132/2115 (epoch 2.676), train_loss = 1.65878581, grad/param norm = 1.5554e-01, time/batch = 0.7729s\u001b[0m\t\n",
            "\u001b[0m1133/2115 (epoch 2.678), train_loss = 1.65336924, grad/param norm = 1.5063e-01, time/batch = 1.0389s\u001b[0m\t\n",
            "\u001b[0m1134/2115 (epoch 2.681), train_loss = 1.72008370, grad/param norm = 1.4795e-01, time/batch = 0.8648s\u001b[0m\t\n",
            "\u001b[0m1135/2115 (epoch 2.683), train_loss = 1.71286757, grad/param norm = 1.3967e-01, time/batch = 0.7457s\u001b[0m\t\n",
            "\u001b[0m1136/2115 (epoch 2.686), train_loss = 1.67853994, grad/param norm = 1.6011e-01, time/batch = 0.4079s\u001b[0m\t\n",
            "\u001b[0m1137/2115 (epoch 2.688), train_loss = 1.67611264, grad/param norm = 1.4375e-01, time/batch = 0.4362s\u001b[0m\t\n",
            "\u001b[0m1138/2115 (epoch 2.690), train_loss = 1.66593114, grad/param norm = 1.4878e-01, time/batch = 0.4566s\u001b[0m\t\n",
            "\u001b[0m1139/2115 (epoch 2.693), train_loss = 1.68060227, grad/param norm = 1.6119e-01, time/batch = 0.4133s\u001b[0m\t\n",
            "\u001b[0m1140/2115 (epoch 2.695), train_loss = 1.73291083, grad/param norm = 1.8506e-01, time/batch = 0.4509s\u001b[0m\t\n",
            "\u001b[0m1141/2115 (epoch 2.697), train_loss = 1.72305237, grad/param norm = 1.8921e-01, time/batch = 0.4272s\u001b[0m\t\n",
            "\u001b[0m1142/2115 (epoch 2.700), train_loss = 1.71512655, grad/param norm = 1.8141e-01, time/batch = 0.4589s\u001b[0m\t\n",
            "\u001b[0m1143/2115 (epoch 2.702), train_loss = 1.72004453, grad/param norm = 1.5017e-01, time/batch = 0.4263s\u001b[0m\t\n",
            "\u001b[0m1144/2115 (epoch 2.704), train_loss = 1.67141115, grad/param norm = 1.6867e-01, time/batch = 0.4565s\u001b[0m\t\n",
            "\u001b[0m1145/2115 (epoch 2.707), train_loss = 1.74227287, grad/param norm = 1.8438e-01, time/batch = 0.4120s\u001b[0m\t\n",
            "\u001b[0m1146/2115 (epoch 2.709), train_loss = 1.61910939, grad/param norm = 1.7940e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m1147/2115 (epoch 2.712), train_loss = 1.63976965, grad/param norm = 1.5746e-01, time/batch = 0.4476s\u001b[0m\t\n",
            "\u001b[0m1148/2115 (epoch 2.714), train_loss = 1.68192217, grad/param norm = 1.2374e-01, time/batch = 0.4104s\u001b[0m\t\n",
            "\u001b[0m1149/2115 (epoch 2.716), train_loss = 1.62519009, grad/param norm = 1.3594e-01, time/batch = 0.4393s\u001b[0m\t\n",
            "\u001b[0m1150/2115 (epoch 2.719), train_loss = 1.60516633, grad/param norm = 1.4368e-01, time/batch = 0.4199s\u001b[0m\t\n",
            "\u001b[0m1151/2115 (epoch 2.721), train_loss = 1.70942207, grad/param norm = 1.4444e-01, time/batch = 0.4260s\u001b[0m\t\n",
            "\u001b[0m1152/2115 (epoch 2.723), train_loss = 1.70930890, grad/param norm = 1.6637e-01, time/batch = 0.4476s\u001b[0m\t\n",
            "\u001b[0m1153/2115 (epoch 2.726), train_loss = 1.62358367, grad/param norm = 1.5495e-01, time/batch = 0.4162s\u001b[0m\t\n",
            "\u001b[0m1154/2115 (epoch 2.728), train_loss = 1.68696033, grad/param norm = 1.4461e-01, time/batch = 0.4339s\u001b[0m\t\n",
            "\u001b[0m1155/2115 (epoch 2.730), train_loss = 1.73410793, grad/param norm = 1.3665e-01, time/batch = 0.4057s\u001b[0m\t\n",
            "\u001b[0m1156/2115 (epoch 2.733), train_loss = 1.69304196, grad/param norm = 1.4871e-01, time/batch = 0.4524s\u001b[0m\t\n",
            "\u001b[0m1157/2115 (epoch 2.735), train_loss = 1.66928212, grad/param norm = 1.6730e-01, time/batch = 0.4096s\u001b[0m\t\n",
            "\u001b[0m1158/2115 (epoch 2.738), train_loss = 1.75825524, grad/param norm = 1.9211e-01, time/batch = 0.5917s\u001b[0m\t\n",
            "\u001b[0m1159/2115 (epoch 2.740), train_loss = 1.73216500, grad/param norm = 1.7530e-01, time/batch = 0.8093s\u001b[0m\t\n",
            "\u001b[0m1160/2115 (epoch 2.742), train_loss = 1.73698323, grad/param norm = 1.9826e-01, time/batch = 0.9786s\u001b[0m\t\n",
            "\u001b[0m1161/2115 (epoch 2.745), train_loss = 1.63858094, grad/param norm = 1.4638e-01, time/batch = 0.9420s\u001b[0m\t\n",
            "\u001b[0m1162/2115 (epoch 2.747), train_loss = 1.68978253, grad/param norm = 1.2400e-01, time/batch = 0.5230s\u001b[0m\t\n",
            "\u001b[0m1163/2115 (epoch 2.749), train_loss = 1.67663603, grad/param norm = 1.4842e-01, time/batch = 0.4215s\u001b[0m\t\n",
            "\u001b[0m1164/2115 (epoch 2.752), train_loss = 1.70138074, grad/param norm = 1.6223e-01, time/batch = 0.4173s\u001b[0m\t\n",
            "\u001b[0m1165/2115 (epoch 2.754), train_loss = 1.64417164, grad/param norm = 1.5653e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m1166/2115 (epoch 2.757), train_loss = 1.63564282, grad/param norm = 1.6573e-01, time/batch = 0.4283s\u001b[0m\t\n",
            "\u001b[0m1167/2115 (epoch 2.759), train_loss = 1.61426127, grad/param norm = 1.3578e-01, time/batch = 0.4414s\u001b[0m\t\n",
            "\u001b[0m1168/2115 (epoch 2.761), train_loss = 1.63776612, grad/param norm = 1.2018e-01, time/batch = 0.4247s\u001b[0m\t\n",
            "\u001b[0m1169/2115 (epoch 2.764), train_loss = 1.64510974, grad/param norm = 1.2923e-01, time/batch = 0.4442s\u001b[0m\t\n",
            "\u001b[0m1170/2115 (epoch 2.766), train_loss = 1.61008699, grad/param norm = 1.1775e-01, time/batch = 0.4366s\u001b[0m\t\n",
            "\u001b[0m1171/2115 (epoch 2.768), train_loss = 1.65055517, grad/param norm = 1.2892e-01, time/batch = 0.4188s\u001b[0m\t\n",
            "\u001b[0m1172/2115 (epoch 2.771), train_loss = 1.64587189, grad/param norm = 1.2727e-01, time/batch = 0.4541s\u001b[0m\t\n",
            "\u001b[0m1173/2115 (epoch 2.773), train_loss = 1.65071826, grad/param norm = 1.4109e-01, time/batch = 0.4283s\u001b[0m\t\n",
            "\u001b[0m1174/2115 (epoch 2.775), train_loss = 1.63189472, grad/param norm = 1.5230e-01, time/batch = 0.4301s\u001b[0m\t\n",
            "\u001b[0m1175/2115 (epoch 2.778), train_loss = 1.71571597, grad/param norm = 1.8523e-01, time/batch = 0.4230s\u001b[0m\t\n",
            "\u001b[0m1176/2115 (epoch 2.780), train_loss = 1.64638388, grad/param norm = 1.8006e-01, time/batch = 0.4355s\u001b[0m\t\n",
            "\u001b[0m1177/2115 (epoch 2.783), train_loss = 1.63872633, grad/param norm = 1.6052e-01, time/batch = 0.4215s\u001b[0m\t\n",
            "\u001b[0m1178/2115 (epoch 2.785), train_loss = 1.69421994, grad/param norm = 1.3090e-01, time/batch = 0.4161s\u001b[0m\t\n",
            "\u001b[0m1179/2115 (epoch 2.787), train_loss = 1.64533013, grad/param norm = 1.1879e-01, time/batch = 0.4353s\u001b[0m\t\n",
            "\u001b[0m1180/2115 (epoch 2.790), train_loss = 1.59081105, grad/param norm = 1.3233e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m1181/2115 (epoch 2.792), train_loss = 1.63649179, grad/param norm = 1.5667e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m1182/2115 (epoch 2.794), train_loss = 1.66147716, grad/param norm = 1.9552e-01, time/batch = 0.4331s\u001b[0m\t\n",
            "\u001b[0m1183/2115 (epoch 2.797), train_loss = 1.69837006, grad/param norm = 2.0362e-01, time/batch = 0.4342s\u001b[0m\t\n",
            "\u001b[0m1184/2115 (epoch 2.799), train_loss = 1.68113053, grad/param norm = 2.5213e-01, time/batch = 0.4485s\u001b[0m\t\n",
            "\u001b[0m1185/2115 (epoch 2.801), train_loss = 1.78886539, grad/param norm = 2.0566e-01, time/batch = 0.7543s\u001b[0m\t\n",
            "\u001b[0m1186/2115 (epoch 2.804), train_loss = 1.63838947, grad/param norm = 1.8577e-01, time/batch = 0.9910s\u001b[0m\t\n",
            "\u001b[0m1187/2115 (epoch 2.806), train_loss = 1.69254569, grad/param norm = 1.7227e-01, time/batch = 0.9259s\u001b[0m\t\n",
            "\u001b[0m1188/2115 (epoch 2.809), train_loss = 1.60675982, grad/param norm = 1.6609e-01, time/batch = 0.7066s\u001b[0m\t\n",
            "\u001b[0m1189/2115 (epoch 2.811), train_loss = 1.68417330, grad/param norm = 1.5757e-01, time/batch = 0.4479s\u001b[0m\t\n",
            "\u001b[0m1190/2115 (epoch 2.813), train_loss = 1.65690789, grad/param norm = 1.5560e-01, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m1191/2115 (epoch 2.816), train_loss = 1.59688068, grad/param norm = 1.2315e-01, time/batch = 0.4173s\u001b[0m\t\n",
            "\u001b[0m1192/2115 (epoch 2.818), train_loss = 1.67519120, grad/param norm = 1.1946e-01, time/batch = 0.4565s\u001b[0m\t\n",
            "\u001b[0m1193/2115 (epoch 2.820), train_loss = 1.62470806, grad/param norm = 1.2045e-01, time/batch = 0.4114s\u001b[0m\t\n",
            "\u001b[0m1194/2115 (epoch 2.823), train_loss = 1.70856913, grad/param norm = 1.3850e-01, time/batch = 0.4468s\u001b[0m\t\n",
            "\u001b[0m1195/2115 (epoch 2.825), train_loss = 1.63733410, grad/param norm = 1.3944e-01, time/batch = 0.4175s\u001b[0m\t\n",
            "\u001b[0m1196/2115 (epoch 2.827), train_loss = 1.74157685, grad/param norm = 1.8127e-01, time/batch = 0.4299s\u001b[0m\t\n",
            "\u001b[0m1197/2115 (epoch 2.830), train_loss = 1.71313442, grad/param norm = 2.0497e-01, time/batch = 0.4325s\u001b[0m\t\n",
            "\u001b[0m1198/2115 (epoch 2.832), train_loss = 1.70381375, grad/param norm = 2.1745e-01, time/batch = 0.4172s\u001b[0m\t\n",
            "\u001b[0m1199/2115 (epoch 2.835), train_loss = 1.61890340, grad/param norm = 1.8355e-01, time/batch = 0.4710s\u001b[0m\t\n",
            "\u001b[0m1200/2115 (epoch 2.837), train_loss = 1.60642369, grad/param norm = 1.6373e-01, time/batch = 0.4190s\u001b[0m\t\n",
            "\u001b[0m1201/2115 (epoch 2.839), train_loss = 1.60466559, grad/param norm = 1.4649e-01, time/batch = 0.4501s\u001b[0m\t\n",
            "\u001b[0m1202/2115 (epoch 2.842), train_loss = 1.63987777, grad/param norm = 1.3465e-01, time/batch = 0.4110s\u001b[0m\t\n",
            "\u001b[0m1203/2115 (epoch 2.844), train_loss = 1.69658553, grad/param norm = 1.3630e-01, time/batch = 0.4304s\u001b[0m\t\n",
            "\u001b[0m1204/2115 (epoch 2.846), train_loss = 1.70562924, grad/param norm = 1.5171e-01, time/batch = 0.4393s\u001b[0m\t\n",
            "\u001b[0m1205/2115 (epoch 2.849), train_loss = 1.60200870, grad/param norm = 1.1858e-01, time/batch = 0.4255s\u001b[0m\t\n",
            "\u001b[0m1206/2115 (epoch 2.851), train_loss = 1.63240221, grad/param norm = 1.3472e-01, time/batch = 0.4403s\u001b[0m\t\n",
            "\u001b[0m1207/2115 (epoch 2.853), train_loss = 1.69337517, grad/param norm = 1.4710e-01, time/batch = 0.4144s\u001b[0m\t\n",
            "\u001b[0m1208/2115 (epoch 2.856), train_loss = 1.64484958, grad/param norm = 1.6369e-01, time/batch = 0.4202s\u001b[0m\t\n",
            "\u001b[0m1209/2115 (epoch 2.858), train_loss = 1.63953849, grad/param norm = 1.3802e-01, time/batch = 0.4335s\u001b[0m\t\n",
            "\u001b[0m1210/2115 (epoch 2.861), train_loss = 1.67894530, grad/param norm = 1.5652e-01, time/batch = 0.4245s\u001b[0m\t\n",
            "\u001b[0m1211/2115 (epoch 2.863), train_loss = 1.68554969, grad/param norm = 1.7891e-01, time/batch = 0.6457s\u001b[0m\t\n",
            "\u001b[0m1212/2115 (epoch 2.865), train_loss = 1.65278488, grad/param norm = 1.9368e-01, time/batch = 0.9118s\u001b[0m\t\n",
            "\u001b[0m1213/2115 (epoch 2.868), train_loss = 1.62080075, grad/param norm = 1.7685e-01, time/batch = 0.9409s\u001b[0m\t\n",
            "\u001b[0m1214/2115 (epoch 2.870), train_loss = 1.62343535, grad/param norm = 1.7229e-01, time/batch = 0.8276s\u001b[0m\t\n",
            "\u001b[0m1215/2115 (epoch 2.872), train_loss = 1.65047078, grad/param norm = 1.7424e-01, time/batch = 0.4892s\u001b[0m\t\n",
            "\u001b[0m1216/2115 (epoch 2.875), train_loss = 1.63630914, grad/param norm = 1.7476e-01, time/batch = 0.4174s\u001b[0m\t\n",
            "\u001b[0m1217/2115 (epoch 2.877), train_loss = 1.77237166, grad/param norm = 1.5089e-01, time/batch = 0.4450s\u001b[0m\t\n",
            "\u001b[0m1218/2115 (epoch 2.879), train_loss = 1.62922196, grad/param norm = 1.3637e-01, time/batch = 0.4254s\u001b[0m\t\n",
            "\u001b[0m1219/2115 (epoch 2.882), train_loss = 1.68265158, grad/param norm = 1.2845e-01, time/batch = 0.4353s\u001b[0m\t\n",
            "\u001b[0m1220/2115 (epoch 2.884), train_loss = 1.66965851, grad/param norm = 1.3804e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m1221/2115 (epoch 2.887), train_loss = 1.66627020, grad/param norm = 1.4589e-01, time/batch = 0.4350s\u001b[0m\t\n",
            "\u001b[0m1222/2115 (epoch 2.889), train_loss = 1.64601139, grad/param norm = 1.3809e-01, time/batch = 0.4282s\u001b[0m\t\n",
            "\u001b[0m1223/2115 (epoch 2.891), train_loss = 1.66512226, grad/param norm = 1.2404e-01, time/batch = 0.4228s\u001b[0m\t\n",
            "\u001b[0m1224/2115 (epoch 2.894), train_loss = 1.62640199, grad/param norm = 1.3010e-01, time/batch = 0.4365s\u001b[0m\t\n",
            "\u001b[0m1225/2115 (epoch 2.896), train_loss = 1.63050374, grad/param norm = 1.2922e-01, time/batch = 0.4310s\u001b[0m\t\n",
            "\u001b[0m1226/2115 (epoch 2.898), train_loss = 1.62873773, grad/param norm = 1.4154e-01, time/batch = 0.4353s\u001b[0m\t\n",
            "\u001b[0m1227/2115 (epoch 2.901), train_loss = 1.67820321, grad/param norm = 1.3964e-01, time/batch = 0.4196s\u001b[0m\t\n",
            "\u001b[0m1228/2115 (epoch 2.903), train_loss = 1.65375288, grad/param norm = 1.3579e-01, time/batch = 0.4209s\u001b[0m\t\n",
            "\u001b[0m1229/2115 (epoch 2.905), train_loss = 1.67973175, grad/param norm = 1.2765e-01, time/batch = 0.4408s\u001b[0m\t\n",
            "\u001b[0m1230/2115 (epoch 2.908), train_loss = 1.64817201, grad/param norm = 1.1965e-01, time/batch = 0.4099s\u001b[0m\t\n",
            "\u001b[0m1231/2115 (epoch 2.910), train_loss = 1.65085404, grad/param norm = 1.2410e-01, time/batch = 0.4579s\u001b[0m\t\n",
            "\u001b[0m1232/2115 (epoch 2.913), train_loss = 1.71189175, grad/param norm = 1.5814e-01, time/batch = 0.4333s\u001b[0m\t\n",
            "\u001b[0m1233/2115 (epoch 2.915), train_loss = 1.67235247, grad/param norm = 1.5175e-01, time/batch = 0.4682s\u001b[0m\t\n",
            "\u001b[0m1234/2115 (epoch 2.917), train_loss = 1.67887149, grad/param norm = 1.5545e-01, time/batch = 0.4397s\u001b[0m\t\n",
            "\u001b[0m1235/2115 (epoch 2.920), train_loss = 1.70369934, grad/param norm = 1.7549e-01, time/batch = 0.4173s\u001b[0m\t\n",
            "\u001b[0m1236/2115 (epoch 2.922), train_loss = 1.64055844, grad/param norm = 1.7450e-01, time/batch = 0.4505s\u001b[0m\t\n",
            "\u001b[0m1237/2115 (epoch 2.924), train_loss = 1.63561262, grad/param norm = 1.5700e-01, time/batch = 0.4515s\u001b[0m\t\n",
            "\u001b[0m1238/2115 (epoch 2.927), train_loss = 1.66109871, grad/param norm = 1.5809e-01, time/batch = 0.7848s\u001b[0m\t\n",
            "\u001b[0m1239/2115 (epoch 2.929), train_loss = 1.67014728, grad/param norm = 1.9151e-01, time/batch = 0.9540s\u001b[0m\t\n",
            "\u001b[0m1240/2115 (epoch 2.931), train_loss = 1.70876151, grad/param norm = 1.8038e-01, time/batch = 0.9371s\u001b[0m\t\n",
            "\u001b[0m1241/2115 (epoch 2.934), train_loss = 1.62209743, grad/param norm = 1.7679e-01, time/batch = 0.6289s\u001b[0m\t\n",
            "\u001b[0m1242/2115 (epoch 2.936), train_loss = 1.68763898, grad/param norm = 1.7209e-01, time/batch = 0.4352s\u001b[0m\t\n",
            "\u001b[0m1243/2115 (epoch 2.939), train_loss = 1.60915414, grad/param norm = 1.5183e-01, time/batch = 0.4810s\u001b[0m\t\n",
            "\u001b[0m1244/2115 (epoch 2.941), train_loss = 1.59009518, grad/param norm = 1.4287e-01, time/batch = 0.4386s\u001b[0m\t\n",
            "\u001b[0m1245/2115 (epoch 2.943), train_loss = 1.64352396, grad/param norm = 1.3281e-01, time/batch = 0.4231s\u001b[0m\t\n",
            "\u001b[0m1246/2115 (epoch 2.946), train_loss = 1.60253060, grad/param norm = 1.3031e-01, time/batch = 0.4446s\u001b[0m\t\n",
            "\u001b[0m1247/2115 (epoch 2.948), train_loss = 1.68943630, grad/param norm = 1.3225e-01, time/batch = 0.4226s\u001b[0m\t\n",
            "\u001b[0m1248/2115 (epoch 2.950), train_loss = 1.66133520, grad/param norm = 1.4507e-01, time/batch = 0.4258s\u001b[0m\t\n",
            "\u001b[0m1249/2115 (epoch 2.953), train_loss = 1.65647756, grad/param norm = 1.4733e-01, time/batch = 0.4376s\u001b[0m\t\n",
            "\u001b[0m1250/2115 (epoch 2.955), train_loss = 1.68187167, grad/param norm = 1.4132e-01, time/batch = 0.4505s\u001b[0m\t\n",
            "\u001b[0m1251/2115 (epoch 2.957), train_loss = 1.65771451, grad/param norm = 1.7259e-01, time/batch = 0.4518s\u001b[0m\t\n",
            "\u001b[0m1252/2115 (epoch 2.960), train_loss = 1.68615888, grad/param norm = 1.9686e-01, time/batch = 0.4231s\u001b[0m\t\n",
            "\u001b[0m1253/2115 (epoch 2.962), train_loss = 1.69798657, grad/param norm = 1.6525e-01, time/batch = 0.4460s\u001b[0m\t\n",
            "\u001b[0m1254/2115 (epoch 2.965), train_loss = 1.62991595, grad/param norm = 1.3884e-01, time/batch = 0.4117s\u001b[0m\t\n",
            "\u001b[0m1255/2115 (epoch 2.967), train_loss = 1.69561773, grad/param norm = 1.3056e-01, time/batch = 0.4277s\u001b[0m\t\n",
            "\u001b[0m1256/2115 (epoch 2.969), train_loss = 1.65183554, grad/param norm = 1.3508e-01, time/batch = 0.4329s\u001b[0m\t\n",
            "\u001b[0m1257/2115 (epoch 2.972), train_loss = 1.69469248, grad/param norm = 1.3787e-01, time/batch = 0.4244s\u001b[0m\t\n",
            "\u001b[0m1258/2115 (epoch 2.974), train_loss = 1.61650880, grad/param norm = 1.3538e-01, time/batch = 0.4489s\u001b[0m\t\n",
            "\u001b[0m1259/2115 (epoch 2.976), train_loss = 1.66866025, grad/param norm = 1.5417e-01, time/batch = 0.4218s\u001b[0m\t\n",
            "\u001b[0m1260/2115 (epoch 2.979), train_loss = 1.61624046, grad/param norm = 1.5944e-01, time/batch = 0.4612s\u001b[0m\t\n",
            "\u001b[0m1261/2115 (epoch 2.981), train_loss = 1.62554877, grad/param norm = 1.3985e-01, time/batch = 0.4314s\u001b[0m\t\n",
            "\u001b[0m1262/2115 (epoch 2.983), train_loss = 1.64520434, grad/param norm = 1.2189e-01, time/batch = 0.4235s\u001b[0m\t\n",
            "\u001b[0m1263/2115 (epoch 2.986), train_loss = 1.63959965, grad/param norm = 1.1990e-01, time/batch = 0.4319s\u001b[0m\t\n",
            "\u001b[0m1264/2115 (epoch 2.988), train_loss = 1.61224111, grad/param norm = 1.1488e-01, time/batch = 0.7307s\u001b[0m\t\n",
            "\u001b[0m1265/2115 (epoch 2.991), train_loss = 1.63620342, grad/param norm = 1.2568e-01, time/batch = 1.0141s\u001b[0m\t\n",
            "\u001b[0m1266/2115 (epoch 2.993), train_loss = 1.67356188, grad/param norm = 1.3034e-01, time/batch = 0.8529s\u001b[0m\t\n",
            "\u001b[0m1267/2115 (epoch 2.995), train_loss = 1.65981291, grad/param norm = 1.3599e-01, time/batch = 0.8118s\u001b[0m\t\n",
            "\u001b[0m1268/2115 (epoch 2.998), train_loss = 1.67799240, grad/param norm = 1.6367e-01, time/batch = 0.4346s\u001b[0m\t\n",
            "\u001b[0m1269/2115 (epoch 3.000), train_loss = 1.71861995, grad/param norm = 2.1138e-01, time/batch = 0.4164s\u001b[0m\t\n",
            "\u001b[0m1270/2115 (epoch 3.002), train_loss = 1.82903316, grad/param norm = 2.1542e-01, time/batch = 0.4190s\u001b[0m\t\n",
            "\u001b[0m1271/2115 (epoch 3.005), train_loss = 1.64979568, grad/param norm = 1.7908e-01, time/batch = 0.4368s\u001b[0m\t\n",
            "\u001b[0m1272/2115 (epoch 3.007), train_loss = 1.64158136, grad/param norm = 1.5064e-01, time/batch = 0.4337s\u001b[0m\t\n",
            "\u001b[0m1273/2115 (epoch 3.009), train_loss = 1.63545522, grad/param norm = 1.3054e-01, time/batch = 0.4395s\u001b[0m\t\n",
            "\u001b[0m1274/2115 (epoch 3.012), train_loss = 1.68426672, grad/param norm = 1.3217e-01, time/batch = 0.4285s\u001b[0m\t\n",
            "\u001b[0m1275/2115 (epoch 3.014), train_loss = 1.66746854, grad/param norm = 1.5572e-01, time/batch = 0.4094s\u001b[0m\t\n",
            "\u001b[0m1276/2115 (epoch 3.017), train_loss = 1.67638353, grad/param norm = 1.4683e-01, time/batch = 0.4378s\u001b[0m\t\n",
            "\u001b[0m1277/2115 (epoch 3.019), train_loss = 1.66873402, grad/param norm = 1.2467e-01, time/batch = 0.4145s\u001b[0m\t\n",
            "\u001b[0m1278/2115 (epoch 3.021), train_loss = 1.61766790, grad/param norm = 1.4929e-01, time/batch = 0.4408s\u001b[0m\t\n",
            "\u001b[0m1279/2115 (epoch 3.024), train_loss = 1.54393832, grad/param norm = 1.4530e-01, time/batch = 0.4254s\u001b[0m\t\n",
            "\u001b[0m1280/2115 (epoch 3.026), train_loss = 1.70162415, grad/param norm = 1.5485e-01, time/batch = 0.4457s\u001b[0m\t\n",
            "\u001b[0m1281/2115 (epoch 3.028), train_loss = 1.59395805, grad/param norm = 1.7151e-01, time/batch = 0.4314s\u001b[0m\t\n",
            "\u001b[0m1282/2115 (epoch 3.031), train_loss = 1.66615978, grad/param norm = 1.5471e-01, time/batch = 0.4269s\u001b[0m\t\n",
            "\u001b[0m1283/2115 (epoch 3.033), train_loss = 1.61063484, grad/param norm = 1.3976e-01, time/batch = 0.4467s\u001b[0m\t\n",
            "\u001b[0m1284/2115 (epoch 3.035), train_loss = 1.60795268, grad/param norm = 1.3733e-01, time/batch = 0.4184s\u001b[0m\t\n",
            "\u001b[0m1285/2115 (epoch 3.038), train_loss = 1.60210731, grad/param norm = 1.4550e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m1286/2115 (epoch 3.040), train_loss = 1.69985170, grad/param norm = 1.5819e-01, time/batch = 0.4172s\u001b[0m\t\n",
            "\u001b[0m1287/2115 (epoch 3.043), train_loss = 1.60415029, grad/param norm = 1.2550e-01, time/batch = 0.4098s\u001b[0m\t\n",
            "\u001b[0m1288/2115 (epoch 3.045), train_loss = 1.61606573, grad/param norm = 1.1784e-01, time/batch = 0.4504s\u001b[0m\t\n",
            "\u001b[0m1289/2115 (epoch 3.047), train_loss = 1.62884494, grad/param norm = 1.3005e-01, time/batch = 0.4127s\u001b[0m\t\n",
            "\u001b[0m1290/2115 (epoch 3.050), train_loss = 1.57565088, grad/param norm = 1.6583e-01, time/batch = 0.5517s\u001b[0m\t\n",
            "\u001b[0m1291/2115 (epoch 3.052), train_loss = 1.61532086, grad/param norm = 1.8983e-01, time/batch = 0.9910s\u001b[0m\t\n",
            "\u001b[0m1292/2115 (epoch 3.054), train_loss = 1.59951573, grad/param norm = 1.7139e-01, time/batch = 0.8878s\u001b[0m\t\n",
            "\u001b[0m1293/2115 (epoch 3.057), train_loss = 1.59084249, grad/param norm = 1.4182e-01, time/batch = 0.9123s\u001b[0m\t\n",
            "\u001b[0m1294/2115 (epoch 3.059), train_loss = 1.55083695, grad/param norm = 1.3658e-01, time/batch = 0.5209s\u001b[0m\t\n",
            "\u001b[0m1295/2115 (epoch 3.061), train_loss = 1.62229662, grad/param norm = 1.4522e-01, time/batch = 0.4359s\u001b[0m\t\n",
            "\u001b[0m1296/2115 (epoch 3.064), train_loss = 1.55789242, grad/param norm = 1.3067e-01, time/batch = 0.4221s\u001b[0m\t\n",
            "\u001b[0m1297/2115 (epoch 3.066), train_loss = 1.59496256, grad/param norm = 1.2437e-01, time/batch = 0.4223s\u001b[0m\t\n",
            "\u001b[0m1298/2115 (epoch 3.069), train_loss = 1.62696765, grad/param norm = 1.1356e-01, time/batch = 0.4373s\u001b[0m\t\n",
            "\u001b[0m1299/2115 (epoch 3.071), train_loss = 1.63901797, grad/param norm = 1.3350e-01, time/batch = 0.4154s\u001b[0m\t\n",
            "\u001b[0m1300/2115 (epoch 3.073), train_loss = 1.62992486, grad/param norm = 1.3955e-01, time/batch = 0.4401s\u001b[0m\t\n",
            "\u001b[0m1301/2115 (epoch 3.076), train_loss = 1.66054216, grad/param norm = 1.6395e-01, time/batch = 0.4198s\u001b[0m\t\n",
            "\u001b[0m1302/2115 (epoch 3.078), train_loss = 1.60524406, grad/param norm = 1.7293e-01, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m1303/2115 (epoch 3.080), train_loss = 1.63102499, grad/param norm = 1.4737e-01, time/batch = 0.4378s\u001b[0m\t\n",
            "\u001b[0m1304/2115 (epoch 3.083), train_loss = 1.67160177, grad/param norm = 1.3553e-01, time/batch = 0.4193s\u001b[0m\t\n",
            "\u001b[0m1305/2115 (epoch 3.085), train_loss = 1.61626774, grad/param norm = 1.5251e-01, time/batch = 0.4407s\u001b[0m\t\n",
            "\u001b[0m1306/2115 (epoch 3.087), train_loss = 1.74150487, grad/param norm = 1.8755e-01, time/batch = 0.4285s\u001b[0m\t\n",
            "\u001b[0m1307/2115 (epoch 3.090), train_loss = 1.72418486, grad/param norm = 1.9832e-01, time/batch = 0.5269s\u001b[0m\t\n",
            "\u001b[0m1308/2115 (epoch 3.092), train_loss = 1.67440391, grad/param norm = 2.0180e-01, time/batch = 0.4239s\u001b[0m\t\n",
            "\u001b[0m1309/2115 (epoch 3.095), train_loss = 1.67880910, grad/param norm = 2.0537e-01, time/batch = 0.4301s\u001b[0m\t\n",
            "\u001b[0m1310/2115 (epoch 3.097), train_loss = 1.69219507, grad/param norm = 1.9250e-01, time/batch = 0.4263s\u001b[0m\t\n",
            "\u001b[0m1311/2115 (epoch 3.099), train_loss = 1.68518038, grad/param norm = 1.5067e-01, time/batch = 0.4343s\u001b[0m\t\n",
            "\u001b[0m1312/2115 (epoch 3.102), train_loss = 1.61100754, grad/param norm = 1.2157e-01, time/batch = 0.4327s\u001b[0m\t\n",
            "\u001b[0m1313/2115 (epoch 3.104), train_loss = 1.58608788, grad/param norm = 1.2744e-01, time/batch = 0.4204s\u001b[0m\t\n",
            "\u001b[0m1314/2115 (epoch 3.106), train_loss = 1.65667275, grad/param norm = 1.5506e-01, time/batch = 0.4476s\u001b[0m\t\n",
            "\u001b[0m1315/2115 (epoch 3.109), train_loss = 1.65735823, grad/param norm = 1.5307e-01, time/batch = 0.4363s\u001b[0m\t\n",
            "\u001b[0m1316/2115 (epoch 3.111), train_loss = 1.60409446, grad/param norm = 1.5893e-01, time/batch = 0.4182s\u001b[0m\t\n",
            "\u001b[0m1317/2115 (epoch 3.113), train_loss = 1.60057362, grad/param norm = 1.5113e-01, time/batch = 0.8772s\u001b[0m\t\n",
            "\u001b[0m1318/2115 (epoch 3.116), train_loss = 1.61752059, grad/param norm = 1.3736e-01, time/batch = 1.0123s\u001b[0m\t\n",
            "\u001b[0m1319/2115 (epoch 3.118), train_loss = 1.57035710, grad/param norm = 1.3966e-01, time/batch = 0.8817s\u001b[0m\t\n",
            "\u001b[0m1320/2115 (epoch 3.121), train_loss = 1.59971908, grad/param norm = 1.1970e-01, time/batch = 0.6996s\u001b[0m\t\n",
            "\u001b[0m1321/2115 (epoch 3.123), train_loss = 1.55400120, grad/param norm = 1.0549e-01, time/batch = 0.4303s\u001b[0m\t\n",
            "\u001b[0m1322/2115 (epoch 3.125), train_loss = 1.57144609, grad/param norm = 1.1892e-01, time/batch = 0.4342s\u001b[0m\t\n",
            "\u001b[0m1323/2115 (epoch 3.128), train_loss = 1.63129996, grad/param norm = 1.4169e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m1324/2115 (epoch 3.130), train_loss = 1.61042818, grad/param norm = 1.5510e-01, time/batch = 0.4195s\u001b[0m\t\n",
            "\u001b[0m1325/2115 (epoch 3.132), train_loss = 1.60127536, grad/param norm = 1.3127e-01, time/batch = 0.4520s\u001b[0m\t\n",
            "\u001b[0m1326/2115 (epoch 3.135), train_loss = 1.57082405, grad/param norm = 1.1931e-01, time/batch = 0.4194s\u001b[0m\t\n",
            "\u001b[0m1327/2115 (epoch 3.137), train_loss = 1.64057720, grad/param norm = 1.1669e-01, time/batch = 0.4504s\u001b[0m\t\n",
            "\u001b[0m1328/2115 (epoch 3.139), train_loss = 1.61537489, grad/param norm = 1.3985e-01, time/batch = 0.4214s\u001b[0m\t\n",
            "\u001b[0m1329/2115 (epoch 3.142), train_loss = 1.62658893, grad/param norm = 1.5282e-01, time/batch = 0.4231s\u001b[0m\t\n",
            "\u001b[0m1330/2115 (epoch 3.144), train_loss = 1.69449254, grad/param norm = 1.8493e-01, time/batch = 0.4440s\u001b[0m\t\n",
            "\u001b[0m1331/2115 (epoch 3.147), train_loss = 1.66545462, grad/param norm = 1.8396e-01, time/batch = 0.4204s\u001b[0m\t\n",
            "\u001b[0m1332/2115 (epoch 3.149), train_loss = 1.63582747, grad/param norm = 1.5397e-01, time/batch = 0.4397s\u001b[0m\t\n",
            "\u001b[0m1333/2115 (epoch 3.151), train_loss = 1.67946340, grad/param norm = 1.6162e-01, time/batch = 0.4246s\u001b[0m\t\n",
            "\u001b[0m1334/2115 (epoch 3.154), train_loss = 1.57638143, grad/param norm = 1.5406e-01, time/batch = 0.4391s\u001b[0m\t\n",
            "\u001b[0m1335/2115 (epoch 3.156), train_loss = 1.65767279, grad/param norm = 1.4428e-01, time/batch = 0.4226s\u001b[0m\t\n",
            "\u001b[0m1336/2115 (epoch 3.158), train_loss = 1.58107798, grad/param norm = 1.2067e-01, time/batch = 0.4172s\u001b[0m\t\n",
            "\u001b[0m1337/2115 (epoch 3.161), train_loss = 1.65262561, grad/param norm = 1.3919e-01, time/batch = 0.4424s\u001b[0m\t\n",
            "\u001b[0m1338/2115 (epoch 3.163), train_loss = 1.68995644, grad/param norm = 1.6048e-01, time/batch = 0.4210s\u001b[0m\t\n",
            "\u001b[0m1339/2115 (epoch 3.165), train_loss = 1.66633410, grad/param norm = 1.6931e-01, time/batch = 0.4492s\u001b[0m\t\n",
            "\u001b[0m1340/2115 (epoch 3.168), train_loss = 1.62939255, grad/param norm = 1.3282e-01, time/batch = 0.4180s\u001b[0m\t\n",
            "\u001b[0m1341/2115 (epoch 3.170), train_loss = 1.56963868, grad/param norm = 1.3426e-01, time/batch = 0.4442s\u001b[0m\t\n",
            "\u001b[0m1342/2115 (epoch 3.173), train_loss = 1.64646811, grad/param norm = 1.3916e-01, time/batch = 0.4176s\u001b[0m\t\n",
            "\u001b[0m1343/2115 (epoch 3.175), train_loss = 1.58841909, grad/param norm = 1.3263e-01, time/batch = 0.7402s\u001b[0m\t\n",
            "\u001b[0m1344/2115 (epoch 3.177), train_loss = 1.57724512, grad/param norm = 1.3049e-01, time/batch = 1.0596s\u001b[0m\t\n",
            "\u001b[0m1345/2115 (epoch 3.180), train_loss = 1.63372103, grad/param norm = 1.3701e-01, time/batch = 0.9475s\u001b[0m\t\n",
            "\u001b[0m1346/2115 (epoch 3.182), train_loss = 1.64383652, grad/param norm = 1.3684e-01, time/batch = 0.6761s\u001b[0m\t\n",
            "\u001b[0m1347/2115 (epoch 3.184), train_loss = 1.63576753, grad/param norm = 1.4112e-01, time/batch = 0.4349s\u001b[0m\t\n",
            "\u001b[0m1348/2115 (epoch 3.187), train_loss = 1.63008128, grad/param norm = 1.4470e-01, time/batch = 0.4238s\u001b[0m\t\n",
            "\u001b[0m1349/2115 (epoch 3.189), train_loss = 1.65430830, grad/param norm = 1.5507e-01, time/batch = 0.4436s\u001b[0m\t\n",
            "\u001b[0m1350/2115 (epoch 3.191), train_loss = 1.63172079, grad/param norm = 1.6220e-01, time/batch = 0.4410s\u001b[0m\t\n",
            "\u001b[0m1351/2115 (epoch 3.194), train_loss = 1.51752982, grad/param norm = 1.4666e-01, time/batch = 0.4507s\u001b[0m\t\n",
            "\u001b[0m1352/2115 (epoch 3.196), train_loss = 1.57809169, grad/param norm = 1.3181e-01, time/batch = 0.4595s\u001b[0m\t\n",
            "\u001b[0m1353/2115 (epoch 3.199), train_loss = 1.61915829, grad/param norm = 1.2758e-01, time/batch = 0.4218s\u001b[0m\t\n",
            "\u001b[0m1354/2115 (epoch 3.201), train_loss = 1.63491890, grad/param norm = 1.2360e-01, time/batch = 0.4482s\u001b[0m\t\n",
            "\u001b[0m1355/2115 (epoch 3.203), train_loss = 1.63504364, grad/param norm = 1.3091e-01, time/batch = 0.4101s\u001b[0m\t\n",
            "\u001b[0m1356/2115 (epoch 3.206), train_loss = 1.59656287, grad/param norm = 1.3053e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m1357/2115 (epoch 3.208), train_loss = 1.56816046, grad/param norm = 1.3263e-01, time/batch = 0.4330s\u001b[0m\t\n",
            "\u001b[0m1358/2115 (epoch 3.210), train_loss = 1.67744215, grad/param norm = 1.4157e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m1359/2115 (epoch 3.213), train_loss = 1.61879784, grad/param norm = 1.6443e-01, time/batch = 0.4414s\u001b[0m\t\n",
            "\u001b[0m1360/2115 (epoch 3.215), train_loss = 1.59195535, grad/param norm = 1.6581e-01, time/batch = 0.4250s\u001b[0m\t\n",
            "\u001b[0m1361/2115 (epoch 3.217), train_loss = 1.57206141, grad/param norm = 1.3965e-01, time/batch = 0.4501s\u001b[0m\t\n",
            "\u001b[0m1362/2115 (epoch 3.220), train_loss = 1.63239006, grad/param norm = 1.3082e-01, time/batch = 0.4313s\u001b[0m\t\n",
            "\u001b[0m1363/2115 (epoch 3.222), train_loss = 1.60122108, grad/param norm = 1.6243e-01, time/batch = 0.4299s\u001b[0m\t\n",
            "\u001b[0m1364/2115 (epoch 3.225), train_loss = 1.64051211, grad/param norm = 1.7524e-01, time/batch = 0.4435s\u001b[0m\t\n",
            "\u001b[0m1365/2115 (epoch 3.227), train_loss = 1.68408621, grad/param norm = 1.6799e-01, time/batch = 0.4741s\u001b[0m\t\n",
            "\u001b[0m1366/2115 (epoch 3.229), train_loss = 1.62223307, grad/param norm = 1.4175e-01, time/batch = 0.4784s\u001b[0m\t\n",
            "\u001b[0m1367/2115 (epoch 3.232), train_loss = 1.64334396, grad/param norm = 1.4295e-01, time/batch = 0.4288s\u001b[0m\t\n",
            "\u001b[0m1368/2115 (epoch 3.234), train_loss = 1.54562155, grad/param norm = 1.3295e-01, time/batch = 0.4485s\u001b[0m\t\n",
            "\u001b[0m1369/2115 (epoch 3.236), train_loss = 1.65484694, grad/param norm = 1.4399e-01, time/batch = 0.7515s\u001b[0m\t\n",
            "\u001b[0m1370/2115 (epoch 3.239), train_loss = 1.59380751, grad/param norm = 1.4823e-01, time/batch = 1.0724s\u001b[0m\t\n",
            "\u001b[0m1371/2115 (epoch 3.241), train_loss = 1.60075559, grad/param norm = 1.3147e-01, time/batch = 0.9760s\u001b[0m\t\n",
            "\u001b[0m1372/2115 (epoch 3.243), train_loss = 1.63653813, grad/param norm = 1.4252e-01, time/batch = 0.5684s\u001b[0m\t\n",
            "\u001b[0m1373/2115 (epoch 3.246), train_loss = 1.59011731, grad/param norm = 1.3338e-01, time/batch = 0.4297s\u001b[0m\t\n",
            "\u001b[0m1374/2115 (epoch 3.248), train_loss = 1.63666298, grad/param norm = 1.3544e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m1375/2115 (epoch 3.251), train_loss = 1.61917512, grad/param norm = 1.2652e-01, time/batch = 0.4260s\u001b[0m\t\n",
            "\u001b[0m1376/2115 (epoch 3.253), train_loss = 1.57420961, grad/param norm = 1.3197e-01, time/batch = 0.4418s\u001b[0m\t\n",
            "\u001b[0m1377/2115 (epoch 3.255), train_loss = 1.56983318, grad/param norm = 1.2624e-01, time/batch = 0.4382s\u001b[0m\t\n",
            "\u001b[0m1378/2115 (epoch 3.258), train_loss = 1.65013843, grad/param norm = 1.2607e-01, time/batch = 0.4196s\u001b[0m\t\n",
            "\u001b[0m1379/2115 (epoch 3.260), train_loss = 1.53978532, grad/param norm = 1.2763e-01, time/batch = 0.4451s\u001b[0m\t\n",
            "\u001b[0m1380/2115 (epoch 3.262), train_loss = 1.52320311, grad/param norm = 1.2792e-01, time/batch = 0.4168s\u001b[0m\t\n",
            "\u001b[0m1381/2115 (epoch 3.265), train_loss = 1.60650607, grad/param norm = 1.3114e-01, time/batch = 0.4492s\u001b[0m\t\n",
            "\u001b[0m1382/2115 (epoch 3.267), train_loss = 1.62442964, grad/param norm = 1.2846e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m1383/2115 (epoch 3.270), train_loss = 1.61620536, grad/param norm = 1.3941e-01, time/batch = 0.4332s\u001b[0m\t\n",
            "\u001b[0m1384/2115 (epoch 3.272), train_loss = 1.63306697, grad/param norm = 1.3204e-01, time/batch = 0.4235s\u001b[0m\t\n",
            "\u001b[0m1385/2115 (epoch 3.274), train_loss = 1.64756420, grad/param norm = 1.4118e-01, time/batch = 0.4206s\u001b[0m\t\n",
            "\u001b[0m1386/2115 (epoch 3.277), train_loss = 1.61763361, grad/param norm = 1.5563e-01, time/batch = 0.4496s\u001b[0m\t\n",
            "\u001b[0m1387/2115 (epoch 3.279), train_loss = 1.66587755, grad/param norm = 1.4372e-01, time/batch = 0.4181s\u001b[0m\t\n",
            "\u001b[0m1388/2115 (epoch 3.281), train_loss = 1.63230998, grad/param norm = 1.5474e-01, time/batch = 0.4475s\u001b[0m\t\n",
            "\u001b[0m1389/2115 (epoch 3.284), train_loss = 1.54378306, grad/param norm = 1.6858e-01, time/batch = 0.4206s\u001b[0m\t\n",
            "\u001b[0m1390/2115 (epoch 3.286), train_loss = 1.63104416, grad/param norm = 1.4942e-01, time/batch = 0.4145s\u001b[0m\t\n",
            "\u001b[0m1391/2115 (epoch 3.288), train_loss = 1.62923283, grad/param norm = 1.3365e-01, time/batch = 0.4355s\u001b[0m\t\n",
            "\u001b[0m1392/2115 (epoch 3.291), train_loss = 1.56086145, grad/param norm = 1.3117e-01, time/batch = 0.4140s\u001b[0m\t\n",
            "\u001b[0m1393/2115 (epoch 3.293), train_loss = 1.57428342, grad/param norm = 1.3658e-01, time/batch = 0.4410s\u001b[0m\t\n",
            "\u001b[0m1394/2115 (epoch 3.296), train_loss = 1.64828065, grad/param norm = 1.7424e-01, time/batch = 0.4168s\u001b[0m\t\n",
            "\u001b[0m1395/2115 (epoch 3.298), train_loss = 1.69549619, grad/param norm = 1.9954e-01, time/batch = 0.8160s\u001b[0m\t\n",
            "\u001b[0m1396/2115 (epoch 3.300), train_loss = 1.62552105, grad/param norm = 2.1969e-01, time/batch = 0.8250s\u001b[0m\t\n",
            "\u001b[0m1397/2115 (epoch 3.303), train_loss = 1.66716441, grad/param norm = 1.8731e-01, time/batch = 1.0330s\u001b[0m\t\n",
            "\u001b[0m1398/2115 (epoch 3.305), train_loss = 1.66159979, grad/param norm = 1.5641e-01, time/batch = 0.7527s\u001b[0m\t\n",
            "\u001b[0m1399/2115 (epoch 3.307), train_loss = 1.56902450, grad/param norm = 1.1990e-01, time/batch = 0.4428s\u001b[0m\t\n",
            "\u001b[0m1400/2115 (epoch 3.310), train_loss = 1.53183409, grad/param norm = 1.1822e-01, time/batch = 0.4159s\u001b[0m\t\n",
            "\u001b[0m1401/2115 (epoch 3.312), train_loss = 1.61343004, grad/param norm = 1.2585e-01, time/batch = 0.4448s\u001b[0m\t\n",
            "\u001b[0m1402/2115 (epoch 3.314), train_loss = 1.66085609, grad/param norm = 1.3117e-01, time/batch = 0.4131s\u001b[0m\t\n",
            "\u001b[0m1403/2115 (epoch 3.317), train_loss = 1.61291643, grad/param norm = 1.4073e-01, time/batch = 0.4390s\u001b[0m\t\n",
            "\u001b[0m1404/2115 (epoch 3.319), train_loss = 1.58962470, grad/param norm = 1.3431e-01, time/batch = 0.4114s\u001b[0m\t\n",
            "\u001b[0m1405/2115 (epoch 3.322), train_loss = 1.60952053, grad/param norm = 1.1567e-01, time/batch = 0.4247s\u001b[0m\t\n",
            "\u001b[0m1406/2115 (epoch 3.324), train_loss = 1.66498705, grad/param norm = 1.3207e-01, time/batch = 0.4334s\u001b[0m\t\n",
            "\u001b[0m1407/2115 (epoch 3.326), train_loss = 1.67776849, grad/param norm = 1.3530e-01, time/batch = 0.4199s\u001b[0m\t\n",
            "\u001b[0m1408/2115 (epoch 3.329), train_loss = 1.66696283, grad/param norm = 1.4300e-01, time/batch = 0.4328s\u001b[0m\t\n",
            "\u001b[0m1409/2115 (epoch 3.331), train_loss = 1.65006487, grad/param norm = 1.3801e-01, time/batch = 0.4237s\u001b[0m\t\n",
            "\u001b[0m1410/2115 (epoch 3.333), train_loss = 1.65532095, grad/param norm = 1.4989e-01, time/batch = 0.4189s\u001b[0m\t\n",
            "\u001b[0m1411/2115 (epoch 3.336), train_loss = 1.66416078, grad/param norm = 1.3746e-01, time/batch = 0.4477s\u001b[0m\t\n",
            "\u001b[0m1412/2115 (epoch 3.338), train_loss = 1.59038646, grad/param norm = 1.5297e-01, time/batch = 0.4244s\u001b[0m\t\n",
            "\u001b[0m1413/2115 (epoch 3.340), train_loss = 1.58108414, grad/param norm = 1.4177e-01, time/batch = 0.4464s\u001b[0m\t\n",
            "\u001b[0m1414/2115 (epoch 3.343), train_loss = 1.61545802, grad/param norm = 1.4652e-01, time/batch = 0.4158s\u001b[0m\t\n",
            "\u001b[0m1415/2115 (epoch 3.345), train_loss = 1.62364642, grad/param norm = 1.3501e-01, time/batch = 0.4469s\u001b[0m\t\n",
            "\u001b[0m1416/2115 (epoch 3.348), train_loss = 1.60730414, grad/param norm = 1.2708e-01, time/batch = 0.4247s\u001b[0m\t\n",
            "\u001b[0m1417/2115 (epoch 3.350), train_loss = 1.58005323, grad/param norm = 1.1347e-01, time/batch = 0.4270s\u001b[0m\t\n",
            "\u001b[0m1418/2115 (epoch 3.352), train_loss = 1.60112128, grad/param norm = 1.0886e-01, time/batch = 0.4483s\u001b[0m\t\n",
            "\u001b[0m1419/2115 (epoch 3.355), train_loss = 1.60176654, grad/param norm = 1.1570e-01, time/batch = 0.4158s\u001b[0m\t\n",
            "\u001b[0m1420/2115 (epoch 3.357), train_loss = 1.55149770, grad/param norm = 1.2960e-01, time/batch = 0.4379s\u001b[0m\t\n",
            "\u001b[0m1421/2115 (epoch 3.359), train_loss = 1.64402497, grad/param norm = 1.5406e-01, time/batch = 0.5638s\u001b[0m\t\n",
            "\u001b[0m1422/2115 (epoch 3.362), train_loss = 1.59332022, grad/param norm = 1.5972e-01, time/batch = 0.9076s\u001b[0m\t\n",
            "\u001b[0m1423/2115 (epoch 3.364), train_loss = 1.59431779, grad/param norm = 1.4370e-01, time/batch = 0.8965s\u001b[0m\t\n",
            "\u001b[0m1424/2115 (epoch 3.366), train_loss = 1.61087702, grad/param norm = 1.4152e-01, time/batch = 0.7854s\u001b[0m\t\n",
            "\u001b[0m1425/2115 (epoch 3.369), train_loss = 1.62520267, grad/param norm = 1.2757e-01, time/batch = 0.5964s\u001b[0m\t\n",
            "\u001b[0m1426/2115 (epoch 3.371), train_loss = 1.60609977, grad/param norm = 1.3334e-01, time/batch = 0.4419s\u001b[0m\t\n",
            "\u001b[0m1427/2115 (epoch 3.374), train_loss = 1.61360806, grad/param norm = 1.2948e-01, time/batch = 0.4188s\u001b[0m\t\n",
            "\u001b[0m1428/2115 (epoch 3.376), train_loss = 1.62895362, grad/param norm = 1.3538e-01, time/batch = 0.4274s\u001b[0m\t\n",
            "\u001b[0m1429/2115 (epoch 3.378), train_loss = 1.61758235, grad/param norm = 1.3529e-01, time/batch = 0.4276s\u001b[0m\t\n",
            "\u001b[0m1430/2115 (epoch 3.381), train_loss = 1.59709535, grad/param norm = 1.4564e-01, time/batch = 0.4096s\u001b[0m\t\n",
            "\u001b[0m1431/2115 (epoch 3.383), train_loss = 1.73460186, grad/param norm = 1.4945e-01, time/batch = 0.4412s\u001b[0m\t\n",
            "\u001b[0m1432/2115 (epoch 3.385), train_loss = 1.64513085, grad/param norm = 1.4842e-01, time/batch = 0.5002s\u001b[0m\t\n",
            "\u001b[0m1433/2115 (epoch 3.388), train_loss = 1.61559784, grad/param norm = 1.5918e-01, time/batch = 0.4609s\u001b[0m\t\n",
            "\u001b[0m1434/2115 (epoch 3.390), train_loss = 1.66956751, grad/param norm = 1.6615e-01, time/batch = 0.4305s\u001b[0m\t\n",
            "\u001b[0m1435/2115 (epoch 3.392), train_loss = 1.64913418, grad/param norm = 1.7835e-01, time/batch = 0.4557s\u001b[0m\t\n",
            "\u001b[0m1436/2115 (epoch 3.395), train_loss = 1.60501325, grad/param norm = 1.5244e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m1437/2115 (epoch 3.397), train_loss = 1.56847426, grad/param norm = 1.5102e-01, time/batch = 0.4135s\u001b[0m\t\n",
            "\u001b[0m1438/2115 (epoch 3.400), train_loss = 1.64086981, grad/param norm = 1.5549e-01, time/batch = 0.4431s\u001b[0m\t\n",
            "\u001b[0m1439/2115 (epoch 3.402), train_loss = 1.67634068, grad/param norm = 1.4896e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m1440/2115 (epoch 3.404), train_loss = 1.64738320, grad/param norm = 1.4416e-01, time/batch = 0.4468s\u001b[0m\t\n",
            "\u001b[0m1441/2115 (epoch 3.407), train_loss = 1.65143064, grad/param norm = 1.2595e-01, time/batch = 0.4410s\u001b[0m\t\n",
            "\u001b[0m1442/2115 (epoch 3.409), train_loss = 1.62452902, grad/param norm = 1.2389e-01, time/batch = 0.4711s\u001b[0m\t\n",
            "\u001b[0m1443/2115 (epoch 3.411), train_loss = 1.58720043, grad/param norm = 1.2304e-01, time/batch = 0.4291s\u001b[0m\t\n",
            "\u001b[0m1444/2115 (epoch 3.414), train_loss = 1.63604994, grad/param norm = 1.2533e-01, time/batch = 0.4219s\u001b[0m\t\n",
            "\u001b[0m1445/2115 (epoch 3.416), train_loss = 1.64905489, grad/param norm = 1.2454e-01, time/batch = 0.4464s\u001b[0m\t\n",
            "\u001b[0m1446/2115 (epoch 3.418), train_loss = 1.61675210, grad/param norm = 1.3510e-01, time/batch = 0.4243s\u001b[0m\t\n",
            "\u001b[0m1447/2115 (epoch 3.421), train_loss = 1.58592871, grad/param norm = 1.1512e-01, time/batch = 0.4625s\u001b[0m\t\n",
            "\u001b[0m1448/2115 (epoch 3.423), train_loss = 1.54621480, grad/param norm = 1.1905e-01, time/batch = 0.9160s\u001b[0m\t\n",
            "\u001b[0m1449/2115 (epoch 3.426), train_loss = 1.58953761, grad/param norm = 1.4303e-01, time/batch = 0.9269s\u001b[0m\t\n",
            "\u001b[0m1450/2115 (epoch 3.428), train_loss = 1.60980659, grad/param norm = 1.7172e-01, time/batch = 0.8814s\u001b[0m\t\n",
            "\u001b[0m1451/2115 (epoch 3.430), train_loss = 1.59426815, grad/param norm = 1.6704e-01, time/batch = 0.5799s\u001b[0m\t\n",
            "\u001b[0m1452/2115 (epoch 3.433), train_loss = 1.62711555, grad/param norm = 1.2823e-01, time/batch = 0.4150s\u001b[0m\t\n",
            "\u001b[0m1453/2115 (epoch 3.435), train_loss = 1.60408499, grad/param norm = 1.1338e-01, time/batch = 0.4500s\u001b[0m\t\n",
            "\u001b[0m1454/2115 (epoch 3.437), train_loss = 1.50145935, grad/param norm = 1.1905e-01, time/batch = 0.4319s\u001b[0m\t\n",
            "\u001b[0m1455/2115 (epoch 3.440), train_loss = 1.56068681, grad/param norm = 1.2275e-01, time/batch = 0.4391s\u001b[0m\t\n",
            "\u001b[0m1456/2115 (epoch 3.442), train_loss = 1.61615662, grad/param norm = 1.1607e-01, time/batch = 0.4318s\u001b[0m\t\n",
            "\u001b[0m1457/2115 (epoch 3.444), train_loss = 1.61147868, grad/param norm = 1.2654e-01, time/batch = 0.4337s\u001b[0m\t\n",
            "\u001b[0m1458/2115 (epoch 3.447), train_loss = 1.62693414, grad/param norm = 1.5533e-01, time/batch = 0.4376s\u001b[0m\t\n",
            "\u001b[0m1459/2115 (epoch 3.449), train_loss = 1.52417127, grad/param norm = 1.4662e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m1460/2115 (epoch 3.452), train_loss = 1.62321422, grad/param norm = 1.5694e-01, time/batch = 0.4392s\u001b[0m\t\n",
            "\u001b[0m1461/2115 (epoch 3.454), train_loss = 1.53205895, grad/param norm = 1.2111e-01, time/batch = 0.4266s\u001b[0m\t\n",
            "\u001b[0m1462/2115 (epoch 3.456), train_loss = 1.56288986, grad/param norm = 1.2746e-01, time/batch = 0.4405s\u001b[0m\t\n",
            "\u001b[0m1463/2115 (epoch 3.459), train_loss = 1.58511535, grad/param norm = 1.4208e-01, time/batch = 0.4246s\u001b[0m\t\n",
            "\u001b[0m1464/2115 (epoch 3.461), train_loss = 1.58646694, grad/param norm = 1.4432e-01, time/batch = 0.4243s\u001b[0m\t\n",
            "\u001b[0m1465/2115 (epoch 3.463), train_loss = 1.55594218, grad/param norm = 1.6502e-01, time/batch = 0.4407s\u001b[0m\t\n",
            "\u001b[0m1466/2115 (epoch 3.466), train_loss = 1.56728752, grad/param norm = 1.6405e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m1467/2115 (epoch 3.468), train_loss = 1.60764098, grad/param norm = 1.3589e-01, time/batch = 0.4433s\u001b[0m\t\n",
            "\u001b[0m1468/2115 (epoch 3.470), train_loss = 1.62518765, grad/param norm = 1.3442e-01, time/batch = 0.4218s\u001b[0m\t\n",
            "\u001b[0m1469/2115 (epoch 3.473), train_loss = 1.63733571, grad/param norm = 1.5386e-01, time/batch = 0.4236s\u001b[0m\t\n",
            "\u001b[0m1470/2115 (epoch 3.475), train_loss = 1.65617168, grad/param norm = 1.5718e-01, time/batch = 0.4388s\u001b[0m\t\n",
            "\u001b[0m1471/2115 (epoch 3.478), train_loss = 1.59725785, grad/param norm = 1.5802e-01, time/batch = 0.4343s\u001b[0m\t\n",
            "\u001b[0m1472/2115 (epoch 3.480), train_loss = 1.62479426, grad/param norm = 1.7699e-01, time/batch = 0.4579s\u001b[0m\t\n",
            "\u001b[0m1473/2115 (epoch 3.482), train_loss = 1.56163241, grad/param norm = 1.6069e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m1474/2115 (epoch 3.485), train_loss = 1.58222609, grad/param norm = 1.4893e-01, time/batch = 0.7159s\u001b[0m\t\n",
            "\u001b[0m1475/2115 (epoch 3.487), train_loss = 1.59800292, grad/param norm = 1.3329e-01, time/batch = 0.8831s\u001b[0m\t\n",
            "\u001b[0m1476/2115 (epoch 3.489), train_loss = 1.62734772, grad/param norm = 1.4084e-01, time/batch = 1.0139s\u001b[0m\t\n",
            "\u001b[0m1477/2115 (epoch 3.492), train_loss = 1.62823737, grad/param norm = 1.3285e-01, time/batch = 0.8174s\u001b[0m\t\n",
            "\u001b[0m1478/2115 (epoch 3.494), train_loss = 1.66489535, grad/param norm = 1.2852e-01, time/batch = 0.4376s\u001b[0m\t\n",
            "\u001b[0m1479/2115 (epoch 3.496), train_loss = 1.60182895, grad/param norm = 1.3048e-01, time/batch = 0.4328s\u001b[0m\t\n",
            "\u001b[0m1480/2115 (epoch 3.499), train_loss = 1.59792340, grad/param norm = 1.2216e-01, time/batch = 0.4402s\u001b[0m\t\n",
            "\u001b[0m1481/2115 (epoch 3.501), train_loss = 1.56897375, grad/param norm = 1.2467e-01, time/batch = 0.4251s\u001b[0m\t\n",
            "\u001b[0m1482/2115 (epoch 3.504), train_loss = 1.59154399, grad/param norm = 1.2267e-01, time/batch = 0.4421s\u001b[0m\t\n",
            "\u001b[0m1483/2115 (epoch 3.506), train_loss = 1.60372995, grad/param norm = 1.4682e-01, time/batch = 0.4181s\u001b[0m\t\n",
            "\u001b[0m1484/2115 (epoch 3.508), train_loss = 1.55512990, grad/param norm = 1.5490e-01, time/batch = 0.4188s\u001b[0m\t\n",
            "\u001b[0m1485/2115 (epoch 3.511), train_loss = 1.57138501, grad/param norm = 1.2966e-01, time/batch = 0.4361s\u001b[0m\t\n",
            "\u001b[0m1486/2115 (epoch 3.513), train_loss = 1.58522467, grad/param norm = 1.2343e-01, time/batch = 0.4247s\u001b[0m\t\n",
            "\u001b[0m1487/2115 (epoch 3.515), train_loss = 1.55681510, grad/param norm = 1.1310e-01, time/batch = 0.4378s\u001b[0m\t\n",
            "\u001b[0m1488/2115 (epoch 3.518), train_loss = 1.55910402, grad/param norm = 1.2918e-01, time/batch = 0.4297s\u001b[0m\t\n",
            "\u001b[0m1489/2115 (epoch 3.520), train_loss = 1.58425252, grad/param norm = 1.2350e-01, time/batch = 0.4177s\u001b[0m\t\n",
            "\u001b[0m1490/2115 (epoch 3.522), train_loss = 1.60431198, grad/param norm = 1.3976e-01, time/batch = 0.4408s\u001b[0m\t\n",
            "\u001b[0m1491/2115 (epoch 3.525), train_loss = 1.61053480, grad/param norm = 1.3985e-01, time/batch = 0.4204s\u001b[0m\t\n",
            "\u001b[0m1492/2115 (epoch 3.527), train_loss = 1.59789915, grad/param norm = 1.5797e-01, time/batch = 0.4450s\u001b[0m\t\n",
            "\u001b[0m1493/2115 (epoch 3.530), train_loss = 1.58724261, grad/param norm = 1.4251e-01, time/batch = 0.4237s\u001b[0m\t\n",
            "\u001b[0m1494/2115 (epoch 3.532), train_loss = 1.53747575, grad/param norm = 1.3509e-01, time/batch = 0.4594s\u001b[0m\t\n",
            "\u001b[0m1495/2115 (epoch 3.534), train_loss = 1.55830564, grad/param norm = 1.2718e-01, time/batch = 0.4875s\u001b[0m\t\n",
            "\u001b[0m1496/2115 (epoch 3.537), train_loss = 1.65171318, grad/param norm = 1.2591e-01, time/batch = 0.4493s\u001b[0m\t\n",
            "\u001b[0m1497/2115 (epoch 3.539), train_loss = 1.58096291, grad/param norm = 1.2167e-01, time/batch = 0.4255s\u001b[0m\t\n",
            "\u001b[0m1498/2115 (epoch 3.541), train_loss = 1.60362389, grad/param norm = 1.3002e-01, time/batch = 0.4245s\u001b[0m\t\n",
            "\u001b[0m1499/2115 (epoch 3.544), train_loss = 1.60512858, grad/param norm = 1.2397e-01, time/batch = 0.4732s\u001b[0m\t\n",
            "\u001b[0m1500/2115 (epoch 3.546), train_loss = 1.64052275, grad/param norm = 1.4567e-01, time/batch = 0.7059s\u001b[0m\t\n",
            "\u001b[0m1501/2115 (epoch 3.548), train_loss = 1.58114915, grad/param norm = 1.3008e-01, time/batch = 0.9419s\u001b[0m\t\n",
            "\u001b[0m1502/2115 (epoch 3.551), train_loss = 1.60319928, grad/param norm = 1.3111e-01, time/batch = 0.9290s\u001b[0m\t\n",
            "\u001b[0m1503/2115 (epoch 3.553), train_loss = 1.64238057, grad/param norm = 1.3104e-01, time/batch = 0.8244s\u001b[0m\t\n",
            "\u001b[0m1504/2115 (epoch 3.556), train_loss = 1.62564361, grad/param norm = 1.4697e-01, time/batch = 0.4421s\u001b[0m\t\n",
            "\u001b[0m1505/2115 (epoch 3.558), train_loss = 1.58815170, grad/param norm = 1.4860e-01, time/batch = 0.4207s\u001b[0m\t\n",
            "\u001b[0m1506/2115 (epoch 3.560), train_loss = 1.61939380, grad/param norm = 1.3867e-01, time/batch = 0.4218s\u001b[0m\t\n",
            "\u001b[0m1507/2115 (epoch 3.563), train_loss = 1.54966921, grad/param norm = 1.3126e-01, time/batch = 0.5105s\u001b[0m\t\n",
            "\u001b[0m1508/2115 (epoch 3.565), train_loss = 1.60140101, grad/param norm = 1.3915e-01, time/batch = 0.4182s\u001b[0m\t\n",
            "\u001b[0m1509/2115 (epoch 3.567), train_loss = 1.58225387, grad/param norm = 1.2551e-01, time/batch = 0.4375s\u001b[0m\t\n",
            "\u001b[0m1510/2115 (epoch 3.570), train_loss = 1.61398385, grad/param norm = 1.1960e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m1511/2115 (epoch 3.572), train_loss = 1.54585382, grad/param norm = 1.2191e-01, time/batch = 0.4428s\u001b[0m\t\n",
            "\u001b[0m1512/2115 (epoch 3.574), train_loss = 1.56970869, grad/param norm = 1.1076e-01, time/batch = 0.4254s\u001b[0m\t\n",
            "\u001b[0m1513/2115 (epoch 3.577), train_loss = 1.58659669, grad/param norm = 1.2659e-01, time/batch = 0.4222s\u001b[0m\t\n",
            "\u001b[0m1514/2115 (epoch 3.579), train_loss = 1.54065228, grad/param norm = 1.2579e-01, time/batch = 0.4398s\u001b[0m\t\n",
            "\u001b[0m1515/2115 (epoch 3.582), train_loss = 1.57406155, grad/param norm = 1.3711e-01, time/batch = 0.4265s\u001b[0m\t\n",
            "\u001b[0m1516/2115 (epoch 3.584), train_loss = 1.60626895, grad/param norm = 1.5492e-01, time/batch = 0.4471s\u001b[0m\t\n",
            "\u001b[0m1517/2115 (epoch 3.586), train_loss = 1.58210802, grad/param norm = 1.6258e-01, time/batch = 0.4205s\u001b[0m\t\n",
            "\u001b[0m1518/2115 (epoch 3.589), train_loss = 1.65437441, grad/param norm = 1.9659e-01, time/batch = 0.4148s\u001b[0m\t\n",
            "\u001b[0m1519/2115 (epoch 3.591), train_loss = 1.56833439, grad/param norm = 1.7927e-01, time/batch = 0.4549s\u001b[0m\t\n",
            "\u001b[0m1520/2115 (epoch 3.593), train_loss = 1.62454628, grad/param norm = 1.7350e-01, time/batch = 0.4189s\u001b[0m\t\n",
            "\u001b[0m1521/2115 (epoch 3.596), train_loss = 1.54500083, grad/param norm = 1.2416e-01, time/batch = 0.4528s\u001b[0m\t\n",
            "\u001b[0m1522/2115 (epoch 3.598), train_loss = 1.60787840, grad/param norm = 1.1527e-01, time/batch = 0.4188s\u001b[0m\t\n",
            "\u001b[0m1523/2115 (epoch 3.600), train_loss = 1.58284750, grad/param norm = 1.2943e-01, time/batch = 0.4421s\u001b[0m\t\n",
            "\u001b[0m1524/2115 (epoch 3.603), train_loss = 1.57001400, grad/param norm = 1.3643e-01, time/batch = 0.4057s\u001b[0m\t\n",
            "\u001b[0m1525/2115 (epoch 3.605), train_loss = 1.60903283, grad/param norm = 1.4723e-01, time/batch = 0.4225s\u001b[0m\t\n",
            "\u001b[0m1526/2115 (epoch 3.608), train_loss = 1.54331945, grad/param norm = 1.6147e-01, time/batch = 0.5987s\u001b[0m\t\n",
            "\u001b[0m1527/2115 (epoch 3.610), train_loss = 1.58140634, grad/param norm = 1.6166e-01, time/batch = 0.9158s\u001b[0m\t\n",
            "\u001b[0m1528/2115 (epoch 3.612), train_loss = 1.59166341, grad/param norm = 1.7066e-01, time/batch = 1.0105s\u001b[0m\t\n",
            "\u001b[0m1529/2115 (epoch 3.615), train_loss = 1.57262255, grad/param norm = 1.7304e-01, time/batch = 0.7610s\u001b[0m\t\n",
            "\u001b[0m1530/2115 (epoch 3.617), train_loss = 1.55318819, grad/param norm = 1.4310e-01, time/batch = 0.5130s\u001b[0m\t\n",
            "\u001b[0m1531/2115 (epoch 3.619), train_loss = 1.55363944, grad/param norm = 1.1564e-01, time/batch = 0.4420s\u001b[0m\t\n",
            "\u001b[0m1532/2115 (epoch 3.622), train_loss = 1.52341141, grad/param norm = 1.3183e-01, time/batch = 0.4386s\u001b[0m\t\n",
            "\u001b[0m1533/2115 (epoch 3.624), train_loss = 1.60076509, grad/param norm = 1.8291e-01, time/batch = 0.4327s\u001b[0m\t\n",
            "\u001b[0m1534/2115 (epoch 3.626), train_loss = 1.54624597, grad/param norm = 1.4352e-01, time/batch = 0.4418s\u001b[0m\t\n",
            "\u001b[0m1535/2115 (epoch 3.629), train_loss = 1.62793029, grad/param norm = 1.3512e-01, time/batch = 0.4496s\u001b[0m\t\n",
            "\u001b[0m1536/2115 (epoch 3.631), train_loss = 1.61971658, grad/param norm = 1.0722e-01, time/batch = 0.4343s\u001b[0m\t\n",
            "\u001b[0m1537/2115 (epoch 3.634), train_loss = 1.59412438, grad/param norm = 1.2184e-01, time/batch = 0.4220s\u001b[0m\t\n",
            "\u001b[0m1538/2115 (epoch 3.636), train_loss = 1.56134098, grad/param norm = 1.2551e-01, time/batch = 0.4133s\u001b[0m\t\n",
            "\u001b[0m1539/2115 (epoch 3.638), train_loss = 1.55715981, grad/param norm = 1.1214e-01, time/batch = 0.4392s\u001b[0m\t\n",
            "\u001b[0m1540/2115 (epoch 3.641), train_loss = 1.56827563, grad/param norm = 1.0249e-01, time/batch = 0.4223s\u001b[0m\t\n",
            "\u001b[0m1541/2115 (epoch 3.643), train_loss = 1.59034867, grad/param norm = 1.1853e-01, time/batch = 0.4465s\u001b[0m\t\n",
            "\u001b[0m1542/2115 (epoch 3.645), train_loss = 1.52209491, grad/param norm = 1.3308e-01, time/batch = 0.4210s\u001b[0m\t\n",
            "\u001b[0m1543/2115 (epoch 3.648), train_loss = 1.63823657, grad/param norm = 1.3914e-01, time/batch = 0.4377s\u001b[0m\t\n",
            "\u001b[0m1544/2115 (epoch 3.650), train_loss = 1.60797921, grad/param norm = 1.4317e-01, time/batch = 0.4301s\u001b[0m\t\n",
            "\u001b[0m1545/2115 (epoch 3.652), train_loss = 1.62032981, grad/param norm = 1.3131e-01, time/batch = 0.4233s\u001b[0m\t\n",
            "\u001b[0m1546/2115 (epoch 3.655), train_loss = 1.61125125, grad/param norm = 1.3013e-01, time/batch = 0.4405s\u001b[0m\t\n",
            "\u001b[0m1547/2115 (epoch 3.657), train_loss = 1.59657595, grad/param norm = 1.1674e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m1548/2115 (epoch 3.660), train_loss = 1.56348439, grad/param norm = 1.1739e-01, time/batch = 0.4426s\u001b[0m\t\n",
            "\u001b[0m1549/2115 (epoch 3.662), train_loss = 1.59249622, grad/param norm = 1.3953e-01, time/batch = 0.4248s\u001b[0m\t\n",
            "\u001b[0m1550/2115 (epoch 3.664), train_loss = 1.55638237, grad/param norm = 1.3840e-01, time/batch = 0.4151s\u001b[0m\t\n",
            "\u001b[0m1551/2115 (epoch 3.667), train_loss = 1.62532086, grad/param norm = 1.2339e-01, time/batch = 0.4450s\u001b[0m\t\n",
            "\u001b[0m1552/2115 (epoch 3.669), train_loss = 1.55694695, grad/param norm = 1.1181e-01, time/batch = 0.4295s\u001b[0m\t\n",
            "\u001b[0m1553/2115 (epoch 3.671), train_loss = 1.54003085, grad/param norm = 1.1707e-01, time/batch = 0.8219s\u001b[0m\t\n",
            "\u001b[0m1554/2115 (epoch 3.674), train_loss = 1.57797675, grad/param norm = 1.3375e-01, time/batch = 1.0150s\u001b[0m\t\n",
            "\u001b[0m1555/2115 (epoch 3.676), train_loss = 1.54359782, grad/param norm = 1.3236e-01, time/batch = 0.8836s\u001b[0m\t\n",
            "\u001b[0m1556/2115 (epoch 3.678), train_loss = 1.54440623, grad/param norm = 1.2254e-01, time/batch = 0.7839s\u001b[0m\t\n",
            "\u001b[0m1557/2115 (epoch 3.681), train_loss = 1.61289232, grad/param norm = 1.2510e-01, time/batch = 0.4433s\u001b[0m\t\n",
            "\u001b[0m1558/2115 (epoch 3.683), train_loss = 1.60871528, grad/param norm = 1.4018e-01, time/batch = 0.4246s\u001b[0m\t\n",
            "\u001b[0m1559/2115 (epoch 3.686), train_loss = 1.57703539, grad/param norm = 1.3671e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m1560/2115 (epoch 3.688), train_loss = 1.57176564, grad/param norm = 1.2854e-01, time/batch = 0.4205s\u001b[0m\t\n",
            "\u001b[0m1561/2115 (epoch 3.690), train_loss = 1.55494158, grad/param norm = 1.4625e-01, time/batch = 0.4434s\u001b[0m\t\n",
            "\u001b[0m1562/2115 (epoch 3.693), train_loss = 1.59157557, grad/param norm = 1.5049e-01, time/batch = 0.4235s\u001b[0m\t\n",
            "\u001b[0m1563/2115 (epoch 3.695), train_loss = 1.62091474, grad/param norm = 1.6239e-01, time/batch = 0.4542s\u001b[0m\t\n",
            "\u001b[0m1564/2115 (epoch 3.697), train_loss = 1.59751684, grad/param norm = 1.5408e-01, time/batch = 0.4190s\u001b[0m\t\n",
            "\u001b[0m1565/2115 (epoch 3.700), train_loss = 1.61782686, grad/param norm = 1.6138e-01, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m1566/2115 (epoch 3.702), train_loss = 1.60428095, grad/param norm = 1.3139e-01, time/batch = 0.4379s\u001b[0m\t\n",
            "\u001b[0m1567/2115 (epoch 3.704), train_loss = 1.55928439, grad/param norm = 1.3870e-01, time/batch = 0.4155s\u001b[0m\t\n",
            "\u001b[0m1568/2115 (epoch 3.707), train_loss = 1.64164246, grad/param norm = 1.4502e-01, time/batch = 0.4356s\u001b[0m\t\n",
            "\u001b[0m1569/2115 (epoch 3.709), train_loss = 1.51172549, grad/param norm = 1.3108e-01, time/batch = 0.4134s\u001b[0m\t\n",
            "\u001b[0m1570/2115 (epoch 3.712), train_loss = 1.53339487, grad/param norm = 1.1740e-01, time/batch = 0.4268s\u001b[0m\t\n",
            "\u001b[0m1571/2115 (epoch 3.714), train_loss = 1.58019212, grad/param norm = 1.1606e-01, time/batch = 0.4268s\u001b[0m\t\n",
            "\u001b[0m1572/2115 (epoch 3.716), train_loss = 1.52514304, grad/param norm = 1.2958e-01, time/batch = 0.4156s\u001b[0m\t\n",
            "\u001b[0m1573/2115 (epoch 3.719), train_loss = 1.50987402, grad/param norm = 1.2258e-01, time/batch = 0.4442s\u001b[0m\t\n",
            "\u001b[0m1574/2115 (epoch 3.721), train_loss = 1.60118150, grad/param norm = 1.2237e-01, time/batch = 0.4239s\u001b[0m\t\n",
            "\u001b[0m1575/2115 (epoch 3.723), train_loss = 1.60004865, grad/param norm = 1.3420e-01, time/batch = 0.4444s\u001b[0m\t\n",
            "\u001b[0m1576/2115 (epoch 3.726), train_loss = 1.52045262, grad/param norm = 1.3475e-01, time/batch = 0.4191s\u001b[0m\t\n",
            "\u001b[0m1577/2115 (epoch 3.728), train_loss = 1.57889117, grad/param norm = 1.2423e-01, time/batch = 0.4815s\u001b[0m\t\n",
            "\u001b[0m1578/2115 (epoch 3.730), train_loss = 1.62742042, grad/param norm = 1.1832e-01, time/batch = 0.4333s\u001b[0m\t\n",
            "\u001b[0m1579/2115 (epoch 3.733), train_loss = 1.59113029, grad/param norm = 1.2076e-01, time/batch = 0.6443s\u001b[0m\t\n",
            "\u001b[0m1580/2115 (epoch 3.735), train_loss = 1.55887435, grad/param norm = 1.3410e-01, time/batch = 1.0037s\u001b[0m\t\n",
            "\u001b[0m1581/2115 (epoch 3.738), train_loss = 1.64008284, grad/param norm = 1.3559e-01, time/batch = 0.8831s\u001b[0m\t\n",
            "\u001b[0m1582/2115 (epoch 3.740), train_loss = 1.62154310, grad/param norm = 1.3494e-01, time/batch = 0.8211s\u001b[0m\t\n",
            "\u001b[0m1583/2115 (epoch 3.742), train_loss = 1.63848976, grad/param norm = 1.6150e-01, time/batch = 0.4456s\u001b[0m\t\n",
            "\u001b[0m1584/2115 (epoch 3.745), train_loss = 1.54451001, grad/param norm = 1.2970e-01, time/batch = 0.4197s\u001b[0m\t\n",
            "\u001b[0m1585/2115 (epoch 3.747), train_loss = 1.57601302, grad/param norm = 1.1918e-01, time/batch = 0.4179s\u001b[0m\t\n",
            "\u001b[0m1586/2115 (epoch 3.749), train_loss = 1.57810732, grad/param norm = 1.2804e-01, time/batch = 0.4362s\u001b[0m\t\n",
            "\u001b[0m1587/2115 (epoch 3.752), train_loss = 1.58463293, grad/param norm = 1.3185e-01, time/batch = 0.4289s\u001b[0m\t\n",
            "\u001b[0m1588/2115 (epoch 3.754), train_loss = 1.53729351, grad/param norm = 1.2268e-01, time/batch = 0.4411s\u001b[0m\t\n",
            "\u001b[0m1589/2115 (epoch 3.757), train_loss = 1.51285587, grad/param norm = 1.2034e-01, time/batch = 0.4568s\u001b[0m\t\n",
            "\u001b[0m1590/2115 (epoch 3.759), train_loss = 1.50555075, grad/param norm = 1.1370e-01, time/batch = 0.4428s\u001b[0m\t\n",
            "\u001b[0m1591/2115 (epoch 3.761), train_loss = 1.54717856, grad/param norm = 1.1883e-01, time/batch = 0.4258s\u001b[0m\t\n",
            "\u001b[0m1592/2115 (epoch 3.764), train_loss = 1.53572964, grad/param norm = 1.2857e-01, time/batch = 0.4431s\u001b[0m\t\n",
            "\u001b[0m1593/2115 (epoch 3.766), train_loss = 1.50401297, grad/param norm = 1.0667e-01, time/batch = 0.4260s\u001b[0m\t\n",
            "\u001b[0m1594/2115 (epoch 3.768), train_loss = 1.54862403, grad/param norm = 1.1774e-01, time/batch = 0.4210s\u001b[0m\t\n",
            "\u001b[0m1595/2115 (epoch 3.771), train_loss = 1.54263580, grad/param norm = 1.1331e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m1596/2115 (epoch 3.773), train_loss = 1.56347194, grad/param norm = 1.2924e-01, time/batch = 0.4360s\u001b[0m\t\n",
            "\u001b[0m1597/2115 (epoch 3.775), train_loss = 1.53122927, grad/param norm = 1.3551e-01, time/batch = 0.4299s\u001b[0m\t\n",
            "\u001b[0m1598/2115 (epoch 3.778), train_loss = 1.61357146, grad/param norm = 1.5568e-01, time/batch = 0.4253s\u001b[0m\t\n",
            "\u001b[0m1599/2115 (epoch 3.780), train_loss = 1.54599942, grad/param norm = 1.4530e-01, time/batch = 0.4131s\u001b[0m\t\n",
            "\u001b[0m1600/2115 (epoch 3.783), train_loss = 1.53327785, grad/param norm = 1.2583e-01, time/batch = 0.4426s\u001b[0m\t\n",
            "\u001b[0m1601/2115 (epoch 3.785), train_loss = 1.58640214, grad/param norm = 1.2143e-01, time/batch = 0.4251s\u001b[0m\t\n",
            "\u001b[0m1602/2115 (epoch 3.787), train_loss = 1.54392452, grad/param norm = 1.2178e-01, time/batch = 0.4413s\u001b[0m\t\n",
            "\u001b[0m1603/2115 (epoch 3.790), train_loss = 1.48245248, grad/param norm = 1.2224e-01, time/batch = 0.4269s\u001b[0m\t\n",
            "\u001b[0m1604/2115 (epoch 3.792), train_loss = 1.52824934, grad/param norm = 1.2950e-01, time/batch = 0.4496s\u001b[0m\t\n",
            "\u001b[0m1605/2115 (epoch 3.794), train_loss = 1.55054999, grad/param norm = 1.2751e-01, time/batch = 0.6263s\u001b[0m\t\n",
            "\u001b[0m1606/2115 (epoch 3.797), train_loss = 1.58948993, grad/param norm = 1.4897e-01, time/batch = 0.9290s\u001b[0m\t\n",
            "\u001b[0m1607/2115 (epoch 3.799), train_loss = 1.57155461, grad/param norm = 1.6154e-01, time/batch = 0.8950s\u001b[0m\t\n",
            "\u001b[0m1608/2115 (epoch 3.801), train_loss = 1.67153513, grad/param norm = 1.7342e-01, time/batch = 0.8313s\u001b[0m\t\n",
            "\u001b[0m1609/2115 (epoch 3.804), train_loss = 1.53962360, grad/param norm = 1.5066e-01, time/batch = 0.4966s\u001b[0m\t\n",
            "\u001b[0m1610/2115 (epoch 3.806), train_loss = 1.59131013, grad/param norm = 1.4484e-01, time/batch = 0.4422s\u001b[0m\t\n",
            "\u001b[0m1611/2115 (epoch 3.809), train_loss = 1.50331548, grad/param norm = 1.4169e-01, time/batch = 0.4292s\u001b[0m\t\n",
            "\u001b[0m1612/2115 (epoch 3.811), train_loss = 1.57761941, grad/param norm = 1.4217e-01, time/batch = 0.4312s\u001b[0m\t\n",
            "\u001b[0m1613/2115 (epoch 3.813), train_loss = 1.56306180, grad/param norm = 1.4504e-01, time/batch = 0.4659s\u001b[0m\t\n",
            "\u001b[0m1614/2115 (epoch 3.816), train_loss = 1.50341602, grad/param norm = 1.1495e-01, time/batch = 0.4273s\u001b[0m\t\n",
            "\u001b[0m1615/2115 (epoch 3.818), train_loss = 1.56628604, grad/param norm = 1.0731e-01, time/batch = 0.4452s\u001b[0m\t\n",
            "\u001b[0m1616/2115 (epoch 3.820), train_loss = 1.52379106, grad/param norm = 1.0519e-01, time/batch = 0.4148s\u001b[0m\t\n",
            "\u001b[0m1617/2115 (epoch 3.823), train_loss = 1.60449084, grad/param norm = 1.0752e-01, time/batch = 0.4807s\u001b[0m\t\n",
            "\u001b[0m1618/2115 (epoch 3.825), train_loss = 1.53216063, grad/param norm = 1.0257e-01, time/batch = 0.4325s\u001b[0m\t\n",
            "\u001b[0m1619/2115 (epoch 3.827), train_loss = 1.64030646, grad/param norm = 1.1981e-01, time/batch = 0.4390s\u001b[0m\t\n",
            "\u001b[0m1620/2115 (epoch 3.830), train_loss = 1.59286682, grad/param norm = 1.1979e-01, time/batch = 0.4414s\u001b[0m\t\n",
            "\u001b[0m1621/2115 (epoch 3.832), train_loss = 1.58697113, grad/param norm = 1.2222e-01, time/batch = 0.4324s\u001b[0m\t\n",
            "\u001b[0m1622/2115 (epoch 3.835), train_loss = 1.49797635, grad/param norm = 1.2130e-01, time/batch = 0.4380s\u001b[0m\t\n",
            "\u001b[0m1623/2115 (epoch 3.837), train_loss = 1.51378677, grad/param norm = 1.2312e-01, time/batch = 0.4246s\u001b[0m\t\n",
            "\u001b[0m1624/2115 (epoch 3.839), train_loss = 1.50764134, grad/param norm = 1.2254e-01, time/batch = 0.4484s\u001b[0m\t\n",
            "\u001b[0m1625/2115 (epoch 3.842), train_loss = 1.53915162, grad/param norm = 1.3058e-01, time/batch = 0.4161s\u001b[0m\t\n",
            "\u001b[0m1626/2115 (epoch 3.844), train_loss = 1.61132753, grad/param norm = 1.3772e-01, time/batch = 0.4280s\u001b[0m\t\n",
            "\u001b[0m1627/2115 (epoch 3.846), train_loss = 1.61723478, grad/param norm = 1.4612e-01, time/batch = 0.4334s\u001b[0m\t\n",
            "\u001b[0m1628/2115 (epoch 3.849), train_loss = 1.52312801, grad/param norm = 1.2378e-01, time/batch = 0.4098s\u001b[0m\t\n",
            "\u001b[0m1629/2115 (epoch 3.851), train_loss = 1.53503280, grad/param norm = 1.1767e-01, time/batch = 0.4316s\u001b[0m\t\n",
            "\u001b[0m1630/2115 (epoch 3.853), train_loss = 1.59135833, grad/param norm = 1.2071e-01, time/batch = 0.4172s\u001b[0m\t\n",
            "\u001b[0m1631/2115 (epoch 3.856), train_loss = 1.54498565, grad/param norm = 1.3326e-01, time/batch = 0.5849s\u001b[0m\t\n",
            "\u001b[0m1632/2115 (epoch 3.858), train_loss = 1.53694645, grad/param norm = 1.1809e-01, time/batch = 0.8810s\u001b[0m\t\n",
            "\u001b[0m1633/2115 (epoch 3.861), train_loss = 1.57266404, grad/param norm = 1.2640e-01, time/batch = 1.0012s\u001b[0m\t\n",
            "\u001b[0m1634/2115 (epoch 3.863), train_loss = 1.58262944, grad/param norm = 1.4103e-01, time/batch = 0.8433s\u001b[0m\t\n",
            "\u001b[0m1635/2115 (epoch 3.865), train_loss = 1.54211596, grad/param norm = 1.5413e-01, time/batch = 0.5532s\u001b[0m\t\n",
            "\u001b[0m1636/2115 (epoch 3.868), train_loss = 1.51198883, grad/param norm = 1.4597e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m1637/2115 (epoch 3.870), train_loss = 1.51998713, grad/param norm = 1.3366e-01, time/batch = 0.4404s\u001b[0m\t\n",
            "\u001b[0m1638/2115 (epoch 3.872), train_loss = 1.53896823, grad/param norm = 1.3642e-01, time/batch = 0.4272s\u001b[0m\t\n",
            "\u001b[0m1639/2115 (epoch 3.875), train_loss = 1.53598327, grad/param norm = 1.3785e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m1640/2115 (epoch 3.877), train_loss = 1.66061174, grad/param norm = 1.2979e-01, time/batch = 0.4530s\u001b[0m\t\n",
            "\u001b[0m1641/2115 (epoch 3.879), train_loss = 1.53631506, grad/param norm = 1.3158e-01, time/batch = 0.4245s\u001b[0m\t\n",
            "\u001b[0m1642/2115 (epoch 3.882), train_loss = 1.58481624, grad/param norm = 1.1659e-01, time/batch = 0.4430s\u001b[0m\t\n",
            "\u001b[0m1643/2115 (epoch 3.884), train_loss = 1.56753178, grad/param norm = 1.2059e-01, time/batch = 0.4079s\u001b[0m\t\n",
            "\u001b[0m1644/2115 (epoch 3.887), train_loss = 1.58265843, grad/param norm = 1.3265e-01, time/batch = 0.4375s\u001b[0m\t\n",
            "\u001b[0m1645/2115 (epoch 3.889), train_loss = 1.54564248, grad/param norm = 1.2643e-01, time/batch = 0.4236s\u001b[0m\t\n",
            "\u001b[0m1646/2115 (epoch 3.891), train_loss = 1.57641038, grad/param norm = 1.2074e-01, time/batch = 0.4161s\u001b[0m\t\n",
            "\u001b[0m1647/2115 (epoch 3.894), train_loss = 1.52822064, grad/param norm = 1.2519e-01, time/batch = 0.4441s\u001b[0m\t\n",
            "\u001b[0m1648/2115 (epoch 3.896), train_loss = 1.53248363, grad/param norm = 1.1683e-01, time/batch = 0.4230s\u001b[0m\t\n",
            "\u001b[0m1649/2115 (epoch 3.898), train_loss = 1.52332046, grad/param norm = 1.2232e-01, time/batch = 0.4384s\u001b[0m\t\n",
            "\u001b[0m1650/2115 (epoch 3.901), train_loss = 1.57765241, grad/param norm = 1.2910e-01, time/batch = 0.4335s\u001b[0m\t\n",
            "\u001b[0m1651/2115 (epoch 3.903), train_loss = 1.55456379, grad/param norm = 1.2101e-01, time/batch = 0.4222s\u001b[0m\t\n",
            "\u001b[0m1652/2115 (epoch 3.905), train_loss = 1.58516257, grad/param norm = 1.1383e-01, time/batch = 0.4363s\u001b[0m\t\n",
            "\u001b[0m1653/2115 (epoch 3.908), train_loss = 1.55402927, grad/param norm = 1.1354e-01, time/batch = 0.4174s\u001b[0m\t\n",
            "\u001b[0m1654/2115 (epoch 3.910), train_loss = 1.55084263, grad/param norm = 1.1726e-01, time/batch = 0.4400s\u001b[0m\t\n",
            "\u001b[0m1655/2115 (epoch 3.913), train_loss = 1.61628549, grad/param norm = 1.2065e-01, time/batch = 0.4296s\u001b[0m\t\n",
            "\u001b[0m1656/2115 (epoch 3.915), train_loss = 1.57693743, grad/param norm = 1.1098e-01, time/batch = 0.4366s\u001b[0m\t\n",
            "\u001b[0m1657/2115 (epoch 3.917), train_loss = 1.57845715, grad/param norm = 1.2118e-01, time/batch = 0.4266s\u001b[0m\t\n",
            "\u001b[0m1658/2115 (epoch 3.920), train_loss = 1.59764938, grad/param norm = 1.4606e-01, time/batch = 0.7249s\u001b[0m\t\n",
            "\u001b[0m1659/2115 (epoch 3.922), train_loss = 1.53275714, grad/param norm = 1.5198e-01, time/batch = 1.0309s\u001b[0m\t\n",
            "\u001b[0m1660/2115 (epoch 3.924), train_loss = 1.53849172, grad/param norm = 1.4810e-01, time/batch = 0.9368s\u001b[0m\t\n",
            "\u001b[0m1661/2115 (epoch 3.927), train_loss = 1.55518792, grad/param norm = 1.4949e-01, time/batch = 0.6570s\u001b[0m\t\n",
            "\u001b[0m1662/2115 (epoch 3.929), train_loss = 1.56820439, grad/param norm = 1.5811e-01, time/batch = 0.4568s\u001b[0m\t\n",
            "\u001b[0m1663/2115 (epoch 3.931), train_loss = 1.61068885, grad/param norm = 1.3526e-01, time/batch = 0.4191s\u001b[0m\t\n",
            "\u001b[0m1664/2115 (epoch 3.934), train_loss = 1.51910232, grad/param norm = 1.3567e-01, time/batch = 0.4410s\u001b[0m\t\n",
            "\u001b[0m1665/2115 (epoch 3.936), train_loss = 1.58504134, grad/param norm = 1.5323e-01, time/batch = 0.4129s\u001b[0m\t\n",
            "\u001b[0m1666/2115 (epoch 3.939), train_loss = 1.51399060, grad/param norm = 1.3724e-01, time/batch = 0.4184s\u001b[0m\t\n",
            "\u001b[0m1667/2115 (epoch 3.941), train_loss = 1.48778275, grad/param norm = 1.2482e-01, time/batch = 0.4434s\u001b[0m\t\n",
            "\u001b[0m1668/2115 (epoch 3.943), train_loss = 1.54738285, grad/param norm = 1.1009e-01, time/batch = 0.4189s\u001b[0m\t\n",
            "\u001b[0m1669/2115 (epoch 3.946), train_loss = 1.50611700, grad/param norm = 1.1574e-01, time/batch = 0.4449s\u001b[0m\t\n",
            "\u001b[0m1670/2115 (epoch 3.948), train_loss = 1.59116444, grad/param norm = 1.1473e-01, time/batch = 0.4267s\u001b[0m\t\n",
            "\u001b[0m1671/2115 (epoch 3.950), train_loss = 1.56242590, grad/param norm = 1.2588e-01, time/batch = 0.4652s\u001b[0m\t\n",
            "\u001b[0m1672/2115 (epoch 3.953), train_loss = 1.55875683, grad/param norm = 1.1340e-01, time/batch = 0.4313s\u001b[0m\t\n",
            "\u001b[0m1673/2115 (epoch 3.955), train_loss = 1.57259546, grad/param norm = 1.1060e-01, time/batch = 0.4208s\u001b[0m\t\n",
            "\u001b[0m1674/2115 (epoch 3.957), train_loss = 1.55472530, grad/param norm = 1.1768e-01, time/batch = 0.4343s\u001b[0m\t\n",
            "\u001b[0m1675/2115 (epoch 3.960), train_loss = 1.58147409, grad/param norm = 1.4328e-01, time/batch = 0.4111s\u001b[0m\t\n",
            "\u001b[0m1676/2115 (epoch 3.962), train_loss = 1.59003115, grad/param norm = 1.3878e-01, time/batch = 0.4341s\u001b[0m\t\n",
            "\u001b[0m1677/2115 (epoch 3.965), train_loss = 1.54466861, grad/param norm = 1.4227e-01, time/batch = 0.4243s\u001b[0m\t\n",
            "\u001b[0m1678/2115 (epoch 3.967), train_loss = 1.60774348, grad/param norm = 1.2529e-01, time/batch = 0.4538s\u001b[0m\t\n",
            "\u001b[0m1679/2115 (epoch 3.969), train_loss = 1.57379733, grad/param norm = 1.5549e-01, time/batch = 0.4462s\u001b[0m\t\n",
            "\u001b[0m1680/2115 (epoch 3.972), train_loss = 1.60937232, grad/param norm = 1.6735e-01, time/batch = 0.4367s\u001b[0m\t\n",
            "\u001b[0m1681/2115 (epoch 3.974), train_loss = 1.53353441, grad/param norm = 1.5531e-01, time/batch = 0.4489s\u001b[0m\t\n",
            "\u001b[0m1682/2115 (epoch 3.976), train_loss = 1.58345521, grad/param norm = 1.4539e-01, time/batch = 0.4153s\u001b[0m\t\n",
            "\u001b[0m1683/2115 (epoch 3.979), train_loss = 1.51345453, grad/param norm = 1.3143e-01, time/batch = 0.5050s\u001b[0m\t\n",
            "\u001b[0m1684/2115 (epoch 3.981), train_loss = 1.52823791, grad/param norm = 1.1130e-01, time/batch = 0.7718s\u001b[0m\t\n",
            "\u001b[0m1685/2115 (epoch 3.983), train_loss = 1.55456514, grad/param norm = 1.0539e-01, time/batch = 0.9745s\u001b[0m\t\n",
            "\u001b[0m1686/2115 (epoch 3.986), train_loss = 1.54207584, grad/param norm = 1.0396e-01, time/batch = 0.8647s\u001b[0m\t\n",
            "\u001b[0m1687/2115 (epoch 3.988), train_loss = 1.52111550, grad/param norm = 1.0006e-01, time/batch = 0.8262s\u001b[0m\t\n",
            "\u001b[0m1688/2115 (epoch 3.991), train_loss = 1.54103062, grad/param norm = 1.1568e-01, time/batch = 0.4124s\u001b[0m\t\n",
            "\u001b[0m1689/2115 (epoch 3.993), train_loss = 1.57512135, grad/param norm = 1.1453e-01, time/batch = 0.4326s\u001b[0m\t\n",
            "\u001b[0m1690/2115 (epoch 3.995), train_loss = 1.56595184, grad/param norm = 1.1710e-01, time/batch = 0.4157s\u001b[0m\t\n",
            "\u001b[0m1691/2115 (epoch 3.998), train_loss = 1.58853710, grad/param norm = 1.3698e-01, time/batch = 0.4447s\u001b[0m\t\n",
            "\u001b[0m1692/2115 (epoch 4.000), train_loss = 1.61709064, grad/param norm = 1.7190e-01, time/batch = 0.4240s\u001b[0m\t\n",
            "\u001b[0m1693/2115 (epoch 4.002), train_loss = 1.74348772, grad/param norm = 1.9997e-01, time/batch = 0.4265s\u001b[0m\t\n",
            "\u001b[0m1694/2115 (epoch 4.005), train_loss = 1.54484739, grad/param norm = 1.4701e-01, time/batch = 0.4505s\u001b[0m\t\n",
            "\u001b[0m1695/2115 (epoch 4.007), train_loss = 1.55682121, grad/param norm = 1.2295e-01, time/batch = 0.4292s\u001b[0m\t\n",
            "\u001b[0m1696/2115 (epoch 4.009), train_loss = 1.54903632, grad/param norm = 1.1528e-01, time/batch = 0.4443s\u001b[0m\t\n",
            "\u001b[0m1697/2115 (epoch 4.012), train_loss = 1.59254567, grad/param norm = 1.0824e-01, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m1698/2115 (epoch 4.014), train_loss = 1.55628771, grad/param norm = 1.1834e-01, time/batch = 0.4429s\u001b[0m\t\n",
            "\u001b[0m1699/2115 (epoch 4.017), train_loss = 1.56960761, grad/param norm = 1.1926e-01, time/batch = 0.4259s\u001b[0m\t\n",
            "\u001b[0m1700/2115 (epoch 4.019), train_loss = 1.57361704, grad/param norm = 1.0954e-01, time/batch = 0.4181s\u001b[0m\t\n",
            "\u001b[0m1701/2115 (epoch 4.021), train_loss = 1.51567652, grad/param norm = 1.2517e-01, time/batch = 0.4511s\u001b[0m\t\n",
            "\u001b[0m1702/2115 (epoch 4.024), train_loss = 1.45309627, grad/param norm = 1.1965e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m1703/2115 (epoch 4.026), train_loss = 1.60755758, grad/param norm = 1.2446e-01, time/batch = 0.4913s\u001b[0m\t\n",
            "\u001b[0m1704/2115 (epoch 4.028), train_loss = 1.50146908, grad/param norm = 1.3994e-01, time/batch = 0.4226s\u001b[0m\t\n",
            "\u001b[0m1705/2115 (epoch 4.031), train_loss = 1.57739355, grad/param norm = 1.3066e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m1706/2115 (epoch 4.033), train_loss = 1.52755046, grad/param norm = 1.2000e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m1707/2115 (epoch 4.035), train_loss = 1.52184198, grad/param norm = 1.1820e-01, time/batch = 0.4176s\u001b[0m\t\n",
            "\u001b[0m1708/2115 (epoch 4.038), train_loss = 1.52051819, grad/param norm = 1.2082e-01, time/batch = 0.4360s\u001b[0m\t\n",
            "\u001b[0m1709/2115 (epoch 4.040), train_loss = 1.60678492, grad/param norm = 1.3254e-01, time/batch = 0.4187s\u001b[0m\t\n",
            "\u001b[0m1710/2115 (epoch 4.043), train_loss = 1.50049928, grad/param norm = 1.1079e-01, time/batch = 0.5980s\u001b[0m\t\n",
            "\u001b[0m1711/2115 (epoch 4.045), train_loss = 1.52878431, grad/param norm = 1.1830e-01, time/batch = 0.9586s\u001b[0m\t\n",
            "\u001b[0m1712/2115 (epoch 4.047), train_loss = 1.54466981, grad/param norm = 1.4725e-01, time/batch = 0.9520s\u001b[0m\t\n",
            "\u001b[0m1713/2115 (epoch 4.050), train_loss = 1.48982081, grad/param norm = 1.6560e-01, time/batch = 0.7760s\u001b[0m\t\n",
            "\u001b[0m1714/2115 (epoch 4.052), train_loss = 1.52259469, grad/param norm = 1.6279e-01, time/batch = 0.5430s\u001b[0m\t\n",
            "\u001b[0m1715/2115 (epoch 4.054), train_loss = 1.50109486, grad/param norm = 1.3515e-01, time/batch = 0.4230s\u001b[0m\t\n",
            "\u001b[0m1716/2115 (epoch 4.057), train_loss = 1.49465092, grad/param norm = 1.1780e-01, time/batch = 0.4365s\u001b[0m\t\n",
            "\u001b[0m1717/2115 (epoch 4.059), train_loss = 1.45435506, grad/param norm = 1.1371e-01, time/batch = 0.4284s\u001b[0m\t\n",
            "\u001b[0m1718/2115 (epoch 4.061), train_loss = 1.52844275, grad/param norm = 1.2036e-01, time/batch = 0.4478s\u001b[0m\t\n",
            "\u001b[0m1719/2115 (epoch 4.064), train_loss = 1.47239887, grad/param norm = 1.2138e-01, time/batch = 0.4199s\u001b[0m\t\n",
            "\u001b[0m1720/2115 (epoch 4.066), train_loss = 1.50317274, grad/param norm = 1.1245e-01, time/batch = 0.4246s\u001b[0m\t\n",
            "\u001b[0m1721/2115 (epoch 4.069), train_loss = 1.54026823, grad/param norm = 1.0017e-01, time/batch = 0.4454s\u001b[0m\t\n",
            "\u001b[0m1722/2115 (epoch 4.071), train_loss = 1.55339111, grad/param norm = 1.1612e-01, time/batch = 0.4306s\u001b[0m\t\n",
            "\u001b[0m1723/2115 (epoch 4.073), train_loss = 1.54058673, grad/param norm = 1.2555e-01, time/batch = 0.4485s\u001b[0m\t\n",
            "\u001b[0m1724/2115 (epoch 4.076), train_loss = 1.56563481, grad/param norm = 1.3159e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m1725/2115 (epoch 4.078), train_loss = 1.51444550, grad/param norm = 1.2454e-01, time/batch = 0.4456s\u001b[0m\t\n",
            "\u001b[0m1726/2115 (epoch 4.080), train_loss = 1.53604195, grad/param norm = 1.1370e-01, time/batch = 0.4122s\u001b[0m\t\n",
            "\u001b[0m1727/2115 (epoch 4.083), train_loss = 1.58124052, grad/param norm = 1.1241e-01, time/batch = 0.4172s\u001b[0m\t\n",
            "\u001b[0m1728/2115 (epoch 4.085), train_loss = 1.51848021, grad/param norm = 1.0753e-01, time/batch = 0.4488s\u001b[0m\t\n",
            "\u001b[0m1729/2115 (epoch 4.087), train_loss = 1.64766515, grad/param norm = 1.1997e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m1730/2115 (epoch 4.090), train_loss = 1.62797354, grad/param norm = 1.2726e-01, time/batch = 0.4480s\u001b[0m\t\n",
            "\u001b[0m1731/2115 (epoch 4.092), train_loss = 1.57802548, grad/param norm = 1.2435e-01, time/batch = 0.4226s\u001b[0m\t\n",
            "\u001b[0m1732/2115 (epoch 4.095), train_loss = 1.58088782, grad/param norm = 1.4117e-01, time/batch = 0.4192s\u001b[0m\t\n",
            "\u001b[0m1733/2115 (epoch 4.097), train_loss = 1.59226147, grad/param norm = 1.5903e-01, time/batch = 0.4536s\u001b[0m\t\n",
            "\u001b[0m1734/2115 (epoch 4.099), train_loss = 1.60449450, grad/param norm = 1.5672e-01, time/batch = 0.4266s\u001b[0m\t\n",
            "\u001b[0m1735/2115 (epoch 4.102), train_loss = 1.53606589, grad/param norm = 1.3433e-01, time/batch = 0.4380s\u001b[0m\t\n",
            "\u001b[0m1736/2115 (epoch 4.104), train_loss = 1.50881625, grad/param norm = 1.1962e-01, time/batch = 0.4201s\u001b[0m\t\n",
            "\u001b[0m1737/2115 (epoch 4.106), train_loss = 1.57142854, grad/param norm = 1.3362e-01, time/batch = 0.6961s\u001b[0m\t\n",
            "\u001b[0m1738/2115 (epoch 4.109), train_loss = 1.56634599, grad/param norm = 1.2942e-01, time/batch = 0.8902s\u001b[0m\t\n",
            "\u001b[0m1739/2115 (epoch 4.111), train_loss = 1.50755762, grad/param norm = 1.4027e-01, time/batch = 1.0897s\u001b[0m\t\n",
            "\u001b[0m1740/2115 (epoch 4.113), train_loss = 1.50244775, grad/param norm = 1.4316e-01, time/batch = 0.7059s\u001b[0m\t\n",
            "\u001b[0m1741/2115 (epoch 4.116), train_loss = 1.53086754, grad/param norm = 1.4023e-01, time/batch = 0.4581s\u001b[0m\t\n",
            "\u001b[0m1742/2115 (epoch 4.118), train_loss = 1.49016085, grad/param norm = 1.2916e-01, time/batch = 0.4307s\u001b[0m\t\n",
            "\u001b[0m1743/2115 (epoch 4.121), train_loss = 1.51723737, grad/param norm = 1.1283e-01, time/batch = 0.4648s\u001b[0m\t\n",
            "\u001b[0m1744/2115 (epoch 4.123), train_loss = 1.46695360, grad/param norm = 1.0464e-01, time/batch = 0.4667s\u001b[0m\t\n",
            "\u001b[0m1745/2115 (epoch 4.125), train_loss = 1.49215792, grad/param norm = 1.1160e-01, time/batch = 0.4519s\u001b[0m\t\n",
            "\u001b[0m1746/2115 (epoch 4.128), train_loss = 1.54027741, grad/param norm = 1.1545e-01, time/batch = 0.4123s\u001b[0m\t\n",
            "\u001b[0m1747/2115 (epoch 4.130), train_loss = 1.52274809, grad/param norm = 1.2773e-01, time/batch = 0.4175s\u001b[0m\t\n",
            "\u001b[0m1748/2115 (epoch 4.132), train_loss = 1.52174993, grad/param norm = 1.1476e-01, time/batch = 0.4407s\u001b[0m\t\n",
            "\u001b[0m1749/2115 (epoch 4.135), train_loss = 1.48007692, grad/param norm = 1.0740e-01, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m1750/2115 (epoch 4.137), train_loss = 1.55208695, grad/param norm = 1.1289e-01, time/batch = 0.4456s\u001b[0m\t\n",
            "\u001b[0m1751/2115 (epoch 4.139), train_loss = 1.52255798, grad/param norm = 1.2569e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m1752/2115 (epoch 4.142), train_loss = 1.53961602, grad/param norm = 1.3334e-01, time/batch = 0.4364s\u001b[0m\t\n",
            "\u001b[0m1753/2115 (epoch 4.144), train_loss = 1.59630499, grad/param norm = 1.4560e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m1754/2115 (epoch 4.147), train_loss = 1.56007779, grad/param norm = 1.4413e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m1755/2115 (epoch 4.149), train_loss = 1.54025039, grad/param norm = 1.3006e-01, time/batch = 0.4413s\u001b[0m\t\n",
            "\u001b[0m1756/2115 (epoch 4.151), train_loss = 1.58411907, grad/param norm = 1.3113e-01, time/batch = 0.4253s\u001b[0m\t\n",
            "\u001b[0m1757/2115 (epoch 4.154), train_loss = 1.48244360, grad/param norm = 1.1630e-01, time/batch = 0.4368s\u001b[0m\t\n",
            "\u001b[0m1758/2115 (epoch 4.156), train_loss = 1.56405136, grad/param norm = 1.1894e-01, time/batch = 0.4111s\u001b[0m\t\n",
            "\u001b[0m1759/2115 (epoch 4.158), train_loss = 1.49905521, grad/param norm = 1.0936e-01, time/batch = 0.4213s\u001b[0m\t\n",
            "\u001b[0m1760/2115 (epoch 4.161), train_loss = 1.56752656, grad/param norm = 1.2405e-01, time/batch = 0.4426s\u001b[0m\t\n",
            "\u001b[0m1761/2115 (epoch 4.163), train_loss = 1.60410286, grad/param norm = 1.2649e-01, time/batch = 0.4197s\u001b[0m\t\n",
            "\u001b[0m1762/2115 (epoch 4.165), train_loss = 1.57510890, grad/param norm = 1.2977e-01, time/batch = 0.4386s\u001b[0m\t\n",
            "\u001b[0m1763/2115 (epoch 4.168), train_loss = 1.55474737, grad/param norm = 1.1970e-01, time/batch = 0.6941s\u001b[0m\t\n",
            "\u001b[0m1764/2115 (epoch 4.170), train_loss = 1.47181498, grad/param norm = 1.2455e-01, time/batch = 0.8610s\u001b[0m\t\n",
            "\u001b[0m1765/2115 (epoch 4.173), train_loss = 1.55775609, grad/param norm = 1.2440e-01, time/batch = 0.9714s\u001b[0m\t\n",
            "\u001b[0m1766/2115 (epoch 4.175), train_loss = 1.50214498, grad/param norm = 1.2245e-01, time/batch = 0.8293s\u001b[0m\t\n",
            "\u001b[0m1767/2115 (epoch 4.177), train_loss = 1.48680728, grad/param norm = 1.1559e-01, time/batch = 0.4370s\u001b[0m\t\n",
            "\u001b[0m1768/2115 (epoch 4.180), train_loss = 1.54988695, grad/param norm = 1.2526e-01, time/batch = 0.4389s\u001b[0m\t\n",
            "\u001b[0m1769/2115 (epoch 4.182), train_loss = 1.56151121, grad/param norm = 1.2937e-01, time/batch = 0.4206s\u001b[0m\t\n",
            "\u001b[0m1770/2115 (epoch 4.184), train_loss = 1.55540995, grad/param norm = 1.2605e-01, time/batch = 0.4392s\u001b[0m\t\n",
            "\u001b[0m1771/2115 (epoch 4.187), train_loss = 1.54885137, grad/param norm = 1.2151e-01, time/batch = 0.4185s\u001b[0m\t\n",
            "\u001b[0m1772/2115 (epoch 4.189), train_loss = 1.56942155, grad/param norm = 1.2795e-01, time/batch = 0.4231s\u001b[0m\t\n",
            "\u001b[0m1773/2115 (epoch 4.191), train_loss = 1.53538491, grad/param norm = 1.2747e-01, time/batch = 0.4513s\u001b[0m\t\n",
            "\u001b[0m1774/2115 (epoch 4.194), train_loss = 1.43916731, grad/param norm = 1.3011e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m1775/2115 (epoch 4.196), train_loss = 1.48602694, grad/param norm = 1.1239e-01, time/batch = 0.4428s\u001b[0m\t\n",
            "\u001b[0m1776/2115 (epoch 4.199), train_loss = 1.53013203, grad/param norm = 1.1208e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m1777/2115 (epoch 4.201), train_loss = 1.55728950, grad/param norm = 1.1368e-01, time/batch = 0.4214s\u001b[0m\t\n",
            "\u001b[0m1778/2115 (epoch 4.203), train_loss = 1.55462854, grad/param norm = 1.1940e-01, time/batch = 0.4398s\u001b[0m\t\n",
            "\u001b[0m1779/2115 (epoch 4.206), train_loss = 1.51960932, grad/param norm = 1.1722e-01, time/batch = 0.4255s\u001b[0m\t\n",
            "\u001b[0m1780/2115 (epoch 4.208), train_loss = 1.48513899, grad/param norm = 1.1554e-01, time/batch = 0.4382s\u001b[0m\t\n",
            "\u001b[0m1781/2115 (epoch 4.210), train_loss = 1.58726113, grad/param norm = 1.1201e-01, time/batch = 0.4259s\u001b[0m\t\n",
            "\u001b[0m1782/2115 (epoch 4.213), train_loss = 1.52866569, grad/param norm = 1.2357e-01, time/batch = 0.4380s\u001b[0m\t\n",
            "\u001b[0m1783/2115 (epoch 4.215), train_loss = 1.50287026, grad/param norm = 1.2997e-01, time/batch = 0.4343s\u001b[0m\t\n",
            "\u001b[0m1784/2115 (epoch 4.217), train_loss = 1.48696307, grad/param norm = 1.1931e-01, time/batch = 0.4151s\u001b[0m\t\n",
            "\u001b[0m1785/2115 (epoch 4.220), train_loss = 1.55247647, grad/param norm = 1.1196e-01, time/batch = 0.4497s\u001b[0m\t\n",
            "\u001b[0m1786/2115 (epoch 4.222), train_loss = 1.50660838, grad/param norm = 1.4237e-01, time/batch = 0.4388s\u001b[0m\t\n",
            "\u001b[0m1787/2115 (epoch 4.225), train_loss = 1.56121948, grad/param norm = 1.7088e-01, time/batch = 0.4364s\u001b[0m\t\n",
            "\u001b[0m1788/2115 (epoch 4.227), train_loss = 1.60248112, grad/param norm = 1.4158e-01, time/batch = 0.4197s\u001b[0m\t\n",
            "\u001b[0m1789/2115 (epoch 4.229), train_loss = 1.53679128, grad/param norm = 1.1653e-01, time/batch = 0.5780s\u001b[0m\t\n",
            "\u001b[0m1790/2115 (epoch 4.232), train_loss = 1.55986318, grad/param norm = 1.1791e-01, time/batch = 0.9171s\u001b[0m\t\n",
            "\u001b[0m1791/2115 (epoch 4.234), train_loss = 1.46191092, grad/param norm = 1.1634e-01, time/batch = 0.8923s\u001b[0m\t\n",
            "\u001b[0m1792/2115 (epoch 4.236), train_loss = 1.57186364, grad/param norm = 1.3512e-01, time/batch = 0.9298s\u001b[0m\t\n",
            "\u001b[0m1793/2115 (epoch 4.239), train_loss = 1.50664117, grad/param norm = 1.3539e-01, time/batch = 0.4822s\u001b[0m\t\n",
            "\u001b[0m1794/2115 (epoch 4.241), train_loss = 1.50923938, grad/param norm = 1.2203e-01, time/batch = 0.4173s\u001b[0m\t\n",
            "\u001b[0m1795/2115 (epoch 4.243), train_loss = 1.54660959, grad/param norm = 1.2463e-01, time/batch = 0.4510s\u001b[0m\t\n",
            "\u001b[0m1796/2115 (epoch 4.246), train_loss = 1.50113339, grad/param norm = 1.1867e-01, time/batch = 0.4459s\u001b[0m\t\n",
            "\u001b[0m1797/2115 (epoch 4.248), train_loss = 1.55326792, grad/param norm = 1.1718e-01, time/batch = 0.4475s\u001b[0m\t\n",
            "\u001b[0m1798/2115 (epoch 4.251), train_loss = 1.52673639, grad/param norm = 1.1888e-01, time/batch = 0.4521s\u001b[0m\t\n",
            "\u001b[0m1799/2115 (epoch 4.253), train_loss = 1.49222048, grad/param norm = 1.2668e-01, time/batch = 0.4231s\u001b[0m\t\n",
            "\u001b[0m1800/2115 (epoch 4.255), train_loss = 1.49346565, grad/param norm = 1.1486e-01, time/batch = 0.4384s\u001b[0m\t\n",
            "\u001b[0m1801/2115 (epoch 4.258), train_loss = 1.56615595, grad/param norm = 1.1232e-01, time/batch = 0.4233s\u001b[0m\t\n",
            "\u001b[0m1802/2115 (epoch 4.260), train_loss = 1.47731771, grad/param norm = 1.3566e-01, time/batch = 0.4420s\u001b[0m\t\n",
            "\u001b[0m1803/2115 (epoch 4.262), train_loss = 1.44540837, grad/param norm = 1.2588e-01, time/batch = 0.4147s\u001b[0m\t\n",
            "\u001b[0m1804/2115 (epoch 4.265), train_loss = 1.51855432, grad/param norm = 1.1929e-01, time/batch = 0.4521s\u001b[0m\t\n",
            "\u001b[0m1805/2115 (epoch 4.267), train_loss = 1.54053121, grad/param norm = 1.1388e-01, time/batch = 0.4131s\u001b[0m\t\n",
            "\u001b[0m1806/2115 (epoch 4.270), train_loss = 1.51927748, grad/param norm = 1.1505e-01, time/batch = 0.4308s\u001b[0m\t\n",
            "\u001b[0m1807/2115 (epoch 4.272), train_loss = 1.53686637, grad/param norm = 1.1350e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m1808/2115 (epoch 4.274), train_loss = 1.56301896, grad/param norm = 1.3752e-01, time/batch = 0.4228s\u001b[0m\t\n",
            "\u001b[0m1809/2115 (epoch 4.277), train_loss = 1.54197475, grad/param norm = 1.4977e-01, time/batch = 0.4379s\u001b[0m\t\n",
            "\u001b[0m1810/2115 (epoch 4.279), train_loss = 1.59355654, grad/param norm = 1.2424e-01, time/batch = 0.4339s\u001b[0m\t\n",
            "\u001b[0m1811/2115 (epoch 4.281), train_loss = 1.55143842, grad/param norm = 1.2424e-01, time/batch = 0.4484s\u001b[0m\t\n",
            "\u001b[0m1812/2115 (epoch 4.284), train_loss = 1.47745172, grad/param norm = 1.4113e-01, time/batch = 0.4280s\u001b[0m\t\n",
            "\u001b[0m1813/2115 (epoch 4.286), train_loss = 1.55308360, grad/param norm = 1.3093e-01, time/batch = 0.4196s\u001b[0m\t\n",
            "\u001b[0m1814/2115 (epoch 4.288), train_loss = 1.54765729, grad/param norm = 1.1676e-01, time/batch = 0.4462s\u001b[0m\t\n",
            "\u001b[0m1815/2115 (epoch 4.291), train_loss = 1.47897344, grad/param norm = 1.0697e-01, time/batch = 0.4821s\u001b[0m\t\n",
            "\u001b[0m1816/2115 (epoch 4.293), train_loss = 1.48895101, grad/param norm = 1.1048e-01, time/batch = 0.8614s\u001b[0m\t\n",
            "\u001b[0m1817/2115 (epoch 4.296), train_loss = 1.56284085, grad/param norm = 1.4413e-01, time/batch = 0.9005s\u001b[0m\t\n",
            "\u001b[0m1818/2115 (epoch 4.298), train_loss = 1.61133756, grad/param norm = 1.6702e-01, time/batch = 0.9343s\u001b[0m\t\n",
            "\u001b[0m1819/2115 (epoch 4.300), train_loss = 1.53843894, grad/param norm = 1.7617e-01, time/batch = 0.6770s\u001b[0m\t\n",
            "\u001b[0m1820/2115 (epoch 4.303), train_loss = 1.58479201, grad/param norm = 1.3387e-01, time/batch = 0.4358s\u001b[0m\t\n",
            "\u001b[0m1821/2115 (epoch 4.305), train_loss = 1.57729406, grad/param norm = 1.3286e-01, time/batch = 0.4338s\u001b[0m\t\n",
            "\u001b[0m1822/2115 (epoch 4.307), train_loss = 1.49459499, grad/param norm = 1.2709e-01, time/batch = 0.4442s\u001b[0m\t\n",
            "\u001b[0m1823/2115 (epoch 4.310), train_loss = 1.45148072, grad/param norm = 1.3346e-01, time/batch = 0.4492s\u001b[0m\t\n",
            "\u001b[0m1824/2115 (epoch 4.312), train_loss = 1.54202463, grad/param norm = 1.1851e-01, time/batch = 0.4459s\u001b[0m\t\n",
            "\u001b[0m1825/2115 (epoch 4.314), train_loss = 1.58937160, grad/param norm = 1.2369e-01, time/batch = 0.4237s\u001b[0m\t\n",
            "\u001b[0m1826/2115 (epoch 4.317), train_loss = 1.52904987, grad/param norm = 1.2915e-01, time/batch = 0.4348s\u001b[0m\t\n",
            "\u001b[0m1827/2115 (epoch 4.319), train_loss = 1.52320355, grad/param norm = 1.2740e-01, time/batch = 0.4244s\u001b[0m\t\n",
            "\u001b[0m1828/2115 (epoch 4.322), train_loss = 1.52929680, grad/param norm = 1.0929e-01, time/batch = 0.4444s\u001b[0m\t\n",
            "\u001b[0m1829/2115 (epoch 4.324), train_loss = 1.58553116, grad/param norm = 1.2462e-01, time/batch = 0.4681s\u001b[0m\t\n",
            "\u001b[0m1830/2115 (epoch 4.326), train_loss = 1.60376192, grad/param norm = 1.3354e-01, time/batch = 0.4282s\u001b[0m\t\n",
            "\u001b[0m1831/2115 (epoch 4.329), train_loss = 1.58952212, grad/param norm = 1.2481e-01, time/batch = 0.4482s\u001b[0m\t\n",
            "\u001b[0m1832/2115 (epoch 4.331), train_loss = 1.56978598, grad/param norm = 1.1315e-01, time/batch = 0.4290s\u001b[0m\t\n",
            "\u001b[0m1833/2115 (epoch 4.333), train_loss = 1.56306921, grad/param norm = 1.1928e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m1834/2115 (epoch 4.336), train_loss = 1.58384942, grad/param norm = 1.0955e-01, time/batch = 0.4246s\u001b[0m\t\n",
            "\u001b[0m1835/2115 (epoch 4.338), train_loss = 1.49917487, grad/param norm = 1.2921e-01, time/batch = 0.4196s\u001b[0m\t\n",
            "\u001b[0m1836/2115 (epoch 4.340), train_loss = 1.49872126, grad/param norm = 1.2282e-01, time/batch = 0.4469s\u001b[0m\t\n",
            "\u001b[0m1837/2115 (epoch 4.343), train_loss = 1.53189661, grad/param norm = 1.2015e-01, time/batch = 0.4253s\u001b[0m\t\n",
            "\u001b[0m1838/2115 (epoch 4.345), train_loss = 1.54934146, grad/param norm = 1.3037e-01, time/batch = 0.4467s\u001b[0m\t\n",
            "\u001b[0m1839/2115 (epoch 4.348), train_loss = 1.52647191, grad/param norm = 1.2623e-01, time/batch = 0.4386s\u001b[0m\t\n",
            "\u001b[0m1840/2115 (epoch 4.350), train_loss = 1.50152135, grad/param norm = 1.1229e-01, time/batch = 0.4351s\u001b[0m\t\n",
            "\u001b[0m1841/2115 (epoch 4.352), train_loss = 1.52933382, grad/param norm = 1.1412e-01, time/batch = 0.4334s\u001b[0m\t\n",
            "\u001b[0m1842/2115 (epoch 4.355), train_loss = 1.51785287, grad/param norm = 1.1035e-01, time/batch = 0.7682s\u001b[0m\t\n",
            "\u001b[0m1843/2115 (epoch 4.357), train_loss = 1.47330589, grad/param norm = 1.0392e-01, time/batch = 1.0680s\u001b[0m\t\n",
            "\u001b[0m1844/2115 (epoch 4.359), train_loss = 1.55776587, grad/param norm = 1.1889e-01, time/batch = 0.8772s\u001b[0m\t\n",
            "\u001b[0m1845/2115 (epoch 4.362), train_loss = 1.50109077, grad/param norm = 1.2134e-01, time/batch = 0.6932s\u001b[0m\t\n",
            "\u001b[0m1846/2115 (epoch 4.364), train_loss = 1.51668530, grad/param norm = 1.1697e-01, time/batch = 0.4396s\u001b[0m\t\n",
            "\u001b[0m1847/2115 (epoch 4.366), train_loss = 1.52784454, grad/param norm = 1.1896e-01, time/batch = 0.4200s\u001b[0m\t\n",
            "\u001b[0m1848/2115 (epoch 4.369), train_loss = 1.53994030, grad/param norm = 1.1428e-01, time/batch = 0.4222s\u001b[0m\t\n",
            "\u001b[0m1849/2115 (epoch 4.371), train_loss = 1.52189528, grad/param norm = 1.1489e-01, time/batch = 0.4430s\u001b[0m\t\n",
            "\u001b[0m1850/2115 (epoch 4.374), train_loss = 1.54230963, grad/param norm = 1.1488e-01, time/batch = 0.4312s\u001b[0m\t\n",
            "\u001b[0m1851/2115 (epoch 4.376), train_loss = 1.55269276, grad/param norm = 1.1380e-01, time/batch = 0.4638s\u001b[0m\t\n",
            "\u001b[0m1852/2115 (epoch 4.378), train_loss = 1.54513445, grad/param norm = 1.1285e-01, time/batch = 0.4211s\u001b[0m\t\n",
            "\u001b[0m1853/2115 (epoch 4.381), train_loss = 1.53103978, grad/param norm = 1.2929e-01, time/batch = 0.4634s\u001b[0m\t\n",
            "\u001b[0m1854/2115 (epoch 4.383), train_loss = 1.65580145, grad/param norm = 1.2747e-01, time/batch = 0.4233s\u001b[0m\t\n",
            "\u001b[0m1855/2115 (epoch 4.385), train_loss = 1.56382322, grad/param norm = 1.0686e-01, time/batch = 0.4322s\u001b[0m\t\n",
            "\u001b[0m1856/2115 (epoch 4.388), train_loss = 1.54288530, grad/param norm = 1.0505e-01, time/batch = 0.4831s\u001b[0m\t\n",
            "\u001b[0m1857/2115 (epoch 4.390), train_loss = 1.58875902, grad/param norm = 1.1923e-01, time/batch = 0.4346s\u001b[0m\t\n",
            "\u001b[0m1858/2115 (epoch 4.392), train_loss = 1.56328249, grad/param norm = 1.3929e-01, time/batch = 0.4503s\u001b[0m\t\n",
            "\u001b[0m1859/2115 (epoch 4.395), train_loss = 1.52785352, grad/param norm = 1.5018e-01, time/batch = 0.4154s\u001b[0m\t\n",
            "\u001b[0m1860/2115 (epoch 4.397), train_loss = 1.50380451, grad/param norm = 1.7675e-01, time/batch = 0.4487s\u001b[0m\t\n",
            "\u001b[0m1861/2115 (epoch 4.400), train_loss = 1.57796196, grad/param norm = 1.5939e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m1862/2115 (epoch 4.402), train_loss = 1.60915910, grad/param norm = 1.3601e-01, time/batch = 0.4360s\u001b[0m\t\n",
            "\u001b[0m1863/2115 (epoch 4.404), train_loss = 1.56180269, grad/param norm = 1.2078e-01, time/batch = 0.4259s\u001b[0m\t\n",
            "\u001b[0m1864/2115 (epoch 4.407), train_loss = 1.56424325, grad/param norm = 1.0923e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m1865/2115 (epoch 4.409), train_loss = 1.54465117, grad/param norm = 1.0533e-01, time/batch = 0.4472s\u001b[0m\t\n",
            "\u001b[0m1866/2115 (epoch 4.411), train_loss = 1.50878939, grad/param norm = 1.0374e-01, time/batch = 0.4093s\u001b[0m\t\n",
            "\u001b[0m1867/2115 (epoch 4.414), train_loss = 1.55819299, grad/param norm = 1.0945e-01, time/batch = 0.4559s\u001b[0m\t\n",
            "\u001b[0m1868/2115 (epoch 4.416), train_loss = 1.57022072, grad/param norm = 1.1636e-01, time/batch = 0.7507s\u001b[0m\t\n",
            "\u001b[0m1869/2115 (epoch 4.418), train_loss = 1.53529300, grad/param norm = 1.2267e-01, time/batch = 0.8713s\u001b[0m\t\n",
            "\u001b[0m1870/2115 (epoch 4.421), train_loss = 1.52554270, grad/param norm = 1.0330e-01, time/batch = 0.9450s\u001b[0m\t\n",
            "\u001b[0m1871/2115 (epoch 4.423), train_loss = 1.46381765, grad/param norm = 1.0499e-01, time/batch = 0.8449s\u001b[0m\t\n",
            "\u001b[0m1872/2115 (epoch 4.426), train_loss = 1.51224786, grad/param norm = 1.1147e-01, time/batch = 0.4400s\u001b[0m\t\n",
            "\u001b[0m1873/2115 (epoch 4.428), train_loss = 1.52234854, grad/param norm = 1.1765e-01, time/batch = 0.4416s\u001b[0m\t\n",
            "\u001b[0m1874/2115 (epoch 4.430), train_loss = 1.50248633, grad/param norm = 1.0934e-01, time/batch = 0.4271s\u001b[0m\t\n",
            "\u001b[0m1875/2115 (epoch 4.433), train_loss = 1.54595882, grad/param norm = 1.0997e-01, time/batch = 0.4544s\u001b[0m\t\n",
            "\u001b[0m1876/2115 (epoch 4.435), train_loss = 1.52842148, grad/param norm = 1.1792e-01, time/batch = 0.4303s\u001b[0m\t\n",
            "\u001b[0m1877/2115 (epoch 4.437), train_loss = 1.42266267, grad/param norm = 1.2780e-01, time/batch = 0.4307s\u001b[0m\t\n",
            "\u001b[0m1878/2115 (epoch 4.440), train_loss = 1.48681393, grad/param norm = 1.4058e-01, time/batch = 0.4324s\u001b[0m\t\n",
            "\u001b[0m1879/2115 (epoch 4.442), train_loss = 1.55191133, grad/param norm = 1.4108e-01, time/batch = 0.4246s\u001b[0m\t\n",
            "\u001b[0m1880/2115 (epoch 4.444), train_loss = 1.54813177, grad/param norm = 1.4343e-01, time/batch = 0.4325s\u001b[0m\t\n",
            "\u001b[0m1881/2115 (epoch 4.447), train_loss = 1.55331917, grad/param norm = 1.5490e-01, time/batch = 0.4339s\u001b[0m\t\n",
            "\u001b[0m1882/2115 (epoch 4.449), train_loss = 1.45346366, grad/param norm = 1.2162e-01, time/batch = 0.4469s\u001b[0m\t\n",
            "\u001b[0m1883/2115 (epoch 4.452), train_loss = 1.54094513, grad/param norm = 1.2275e-01, time/batch = 0.4313s\u001b[0m\t\n",
            "\u001b[0m1884/2115 (epoch 4.454), train_loss = 1.45964338, grad/param norm = 1.2999e-01, time/batch = 0.4230s\u001b[0m\t\n",
            "\u001b[0m1885/2115 (epoch 4.456), train_loss = 1.49727387, grad/param norm = 1.3740e-01, time/batch = 0.4501s\u001b[0m\t\n",
            "\u001b[0m1886/2115 (epoch 4.459), train_loss = 1.52019858, grad/param norm = 1.3596e-01, time/batch = 0.4288s\u001b[0m\t\n",
            "\u001b[0m1887/2115 (epoch 4.461), train_loss = 1.51091481, grad/param norm = 1.2112e-01, time/batch = 0.4480s\u001b[0m\t\n",
            "\u001b[0m1888/2115 (epoch 4.463), train_loss = 1.47785639, grad/param norm = 1.3159e-01, time/batch = 0.4308s\u001b[0m\t\n",
            "\u001b[0m1889/2115 (epoch 4.466), train_loss = 1.48884375, grad/param norm = 1.3420e-01, time/batch = 0.4240s\u001b[0m\t\n",
            "\u001b[0m1890/2115 (epoch 4.468), train_loss = 1.53481993, grad/param norm = 1.2278e-01, time/batch = 0.4524s\u001b[0m\t\n",
            "\u001b[0m1891/2115 (epoch 4.470), train_loss = 1.54357343, grad/param norm = 1.1204e-01, time/batch = 0.4273s\u001b[0m\t\n",
            "\u001b[0m1892/2115 (epoch 4.473), train_loss = 1.55732973, grad/param norm = 1.2689e-01, time/batch = 0.4408s\u001b[0m\t\n",
            "\u001b[0m1893/2115 (epoch 4.475), train_loss = 1.58646271, grad/param norm = 1.2868e-01, time/batch = 0.4199s\u001b[0m\t\n",
            "\u001b[0m1894/2115 (epoch 4.478), train_loss = 1.51639610, grad/param norm = 1.2902e-01, time/batch = 0.6547s\u001b[0m\t\n",
            "\u001b[0m1895/2115 (epoch 4.480), train_loss = 1.54583230, grad/param norm = 1.5037e-01, time/batch = 0.9680s\u001b[0m\t\n",
            "\u001b[0m1896/2115 (epoch 4.482), train_loss = 1.47257521, grad/param norm = 1.3536e-01, time/batch = 0.9019s\u001b[0m\t\n",
            "\u001b[0m1897/2115 (epoch 4.485), train_loss = 1.49387812, grad/param norm = 1.3231e-01, time/batch = 0.9088s\u001b[0m\t\n",
            "\u001b[0m1898/2115 (epoch 4.487), train_loss = 1.52241235, grad/param norm = 1.1734e-01, time/batch = 0.4479s\u001b[0m\t\n",
            "\u001b[0m1899/2115 (epoch 4.489), train_loss = 1.54976639, grad/param norm = 1.2380e-01, time/batch = 0.4263s\u001b[0m\t\n",
            "\u001b[0m1900/2115 (epoch 4.492), train_loss = 1.55076747, grad/param norm = 1.1955e-01, time/batch = 0.4484s\u001b[0m\t\n",
            "\u001b[0m1901/2115 (epoch 4.494), train_loss = 1.58737428, grad/param norm = 1.1849e-01, time/batch = 0.4288s\u001b[0m\t\n",
            "\u001b[0m1902/2115 (epoch 4.496), train_loss = 1.52485643, grad/param norm = 1.1603e-01, time/batch = 0.4580s\u001b[0m\t\n",
            "\u001b[0m1903/2115 (epoch 4.499), train_loss = 1.51773514, grad/param norm = 1.0977e-01, time/batch = 0.4288s\u001b[0m\t\n",
            "\u001b[0m1904/2115 (epoch 4.501), train_loss = 1.49653029, grad/param norm = 1.1534e-01, time/batch = 0.4414s\u001b[0m\t\n",
            "\u001b[0m1905/2115 (epoch 4.504), train_loss = 1.52394694, grad/param norm = 1.1184e-01, time/batch = 0.4465s\u001b[0m\t\n",
            "\u001b[0m1906/2115 (epoch 4.506), train_loss = 1.52550447, grad/param norm = 1.2873e-01, time/batch = 0.4118s\u001b[0m\t\n",
            "\u001b[0m1907/2115 (epoch 4.508), train_loss = 1.48015013, grad/param norm = 1.2960e-01, time/batch = 0.4491s\u001b[0m\t\n",
            "\u001b[0m1908/2115 (epoch 4.511), train_loss = 1.48835867, grad/param norm = 1.1697e-01, time/batch = 0.4170s\u001b[0m\t\n",
            "\u001b[0m1909/2115 (epoch 4.513), train_loss = 1.49831448, grad/param norm = 1.1082e-01, time/batch = 0.4580s\u001b[0m\t\n",
            "\u001b[0m1910/2115 (epoch 4.515), train_loss = 1.48458490, grad/param norm = 1.0715e-01, time/batch = 0.4175s\u001b[0m\t\n",
            "\u001b[0m1911/2115 (epoch 4.518), train_loss = 1.48634683, grad/param norm = 1.1782e-01, time/batch = 0.4324s\u001b[0m\t\n",
            "\u001b[0m1912/2115 (epoch 4.520), train_loss = 1.50800410, grad/param norm = 1.1895e-01, time/batch = 0.4362s\u001b[0m\t\n",
            "\u001b[0m1913/2115 (epoch 4.522), train_loss = 1.52525747, grad/param norm = 1.3108e-01, time/batch = 0.4278s\u001b[0m\t\n",
            "\u001b[0m1914/2115 (epoch 4.525), train_loss = 1.52873368, grad/param norm = 1.1975e-01, time/batch = 0.4348s\u001b[0m\t\n",
            "\u001b[0m1915/2115 (epoch 4.527), train_loss = 1.52726102, grad/param norm = 1.2648e-01, time/batch = 0.4168s\u001b[0m\t\n",
            "\u001b[0m1916/2115 (epoch 4.530), train_loss = 1.51318792, grad/param norm = 1.0934e-01, time/batch = 0.4259s\u001b[0m\t\n",
            "\u001b[0m1917/2115 (epoch 4.532), train_loss = 1.46101670, grad/param norm = 1.1694e-01, time/batch = 0.4791s\u001b[0m\t\n",
            "\u001b[0m1918/2115 (epoch 4.534), train_loss = 1.47906446, grad/param norm = 1.1686e-01, time/batch = 0.4310s\u001b[0m\t\n",
            "\u001b[0m1919/2115 (epoch 4.537), train_loss = 1.57381120, grad/param norm = 1.1904e-01, time/batch = 0.4483s\u001b[0m\t\n",
            "\u001b[0m1920/2115 (epoch 4.539), train_loss = 1.50966018, grad/param norm = 1.0841e-01, time/batch = 0.6100s\u001b[0m\t\n",
            "\u001b[0m1921/2115 (epoch 4.541), train_loss = 1.52922553, grad/param norm = 1.1511e-01, time/batch = 1.0351s\u001b[0m\t\n",
            "\u001b[0m1922/2115 (epoch 4.544), train_loss = 1.53693834, grad/param norm = 1.1022e-01, time/batch = 0.8512s\u001b[0m\t\n",
            "\u001b[0m1923/2115 (epoch 4.546), train_loss = 1.57021949, grad/param norm = 1.2329e-01, time/batch = 0.9046s\u001b[0m\t\n",
            "\u001b[0m1924/2115 (epoch 4.548), train_loss = 1.51240565, grad/param norm = 1.1980e-01, time/batch = 0.4402s\u001b[0m\t\n",
            "\u001b[0m1925/2115 (epoch 4.551), train_loss = 1.53171302, grad/param norm = 1.2084e-01, time/batch = 0.4261s\u001b[0m\t\n",
            "\u001b[0m1926/2115 (epoch 4.553), train_loss = 1.57122448, grad/param norm = 1.1473e-01, time/batch = 0.4232s\u001b[0m\t\n",
            "\u001b[0m1927/2115 (epoch 4.556), train_loss = 1.55999508, grad/param norm = 1.2475e-01, time/batch = 0.4459s\u001b[0m\t\n",
            "\u001b[0m1928/2115 (epoch 4.558), train_loss = 1.51400448, grad/param norm = 1.3248e-01, time/batch = 0.4223s\u001b[0m\t\n",
            "\u001b[0m1929/2115 (epoch 4.560), train_loss = 1.54647250, grad/param norm = 1.1858e-01, time/batch = 0.4358s\u001b[0m\t\n",
            "\u001b[0m1930/2115 (epoch 4.563), train_loss = 1.47427346, grad/param norm = 1.1220e-01, time/batch = 0.4233s\u001b[0m\t\n",
            "\u001b[0m1931/2115 (epoch 4.565), train_loss = 1.53336758, grad/param norm = 1.2389e-01, time/batch = 0.4320s\u001b[0m\t\n",
            "\u001b[0m1932/2115 (epoch 4.567), train_loss = 1.51617067, grad/param norm = 1.0857e-01, time/batch = 0.4648s\u001b[0m\t\n",
            "\u001b[0m1933/2115 (epoch 4.570), train_loss = 1.52743864, grad/param norm = 1.1044e-01, time/batch = 0.4313s\u001b[0m\t\n",
            "\u001b[0m1934/2115 (epoch 4.572), train_loss = 1.47907370, grad/param norm = 1.1092e-01, time/batch = 0.5052s\u001b[0m\t\n",
            "\u001b[0m1935/2115 (epoch 4.574), train_loss = 1.49218636, grad/param norm = 1.0371e-01, time/batch = 0.4362s\u001b[0m\t\n",
            "\u001b[0m1936/2115 (epoch 4.577), train_loss = 1.50848728, grad/param norm = 1.1514e-01, time/batch = 0.4349s\u001b[0m\t\n",
            "\u001b[0m1937/2115 (epoch 4.579), train_loss = 1.47621072, grad/param norm = 1.1285e-01, time/batch = 0.4279s\u001b[0m\t\n",
            "\u001b[0m1938/2115 (epoch 4.582), train_loss = 1.51382695, grad/param norm = 1.1959e-01, time/batch = 0.4416s\u001b[0m\t\n",
            "\u001b[0m1939/2115 (epoch 4.584), train_loss = 1.53320685, grad/param norm = 1.1790e-01, time/batch = 0.4419s\u001b[0m\t\n",
            "\u001b[0m1940/2115 (epoch 4.586), train_loss = 1.49453827, grad/param norm = 1.1125e-01, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m1941/2115 (epoch 4.589), train_loss = 1.57071882, grad/param norm = 1.3658e-01, time/batch = 0.4481s\u001b[0m\t\n",
            "\u001b[0m1942/2115 (epoch 4.591), train_loss = 1.49246402, grad/param norm = 1.3881e-01, time/batch = 0.4178s\u001b[0m\t\n",
            "\u001b[0m1943/2115 (epoch 4.593), train_loss = 1.55088515, grad/param norm = 1.4748e-01, time/batch = 0.4493s\u001b[0m\t\n",
            "\u001b[0m1944/2115 (epoch 4.596), train_loss = 1.47947281, grad/param norm = 1.1591e-01, time/batch = 0.4219s\u001b[0m\t\n",
            "\u001b[0m1945/2115 (epoch 4.598), train_loss = 1.53765736, grad/param norm = 1.0842e-01, time/batch = 0.4545s\u001b[0m\t\n",
            "\u001b[0m1946/2115 (epoch 4.600), train_loss = 1.50451233, grad/param norm = 1.1603e-01, time/batch = 0.6536s\u001b[0m\t\n",
            "\u001b[0m1947/2115 (epoch 4.603), train_loss = 1.49945629, grad/param norm = 1.2227e-01, time/batch = 1.0502s\u001b[0m\t\n",
            "\u001b[0m1948/2115 (epoch 4.605), train_loss = 1.54013372, grad/param norm = 1.4017e-01, time/batch = 0.8967s\u001b[0m\t\n",
            "\u001b[0m1949/2115 (epoch 4.608), train_loss = 1.48149963, grad/param norm = 1.6373e-01, time/batch = 0.8655s\u001b[0m\t\n",
            "\u001b[0m1950/2115 (epoch 4.610), train_loss = 1.52336557, grad/param norm = 1.5562e-01, time/batch = 0.4156s\u001b[0m\t\n",
            "\u001b[0m1951/2115 (epoch 4.612), train_loss = 1.51179761, grad/param norm = 1.5327e-01, time/batch = 0.4573s\u001b[0m\t\n",
            "\u001b[0m1952/2115 (epoch 4.615), train_loss = 1.49678869, grad/param norm = 1.4628e-01, time/batch = 0.4223s\u001b[0m\t\n",
            "\u001b[0m1953/2115 (epoch 4.617), train_loss = 1.48416050, grad/param norm = 1.2093e-01, time/batch = 0.4483s\u001b[0m\t\n",
            "\u001b[0m1954/2115 (epoch 4.619), train_loss = 1.49574480, grad/param norm = 1.0573e-01, time/batch = 0.4188s\u001b[0m\t\n",
            "\u001b[0m1955/2115 (epoch 4.622), train_loss = 1.45468674, grad/param norm = 1.1574e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m1956/2115 (epoch 4.624), train_loss = 1.52102445, grad/param norm = 1.4444e-01, time/batch = 0.4545s\u001b[0m\t\n",
            "\u001b[0m1957/2115 (epoch 4.626), train_loss = 1.48121052, grad/param norm = 1.1141e-01, time/batch = 0.4139s\u001b[0m\t\n",
            "\u001b[0m1958/2115 (epoch 4.629), train_loss = 1.55825975, grad/param norm = 1.1516e-01, time/batch = 0.4478s\u001b[0m\t\n",
            "\u001b[0m1959/2115 (epoch 4.631), train_loss = 1.54991604, grad/param norm = 9.8622e-02, time/batch = 0.4162s\u001b[0m\t\n",
            "\u001b[0m1960/2115 (epoch 4.634), train_loss = 1.52420318, grad/param norm = 1.1534e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m1961/2115 (epoch 4.636), train_loss = 1.48597798, grad/param norm = 1.1217e-01, time/batch = 0.4460s\u001b[0m\t\n",
            "\u001b[0m1962/2115 (epoch 4.638), train_loss = 1.48828961, grad/param norm = 1.0367e-01, time/batch = 0.4218s\u001b[0m\t\n",
            "\u001b[0m1963/2115 (epoch 4.641), train_loss = 1.49907871, grad/param norm = 9.5376e-02, time/batch = 0.4353s\u001b[0m\t\n",
            "\u001b[0m1964/2115 (epoch 4.643), train_loss = 1.51087912, grad/param norm = 1.0444e-01, time/batch = 0.4204s\u001b[0m\t\n",
            "\u001b[0m1965/2115 (epoch 4.645), train_loss = 1.44673166, grad/param norm = 1.1065e-01, time/batch = 0.4528s\u001b[0m\t\n",
            "\u001b[0m1966/2115 (epoch 4.648), train_loss = 1.55816462, grad/param norm = 1.1274e-01, time/batch = 0.4097s\u001b[0m\t\n",
            "\u001b[0m1967/2115 (epoch 4.650), train_loss = 1.53980862, grad/param norm = 1.2254e-01, time/batch = 0.4292s\u001b[0m\t\n",
            "\u001b[0m1968/2115 (epoch 4.652), train_loss = 1.54768066, grad/param norm = 1.1636e-01, time/batch = 0.4324s\u001b[0m\t\n",
            "\u001b[0m1969/2115 (epoch 4.655), train_loss = 1.54141203, grad/param norm = 1.2385e-01, time/batch = 0.4289s\u001b[0m\t\n",
            "\u001b[0m1970/2115 (epoch 4.657), train_loss = 1.52651004, grad/param norm = 1.0681e-01, time/batch = 0.4444s\u001b[0m\t\n",
            "\u001b[0m1971/2115 (epoch 4.660), train_loss = 1.48932520, grad/param norm = 1.1136e-01, time/batch = 0.4348s\u001b[0m\t\n",
            "\u001b[0m1972/2115 (epoch 4.662), train_loss = 1.51884613, grad/param norm = 1.2210e-01, time/batch = 0.5346s\u001b[0m\t\n",
            "\u001b[0m1973/2115 (epoch 4.664), train_loss = 1.47874129, grad/param norm = 1.1104e-01, time/batch = 0.9292s\u001b[0m\t\n",
            "\u001b[0m1974/2115 (epoch 4.667), train_loss = 1.54917243, grad/param norm = 1.0645e-01, time/batch = 0.8744s\u001b[0m\t\n",
            "\u001b[0m1975/2115 (epoch 4.669), train_loss = 1.48959037, grad/param norm = 1.0122e-01, time/batch = 0.8777s\u001b[0m\t\n",
            "\u001b[0m1976/2115 (epoch 4.671), train_loss = 1.46216070, grad/param norm = 1.0985e-01, time/batch = 0.6559s\u001b[0m\t\n",
            "\u001b[0m1977/2115 (epoch 4.674), train_loss = 1.51101750, grad/param norm = 1.2159e-01, time/batch = 0.4224s\u001b[0m\t\n",
            "\u001b[0m1978/2115 (epoch 4.676), train_loss = 1.47950532, grad/param norm = 1.2925e-01, time/batch = 0.4431s\u001b[0m\t\n",
            "\u001b[0m1979/2115 (epoch 4.678), train_loss = 1.48057900, grad/param norm = 1.0764e-01, time/batch = 0.4351s\u001b[0m\t\n",
            "\u001b[0m1980/2115 (epoch 4.681), train_loss = 1.54480612, grad/param norm = 1.0803e-01, time/batch = 0.4201s\u001b[0m\t\n",
            "\u001b[0m1981/2115 (epoch 4.683), train_loss = 1.54088487, grad/param norm = 1.2191e-01, time/batch = 0.4407s\u001b[0m\t\n",
            "\u001b[0m1982/2115 (epoch 4.686), train_loss = 1.51811824, grad/param norm = 1.4226e-01, time/batch = 0.4246s\u001b[0m\t\n",
            "\u001b[0m1983/2115 (epoch 4.688), train_loss = 1.51011040, grad/param norm = 1.6820e-01, time/batch = 0.4271s\u001b[0m\t\n",
            "\u001b[0m1984/2115 (epoch 4.690), train_loss = 1.48174818, grad/param norm = 1.4018e-01, time/batch = 0.4337s\u001b[0m\t\n",
            "\u001b[0m1985/2115 (epoch 4.693), train_loss = 1.52492462, grad/param norm = 1.1446e-01, time/batch = 0.4445s\u001b[0m\t\n",
            "\u001b[0m1986/2115 (epoch 4.695), train_loss = 1.53810463, grad/param norm = 1.1651e-01, time/batch = 0.4324s\u001b[0m\t\n",
            "\u001b[0m1987/2115 (epoch 4.697), train_loss = 1.51164695, grad/param norm = 1.1351e-01, time/batch = 0.4161s\u001b[0m\t\n",
            "\u001b[0m1988/2115 (epoch 4.700), train_loss = 1.54831447, grad/param norm = 1.2420e-01, time/batch = 0.4479s\u001b[0m\t\n",
            "\u001b[0m1989/2115 (epoch 4.702), train_loss = 1.52876142, grad/param norm = 1.0586e-01, time/batch = 0.4263s\u001b[0m\t\n",
            "\u001b[0m1990/2115 (epoch 4.704), train_loss = 1.48859885, grad/param norm = 1.2027e-01, time/batch = 0.4338s\u001b[0m\t\n",
            "\u001b[0m1991/2115 (epoch 4.707), train_loss = 1.57126621, grad/param norm = 1.2207e-01, time/batch = 0.4253s\u001b[0m\t\n",
            "\u001b[0m1992/2115 (epoch 4.709), train_loss = 1.44647554, grad/param norm = 1.1632e-01, time/batch = 0.4478s\u001b[0m\t\n",
            "\u001b[0m1993/2115 (epoch 4.712), train_loss = 1.47219866, grad/param norm = 1.1492e-01, time/batch = 0.4334s\u001b[0m\t\n",
            "\u001b[0m1994/2115 (epoch 4.714), train_loss = 1.51659353, grad/param norm = 1.1213e-01, time/batch = 0.4183s\u001b[0m\t\n",
            "\u001b[0m1995/2115 (epoch 4.716), train_loss = 1.45968521, grad/param norm = 1.2117e-01, time/batch = 0.4432s\u001b[0m\t\n",
            "\u001b[0m1996/2115 (epoch 4.719), train_loss = 1.44719176, grad/param norm = 1.0954e-01, time/batch = 0.4278s\u001b[0m\t\n",
            "\u001b[0m1997/2115 (epoch 4.721), train_loss = 1.53856611, grad/param norm = 1.1559e-01, time/batch = 0.4474s\u001b[0m\t\n",
            "\u001b[0m1998/2115 (epoch 4.723), train_loss = 1.53388636, grad/param norm = 1.1949e-01, time/batch = 0.4786s\u001b[0m\t\n",
            "\u001b[0m1999/2115 (epoch 4.726), train_loss = 1.45521095, grad/param norm = 1.1284e-01, time/batch = 0.8307s\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch4.73_1.5524.t7\u001b[0m\t\n",
            "\u001b[0m2000/2115 (epoch 4.728), train_loss = 1.51006578, grad/param norm = 1.1244e-01, time/batch = 1.0731s\u001b[0m\t\n",
            "\u001b[0m2001/2115 (epoch 4.730), train_loss = 1.71738646, grad/param norm = 1.1743e-01, time/batch = 0.4570s\u001b[0m\t\n",
            "\u001b[0m2002/2115 (epoch 4.733), train_loss = 1.53227866, grad/param norm = 1.1391e-01, time/batch = 0.4188s\u001b[0m\t\n",
            "\u001b[0m2003/2115 (epoch 4.735), train_loss = 1.49098378, grad/param norm = 1.1736e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m2004/2115 (epoch 4.738), train_loss = 1.56187015, grad/param norm = 1.1555e-01, time/batch = 0.4259s\u001b[0m\t\n",
            "\u001b[0m2005/2115 (epoch 4.740), train_loss = 1.55175079, grad/param norm = 1.2566e-01, time/batch = 0.4432s\u001b[0m\t\n",
            "\u001b[0m2006/2115 (epoch 4.742), train_loss = 1.56942390, grad/param norm = 1.3301e-01, time/batch = 0.4254s\u001b[0m\t\n",
            "\u001b[0m2007/2115 (epoch 4.745), train_loss = 1.48240462, grad/param norm = 1.1102e-01, time/batch = 0.4263s\u001b[0m\t\n",
            "\u001b[0m2008/2115 (epoch 4.747), train_loss = 1.50354268, grad/param norm = 1.1514e-01, time/batch = 0.4499s\u001b[0m\t\n",
            "\u001b[0m2009/2115 (epoch 4.749), train_loss = 1.51662539, grad/param norm = 1.1431e-01, time/batch = 0.4263s\u001b[0m\t\n",
            "\u001b[0m2010/2115 (epoch 4.752), train_loss = 1.51640878, grad/param norm = 1.2119e-01, time/batch = 0.4578s\u001b[0m\t\n",
            "\u001b[0m2011/2115 (epoch 4.754), train_loss = 1.47496387, grad/param norm = 1.0587e-01, time/batch = 0.4296s\u001b[0m\t\n",
            "\u001b[0m2012/2115 (epoch 4.757), train_loss = 1.43081976, grad/param norm = 9.5497e-02, time/batch = 0.4456s\u001b[0m\t\n",
            "\u001b[0m2013/2115 (epoch 4.759), train_loss = 1.43629704, grad/param norm = 1.0924e-01, time/batch = 0.6700s\u001b[0m\t\n",
            "\u001b[0m2014/2115 (epoch 4.761), train_loss = 1.49293610, grad/param norm = 1.2086e-01, time/batch = 1.0124s\u001b[0m\t\n",
            "\u001b[0m2015/2115 (epoch 4.764), train_loss = 1.46859533, grad/param norm = 1.1590e-01, time/batch = 0.9405s\u001b[0m\t\n",
            "\u001b[0m2016/2115 (epoch 4.766), train_loss = 1.44045983, grad/param norm = 1.0272e-01, time/batch = 0.8198s\u001b[0m\t\n",
            "\u001b[0m2017/2115 (epoch 4.768), train_loss = 1.48623267, grad/param norm = 1.1592e-01, time/batch = 0.4177s\u001b[0m\t\n",
            "\u001b[0m2018/2115 (epoch 4.771), train_loss = 1.48040034, grad/param norm = 1.0840e-01, time/batch = 0.4399s\u001b[0m\t\n",
            "\u001b[0m2019/2115 (epoch 4.773), train_loss = 1.50382834, grad/param norm = 1.2160e-01, time/batch = 0.4281s\u001b[0m\t\n",
            "\u001b[0m2020/2115 (epoch 4.775), train_loss = 1.46983297, grad/param norm = 1.2301e-01, time/batch = 0.4115s\u001b[0m\t\n",
            "\u001b[0m2021/2115 (epoch 4.778), train_loss = 1.54106708, grad/param norm = 1.2738e-01, time/batch = 0.4290s\u001b[0m\t\n",
            "\u001b[0m2022/2115 (epoch 4.780), train_loss = 1.48085237, grad/param norm = 1.2228e-01, time/batch = 0.4443s\u001b[0m\t\n",
            "\u001b[0m2023/2115 (epoch 4.783), train_loss = 1.46500056, grad/param norm = 1.1475e-01, time/batch = 0.4635s\u001b[0m\t\n",
            "\u001b[0m2024/2115 (epoch 4.785), train_loss = 1.51745666, grad/param norm = 1.1354e-01, time/batch = 0.4251s\u001b[0m\t\n",
            "\u001b[0m2025/2115 (epoch 4.787), train_loss = 1.47014865, grad/param norm = 1.0029e-01, time/batch = 0.4456s\u001b[0m\t\n",
            "\u001b[0m2026/2115 (epoch 4.790), train_loss = 1.40633423, grad/param norm = 9.7874e-02, time/batch = 0.4300s\u001b[0m\t\n",
            "\u001b[0m2027/2115 (epoch 4.792), train_loss = 1.45973564, grad/param norm = 1.0960e-01, time/batch = 0.4266s\u001b[0m\t\n",
            "\u001b[0m2028/2115 (epoch 4.794), train_loss = 1.47957849, grad/param norm = 1.0449e-01, time/batch = 0.4471s\u001b[0m\t\n",
            "\u001b[0m2029/2115 (epoch 4.797), train_loss = 1.52211251, grad/param norm = 1.0766e-01, time/batch = 0.4216s\u001b[0m\t\n",
            "\u001b[0m2030/2115 (epoch 4.799), train_loss = 1.49455694, grad/param norm = 1.0322e-01, time/batch = 0.4424s\u001b[0m\t\n",
            "\u001b[0m2031/2115 (epoch 4.801), train_loss = 1.58330292, grad/param norm = 1.3044e-01, time/batch = 0.4249s\u001b[0m\t\n",
            "\u001b[0m2032/2115 (epoch 4.804), train_loss = 1.48377325, grad/param norm = 1.2940e-01, time/batch = 0.4362s\u001b[0m\t\n",
            "\u001b[0m2033/2115 (epoch 4.806), train_loss = 1.52759347, grad/param norm = 1.2941e-01, time/batch = 0.4269s\u001b[0m\t\n",
            "\u001b[0m2034/2115 (epoch 4.809), train_loss = 1.43728218, grad/param norm = 1.2644e-01, time/batch = 0.4144s\u001b[0m\t\n",
            "\u001b[0m2035/2115 (epoch 4.811), train_loss = 1.51422641, grad/param norm = 1.3216e-01, time/batch = 0.4414s\u001b[0m\t\n",
            "\u001b[0m2036/2115 (epoch 4.813), train_loss = 1.50407525, grad/param norm = 1.4680e-01, time/batch = 0.4152s\u001b[0m\t\n",
            "\u001b[0m2037/2115 (epoch 4.816), train_loss = 1.44470112, grad/param norm = 1.1944e-01, time/batch = 0.4394s\u001b[0m\t\n",
            "\u001b[0m2038/2115 (epoch 4.818), train_loss = 1.50098977, grad/param norm = 1.0468e-01, time/batch = 0.4218s\u001b[0m\t\n",
            "\u001b[0m2039/2115 (epoch 4.820), train_loss = 1.46253023, grad/param norm = 1.0422e-01, time/batch = 0.5972s\u001b[0m\t\n",
            "\u001b[0m2040/2115 (epoch 4.823), train_loss = 1.53923705, grad/param norm = 1.0108e-01, time/batch = 0.7167s\u001b[0m\t\n",
            "\u001b[0m2041/2115 (epoch 4.825), train_loss = 1.46390104, grad/param norm = 9.5220e-02, time/batch = 1.0305s\u001b[0m\t\n",
            "\u001b[0m2042/2115 (epoch 4.827), train_loss = 1.57692481, grad/param norm = 1.0561e-01, time/batch = 0.8594s\u001b[0m\t\n",
            "\u001b[0m2043/2115 (epoch 4.830), train_loss = 1.51981170, grad/param norm = 1.0173e-01, time/batch = 0.5738s\u001b[0m\t\n",
            "\u001b[0m2044/2115 (epoch 4.832), train_loss = 1.52294280, grad/param norm = 1.0593e-01, time/batch = 0.4218s\u001b[0m\t\n",
            "\u001b[0m2045/2115 (epoch 4.835), train_loss = 1.42760095, grad/param norm = 1.0931e-01, time/batch = 0.4562s\u001b[0m\t\n",
            "\u001b[0m2046/2115 (epoch 4.837), train_loss = 1.45695121, grad/param norm = 1.1068e-01, time/batch = 0.4326s\u001b[0m\t\n",
            "\u001b[0m2047/2115 (epoch 4.839), train_loss = 1.43978203, grad/param norm = 1.0527e-01, time/batch = 0.4367s\u001b[0m\t\n",
            "\u001b[0m2048/2115 (epoch 4.842), train_loss = 1.47267420, grad/param norm = 1.1555e-01, time/batch = 0.4524s\u001b[0m\t\n",
            "\u001b[0m2049/2115 (epoch 4.844), train_loss = 1.55529566, grad/param norm = 1.2087e-01, time/batch = 0.4300s\u001b[0m\t\n",
            "\u001b[0m2050/2115 (epoch 4.846), train_loss = 1.55344133, grad/param norm = 1.2547e-01, time/batch = 0.4480s\u001b[0m\t\n",
            "\u001b[0m2051/2115 (epoch 4.849), train_loss = 1.46317701, grad/param norm = 1.0911e-01, time/batch = 0.4309s\u001b[0m\t\n",
            "\u001b[0m2052/2115 (epoch 4.851), train_loss = 1.46446106, grad/param norm = 1.0453e-01, time/batch = 0.4436s\u001b[0m\t\n",
            "\u001b[0m2053/2115 (epoch 4.853), train_loss = 1.52540700, grad/param norm = 1.1191e-01, time/batch = 0.4282s\u001b[0m\t\n",
            "\u001b[0m2054/2115 (epoch 4.856), train_loss = 1.47725201, grad/param norm = 1.2380e-01, time/batch = 0.4573s\u001b[0m\t\n",
            "\u001b[0m2055/2115 (epoch 4.858), train_loss = 1.47093865, grad/param norm = 1.0576e-01, time/batch = 0.4717s\u001b[0m\t\n",
            "\u001b[0m2056/2115 (epoch 4.861), train_loss = 1.50639567, grad/param norm = 1.1297e-01, time/batch = 0.4293s\u001b[0m\t\n",
            "\u001b[0m2057/2115 (epoch 4.863), train_loss = 1.51744768, grad/param norm = 1.2665e-01, time/batch = 0.4529s\u001b[0m\t\n",
            "\u001b[0m2058/2115 (epoch 4.865), train_loss = 1.46801324, grad/param norm = 1.2977e-01, time/batch = 0.4176s\u001b[0m\t\n",
            "\u001b[0m2059/2115 (epoch 4.868), train_loss = 1.43721048, grad/param norm = 1.2116e-01, time/batch = 0.4396s\u001b[0m\t\n",
            "\u001b[0m2060/2115 (epoch 4.870), train_loss = 1.44775852, grad/param norm = 1.1204e-01, time/batch = 0.4222s\u001b[0m\t\n",
            "\u001b[0m2061/2115 (epoch 4.872), train_loss = 1.46405595, grad/param norm = 1.1953e-01, time/batch = 0.4424s\u001b[0m\t\n",
            "\u001b[0m2062/2115 (epoch 4.875), train_loss = 1.47177768, grad/param norm = 1.2285e-01, time/batch = 0.4141s\u001b[0m\t\n",
            "\u001b[0m2063/2115 (epoch 4.877), train_loss = 1.58064426, grad/param norm = 1.1257e-01, time/batch = 0.4180s\u001b[0m\t\n",
            "\u001b[0m2064/2115 (epoch 4.879), train_loss = 1.46846435, grad/param norm = 1.1900e-01, time/batch = 0.4483s\u001b[0m\t\n",
            "\u001b[0m2065/2115 (epoch 4.882), train_loss = 1.52194729, grad/param norm = 1.0935e-01, time/batch = 0.5065s\u001b[0m\t\n",
            "\u001b[0m2066/2115 (epoch 4.884), train_loss = 1.49922520, grad/param norm = 1.0731e-01, time/batch = 0.7709s\u001b[0m\t\n",
            "\u001b[0m2067/2115 (epoch 4.887), train_loss = 1.52160330, grad/param norm = 1.2301e-01, time/batch = 0.9299s\u001b[0m\t\n",
            "\u001b[0m2068/2115 (epoch 4.889), train_loss = 1.47997405, grad/param norm = 1.1118e-01, time/batch = 0.9136s\u001b[0m\t\n",
            "\u001b[0m2069/2115 (epoch 4.891), train_loss = 1.51508530, grad/param norm = 1.1026e-01, time/batch = 0.7047s\u001b[0m\t\n",
            "\u001b[0m2070/2115 (epoch 4.894), train_loss = 1.45905979, grad/param norm = 1.1396e-01, time/batch = 0.4406s\u001b[0m\t\n",
            "\u001b[0m2071/2115 (epoch 4.896), train_loss = 1.47114319, grad/param norm = 1.0269e-01, time/batch = 0.4262s\u001b[0m\t\n",
            "\u001b[0m2072/2115 (epoch 4.898), train_loss = 1.45586884, grad/param norm = 1.0960e-01, time/batch = 0.4414s\u001b[0m\t\n",
            "\u001b[0m2073/2115 (epoch 4.901), train_loss = 1.50789150, grad/param norm = 1.1538e-01, time/batch = 0.4197s\u001b[0m\t\n",
            "\u001b[0m2074/2115 (epoch 4.903), train_loss = 1.49300571, grad/param norm = 1.1069e-01, time/batch = 0.4669s\u001b[0m\t\n",
            "\u001b[0m2075/2115 (epoch 4.905), train_loss = 1.52654111, grad/param norm = 1.0765e-01, time/batch = 0.4395s\u001b[0m\t\n",
            "\u001b[0m2076/2115 (epoch 4.908), train_loss = 1.49378562, grad/param norm = 1.0878e-01, time/batch = 0.4359s\u001b[0m\t\n",
            "\u001b[0m2077/2115 (epoch 4.910), train_loss = 1.48376577, grad/param norm = 1.0988e-01, time/batch = 0.4415s\u001b[0m\t\n",
            "\u001b[0m2078/2115 (epoch 4.913), train_loss = 1.55155560, grad/param norm = 1.1292e-01, time/batch = 0.4231s\u001b[0m\t\n",
            "\u001b[0m2079/2115 (epoch 4.915), train_loss = 1.51381156, grad/param norm = 1.0103e-01, time/batch = 0.4431s\u001b[0m\t\n",
            "\u001b[0m2080/2115 (epoch 4.917), train_loss = 1.51479444, grad/param norm = 1.0614e-01, time/batch = 0.4264s\u001b[0m\t\n",
            "\u001b[0m2081/2115 (epoch 4.920), train_loss = 1.51900132, grad/param norm = 1.2116e-01, time/batch = 0.4517s\u001b[0m\t\n",
            "\u001b[0m2082/2115 (epoch 4.922), train_loss = 1.45544589, grad/param norm = 1.2214e-01, time/batch = 0.4509s\u001b[0m\t\n",
            "\u001b[0m2083/2115 (epoch 4.924), train_loss = 1.47182366, grad/param norm = 1.1768e-01, time/batch = 0.4332s\u001b[0m\t\n",
            "\u001b[0m2084/2115 (epoch 4.927), train_loss = 1.48119874, grad/param norm = 1.2284e-01, time/batch = 0.4497s\u001b[0m\t\n",
            "\u001b[0m2085/2115 (epoch 4.929), train_loss = 1.49168308, grad/param norm = 1.2519e-01, time/batch = 0.4209s\u001b[0m\t\n",
            "\u001b[0m2086/2115 (epoch 4.931), train_loss = 1.55039103, grad/param norm = 1.1766e-01, time/batch = 0.4311s\u001b[0m\t\n",
            "\u001b[0m2087/2115 (epoch 4.934), train_loss = 1.45343663, grad/param norm = 1.1856e-01, time/batch = 0.4338s\u001b[0m\t\n",
            "\u001b[0m2088/2115 (epoch 4.936), train_loss = 1.51886352, grad/param norm = 1.2482e-01, time/batch = 0.4532s\u001b[0m\t\n",
            "\u001b[0m2089/2115 (epoch 4.939), train_loss = 1.44941606, grad/param norm = 1.2044e-01, time/batch = 0.4334s\u001b[0m\t\n",
            "\u001b[0m2090/2115 (epoch 4.941), train_loss = 1.42164806, grad/param norm = 1.0585e-01, time/batch = 0.4212s\u001b[0m\t\n",
            "\u001b[0m2091/2115 (epoch 4.943), train_loss = 1.48334150, grad/param norm = 1.0082e-01, time/batch = 0.4514s\u001b[0m\t\n",
            "\u001b[0m2092/2115 (epoch 4.946), train_loss = 1.44354280, grad/param norm = 1.0801e-01, time/batch = 0.7807s\u001b[0m\t\n",
            "\u001b[0m2093/2115 (epoch 4.948), train_loss = 1.52205009, grad/param norm = 1.0539e-01, time/batch = 0.9740s\u001b[0m\t\n",
            "\u001b[0m2094/2115 (epoch 4.950), train_loss = 1.50010285, grad/param norm = 1.2438e-01, time/batch = 0.9353s\u001b[0m\t\n",
            "\u001b[0m2095/2115 (epoch 4.953), train_loss = 1.50179496, grad/param norm = 1.1622e-01, time/batch = 0.6766s\u001b[0m\t\n",
            "\u001b[0m2096/2115 (epoch 4.955), train_loss = 1.50905518, grad/param norm = 1.0944e-01, time/batch = 0.4444s\u001b[0m\t\n",
            "\u001b[0m2097/2115 (epoch 4.957), train_loss = 1.49388148, grad/param norm = 1.0617e-01, time/batch = 0.4273s\u001b[0m\t\n",
            "\u001b[0m2098/2115 (epoch 4.960), train_loss = 1.51201032, grad/param norm = 1.2002e-01, time/batch = 0.4255s\u001b[0m\t\n",
            "\u001b[0m2099/2115 (epoch 4.962), train_loss = 1.52146100, grad/param norm = 1.1471e-01, time/batch = 0.4420s\u001b[0m\t\n",
            "\u001b[0m2100/2115 (epoch 4.965), train_loss = 1.48513539, grad/param norm = 1.1608e-01, time/batch = 0.4227s\u001b[0m\t\n",
            "\u001b[0m2101/2115 (epoch 4.967), train_loss = 1.53339451, grad/param norm = 1.1050e-01, time/batch = 0.4504s\u001b[0m\t\n",
            "\u001b[0m2102/2115 (epoch 4.969), train_loss = 1.51012270, grad/param norm = 1.3861e-01, time/batch = 0.4256s\u001b[0m\t\n",
            "\u001b[0m2103/2115 (epoch 4.972), train_loss = 1.53872134, grad/param norm = 1.4314e-01, time/batch = 0.4419s\u001b[0m\t\n",
            "\u001b[0m2104/2115 (epoch 4.974), train_loss = 1.47081995, grad/param norm = 1.3198e-01, time/batch = 0.4185s\u001b[0m\t\n",
            "\u001b[0m2105/2115 (epoch 4.976), train_loss = 1.52022996, grad/param norm = 1.2309e-01, time/batch = 0.4070s\u001b[0m\t\n",
            "\u001b[0m2106/2115 (epoch 4.979), train_loss = 1.44450766, grad/param norm = 1.1055e-01, time/batch = 0.4508s\u001b[0m\t\n",
            "\u001b[0m2107/2115 (epoch 4.981), train_loss = 1.46749643, grad/param norm = 9.8151e-02, time/batch = 0.4204s\u001b[0m\t\n",
            "\u001b[0m2108/2115 (epoch 4.983), train_loss = 1.49211457, grad/param norm = 1.0230e-01, time/batch = 0.4383s\u001b[0m\t\n",
            "\u001b[0m2109/2115 (epoch 4.986), train_loss = 1.47898643, grad/param norm = 9.7618e-02, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0m2110/2115 (epoch 4.988), train_loss = 1.46190918, grad/param norm = 9.7917e-02, time/batch = 0.4206s\u001b[0m\t\n",
            "\u001b[0m2111/2115 (epoch 4.991), train_loss = 1.47535665, grad/param norm = 1.0301e-01, time/batch = 0.4433s\u001b[0m\t\n",
            "\u001b[0m2112/2115 (epoch 4.993), train_loss = 1.50808014, grad/param norm = 1.0116e-01, time/batch = 0.4129s\u001b[0m\t\n",
            "\u001b[0m2113/2115 (epoch 4.995), train_loss = 1.50207518, grad/param norm = 1.1005e-01, time/batch = 0.4554s\u001b[0m\t\n",
            "\u001b[0m2114/2115 (epoch 4.998), train_loss = 1.52807408, grad/param norm = 1.3142e-01, time/batch = 0.4441s\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch5.00_1.5482.t7\u001b[0m\t\n",
            "\u001b[0m2115/2115 (epoch 5.000), train_loss = 1.55228931, grad/param norm = 1.6225e-01, time/batch = 0.4432s\u001b[0m\t\n"
          ]
        }
      ],
      "source": [
        "! th train.lua -gpuid -1 -max_epochs 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!th sample.lua  cv/lm_lstm_epoch5.00_1.5482.t7 -gpuid -1 -length 500 -temperature 0.8 -primetext \"The \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5Av8jUlBFU0",
        "outputId": "505a7293-8b2b-47fd-9bec-990a57c88d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mcreating an lstm...\u001b[0m\t\n",
            "\u001b[0mseeding with The \u001b[0m\t\n",
            "\u001b[0m--------------------------\u001b[0m\t\n",
            "The stand of mortard be\n",
            "The lading pard of king pronound in huspard\n",
            "Than a thoushard of he stremdes me them pleas,\n",
            "I prove polish pospenting of a sent,--\n",
            "Dismanch, and pend, the bear of yet say\n",
            "To have it and father then me company\n",
            "Than she writ in nobled for the rud out wonch\n",
            "Is the agport of my both, and he will not she\n",
            "gave have the varding and this ban of my indart my sat it\n",
            "in my hims! be one thou aghey there shen\n",
            "To poor that be pronougn destricious grace\n",
            "And thin is with take you again, and p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls cv/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0ZwENtrCEgx",
        "outputId": "7e145642-dedf-4f88-d5ef-788bad1ad04b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lm_lstm_epoch2.36_1.7680.t7  lm_lstm_epoch4.73_1.5524.t7  lm_lstm_epoch5.00_1.5482.t7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 37 Epochs - Original RNN"
      ],
      "metadata": {
        "id": "KJ50_8nYoI6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! th train.lua -gpuid -1 -max_epochs 50 -print_every 500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSYmKhET82RC",
        "outputId": "5277d843-7c74-4328-9d94-0f2c0a16dcc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mloading data files...\u001b[0m\t\n",
            "\u001b[0mcutting off end of data so that the batches/sequences divide evenly\u001b[0m\t\n",
            "\u001b[0mreshaping tensor...\u001b[0m\t\n",
            "\u001b[0mdata load done. Number of data batches in train: 423, val: 23, test: 0\u001b[0m\t\n",
            "\u001b[0mvocab size: 65\u001b[0m\t\n",
            "\u001b[0mcreating an lstm with 2 layers\u001b[0m\t\n",
            "\u001b[0msetting forget gate biases to 1 in LSTM layer 1\u001b[0m\t\n",
            "\u001b[0msetting forget gate biases to 1 in LSTM layer 2\u001b[0m\t\n",
            "\u001b[0mnumber of parameters in the model: 240321\u001b[0m\t\n",
            "\u001b[0mcloning rnn\u001b[0m\t\n",
            "\u001b[0mcloning criterion\u001b[0m\t\n",
            "\u001b[0m500/21150 (epoch 1.182), train_loss = 2.03018467, grad/param norm = 2.6614e-01, time/batch = 0.4088s\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch2.36_1.7680.t7\u001b[0m\t\n",
            "\u001b[0m1000/21150 (epoch 2.364), train_loss = 1.72991061, grad/param norm = 1.9743e-01, time/batch = 0.4257s\u001b[0m\t\n",
            "\u001b[0m1500/21150 (epoch 3.546), train_loss = 1.64052275, grad/param norm = 1.4567e-01, time/batch = 0.8805s\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch4.73_1.5524.t7\u001b[0m\t\n",
            "\u001b[0m2000/21150 (epoch 4.728), train_loss = 1.51006578, grad/param norm = 1.1244e-01, time/batch = 0.4214s\u001b[0m\t\n",
            "\u001b[0m2500/21150 (epoch 5.910), train_loss = 1.43894094, grad/param norm = 1.0725e-01, time/batch = 0.4115s\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch7.09_1.4655.t7\u001b[0m\t\n",
            "\u001b[0m3000/21150 (epoch 7.092), train_loss = 1.44981201, grad/param norm = 9.7278e-02, time/batch = 0.4392s\u001b[0m\t\n",
            "\u001b[0m3500/21150 (epoch 8.274), train_loss = 1.39565204, grad/param norm = 1.0635e-01, time/batch = 0.4366s\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch9.46_1.4314.t7\u001b[0m\t\n",
            "\u001b[0m4000/21150 (epoch 9.456), train_loss = 1.33701948, grad/param norm = 8.0051e-02, time/batch = 0.4118s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00194\u001b[0m\t\n",
            "\u001b[0m4500/21150 (epoch 10.638), train_loss = 1.29835540, grad/param norm = 8.3262e-02, time/batch = 0.4140s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0018818\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch11.82_1.4131.t7\u001b[0m\t\n",
            "\u001b[0m5000/21150 (epoch 11.820), train_loss = 1.28910164, grad/param norm = 7.8439e-02, time/batch = 0.4186s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.001825346\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00177058562\u001b[0m\t\n",
            "\u001b[0m5500/21150 (epoch 13.002), train_loss = 1.53320074, grad/param norm = 9.5464e-02, time/batch = 0.4380s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0017174680514\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch14.18_1.4000.t7\u001b[0m\t\n",
            "\u001b[0m6000/21150 (epoch 14.184), train_loss = 1.30196435, grad/param norm = 7.8233e-02, time/batch = 0.8177s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.001665944009858\u001b[0m\t\n",
            "\u001b[0m6500/21150 (epoch 15.366), train_loss = 1.28361406, grad/param norm = 7.4694e-02, time/batch = 0.4167s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0016159656895623\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch16.55_1.3963.t7\u001b[0m\t\n",
            "\u001b[0m7000/21150 (epoch 16.548), train_loss = 1.29866853, grad/param norm = 7.3420e-02, time/batch = 0.4342s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0015674867188754\u001b[0m\t\n",
            "\u001b[0m7500/21150 (epoch 17.730), train_loss = 1.30829962, grad/param norm = 7.7302e-02, time/batch = 0.4279s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0015204621173091\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch18.91_1.3938.t7\u001b[0m\t\n",
            "\u001b[0m8000/21150 (epoch 18.913), train_loss = 1.29278989, grad/param norm = 8.2264e-02, time/batch = 0.9658s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0014748482537899\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0014306028061762\u001b[0m\t\n",
            "\u001b[0m8500/21150 (epoch 20.095), train_loss = 1.26899519, grad/param norm = 7.2371e-02, time/batch = 0.4432s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0013876847219909\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch21.28_1.3971.t7\u001b[0m\t\n",
            "\u001b[0m9000/21150 (epoch 21.277), train_loss = 1.23985672, grad/param norm = 7.4099e-02, time/batch = 0.4517s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0013460541803311\u001b[0m\t\n",
            "\u001b[0m9500/21150 (epoch 22.459), train_loss = 1.24640761, grad/param norm = 7.2843e-02, time/batch = 0.4396s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0013056725549212\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch23.64_1.3999.t7\u001b[0m\t\n",
            "\u001b[0m10000/21150 (epoch 23.641), train_loss = 1.22761321, grad/param norm = 7.8091e-02, time/batch = 0.4459s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0012665023782736\u001b[0m\t\n",
            "\u001b[0m10500/21150 (epoch 24.823), train_loss = 1.24397896, grad/param norm = 7.4166e-02, time/batch = 0.4378s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0012285073069254\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0011916520877176\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch26.00_1.3952.t7\u001b[0m\t\n",
            "\u001b[0m11000/21150 (epoch 26.005), train_loss = 1.18680661, grad/param norm = 7.5182e-02, time/batch = 0.4612s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0011559025250861\u001b[0m\t\n",
            "\u001b[0m11500/21150 (epoch 27.187), train_loss = 1.20496258, grad/param norm = 7.5210e-02, time/batch = 0.4328s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0011212254493335\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch28.37_1.4009.t7\u001b[0m\t\n",
            "\u001b[0m12000/21150 (epoch 28.369), train_loss = 1.20184700, grad/param norm = 7.3567e-02, time/batch = 0.4281s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0010875886858535\u001b[0m\t\n",
            "\u001b[0m12500/21150 (epoch 29.551), train_loss = 1.22785235, grad/param norm = 7.2733e-02, time/batch = 0.4319s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0010549610252779\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch30.73_1.4070.t7\u001b[0m\t\n",
            "\u001b[0m13000/21150 (epoch 30.733), train_loss = 1.18574379, grad/param norm = 7.0727e-02, time/batch = 0.4380s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.0010233121945196\u001b[0m\t\n",
            "\u001b[0m13500/21150 (epoch 31.915), train_loss = 1.17546253, grad/param norm = 7.3085e-02, time/batch = 0.4346s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00099261282868397\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00096283444382345\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch33.10_1.4067.t7\u001b[0m\t\n",
            "\u001b[0m14000/21150 (epoch 33.097), train_loss = 1.22435533, grad/param norm = 7.6169e-02, time/batch = 0.4337s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00093394941050874\u001b[0m\t\n",
            "\u001b[0m14500/21150 (epoch 34.279), train_loss = 1.24252798, grad/param norm = 7.1042e-02, time/batch = 0.4392s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00090593092819348\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch35.46_1.4118.t7\u001b[0m\t\n",
            "\u001b[0m15000/21150 (epoch 35.461), train_loss = 1.17643860, grad/param norm = 7.4737e-02, time/batch = 0.4283s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00087875300034768\u001b[0m\t\n",
            "\u001b[0m15500/21150 (epoch 36.643), train_loss = 1.19708464, grad/param norm = 7.3648e-02, time/batch = 0.4336s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00085239041033725\u001b[0m\t\n",
            "\u001b[0mevaluating loss over split index 2\u001b[0m\t\n",
            "\u001b[0m1/23...\u001b[0m\t\n",
            "\u001b[0m2/23...\u001b[0m\t\n",
            "\u001b[0m3/23...\u001b[0m\t\n",
            "\u001b[0m4/23...\u001b[0m\t\n",
            "\u001b[0m5/23...\u001b[0m\t\n",
            "\u001b[0m6/23...\u001b[0m\t\n",
            "\u001b[0m7/23...\u001b[0m\t\n",
            "\u001b[0m8/23...\u001b[0m\t\n",
            "\u001b[0m9/23...\u001b[0m\t\n",
            "\u001b[0m10/23...\u001b[0m\t\n",
            "\u001b[0m11/23...\u001b[0m\t\n",
            "\u001b[0m12/23...\u001b[0m\t\n",
            "\u001b[0m13/23...\u001b[0m\t\n",
            "\u001b[0m14/23...\u001b[0m\t\n",
            "\u001b[0m15/23...\u001b[0m\t\n",
            "\u001b[0m16/23...\u001b[0m\t\n",
            "\u001b[0m17/23...\u001b[0m\t\n",
            "\u001b[0m18/23...\u001b[0m\t\n",
            "\u001b[0m19/23...\u001b[0m\t\n",
            "\u001b[0m20/23...\u001b[0m\t\n",
            "\u001b[0m21/23...\u001b[0m\t\n",
            "\u001b[0m22/23...\u001b[0m\t\n",
            "\u001b[0m23/23...\u001b[0m\t\n",
            "\u001b[0msaving checkpoint to cv/lm_lstm_epoch37.83_1.4166.t7\u001b[0m\t\n",
            "\u001b[0m16000/21150 (epoch 37.825), train_loss = 1.14778473, grad/param norm = 7.4840e-02, time/batch = 0.7682s\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00082681869802713\u001b[0m\t\n",
            "\u001b[0mdecayed learning rate by a factor 0.97 to 0.00080201413708631\u001b[0m\t\n",
            "\u001b[0m16500/21150 (epoch 39.007), train_loss = 1.18112533, grad/param norm = 7.2065e-02, time/batch = 0.4280s\u001b[0m\t\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After reading the documentation and code for the original https://github.com/karpathy/char-rnn, I trained the model on tiny Shakespeare for 5 epochs. This took 18 minutes to train and I decided it would be impossibly long and not worth it to train it on 50 and 500 epochs (estimated at 3 hours and 6.25 days respectively).\n",
        "\n",
        "Instead I have switched to a PyTorch implementation I found https://github.com/spro/char-rnn.pytorch which did run faster."
      ],
      "metadata": {
        "id": "ZqmYu5RgLQTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/spro/char-rnn.pytorch.git\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt shakespeare.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWlqcx1mxcPF",
        "outputId": "ac83164d-87b9-4165-8d6f-22c40d95b369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'char-rnn.pytorch'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Total 54 (delta 0), reused 0 (delta 0), pack-reused 54 (from 1)\u001b[K\n",
            "Receiving objects: 100% (54/54), 11.79 KiB | 3.93 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n",
            "--2025-02-26 13:58:54--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: input.txt\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  4.86MB/s    in 0.2s    \n",
            "\n",
            "2025-02-26 13:58:57 (4.86 MB/s) - input.txt saved [1115394/1115394]\n",
            "\n",
            "--2025-02-26 13:58:57--  http://shakespeare.txt/\n",
            "Resolving shakespeare.txt (shakespeare.txt)... failed: Name or service not known.\n",
            "wget: unable to resolve host address shakespeare.txt\n",
            "FINISHED --2025-02-26 13:58:57--\n",
            "Total wall clock time: 3.0s\n",
            "Downloaded: 1 files, 1.1M in 0.2s (4.86 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1vUN08Syg50",
        "outputId": "355c64ba-693d-4224-dfbd-48c5d81091d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Epochs - PyTorch RNN"
      ],
      "metadata": {
        "id": "QrvccdXenOw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py input.txt --n_epochs 5 --print_every 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5KSvCHwp7W1",
        "outputId": "a18430db-bb45-4679-8bf1-ad7edf367dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 5 epochs...\n",
            "  0% 0/5 [00:00<?, ?it/s][0m 1s (1 20%) 4.6088]\n",
            "WhlAvh{sGqC6IREyYZ+t:`[jg]T~\n",
            "g=neaeC(,*6zo:%9 \\\to-mE<Nti!*lFe:yas+TeeLvp7ZLU|~SXt`L[+@DoXys\"dFeH<-ek8, \n",
            "\n",
            " 20% 1/5 [00:01<00:04,  1.16s/it][0m 2s (2 40%) 4.2366]\n",
            "WhliWmiB(\feX& eiJase hi\n",
            "sB taAm sd ytdmB\u000bsa3Di|Ze!ygs m\n",
            "bdnhe&e  w'wdq]oEm'aat\n",
            "dzsTna?Vtv.WaU$Yiut sdy \n",
            "\n",
            " 40% 2/5 [00:02<00:03,  1.02s/it][0m 2s (3 60%) 3.5267]\n",
            "Wh ynhwd ole  aaosRanaee yu \n",
            "terah eeaedeyheewlopo oelh  eeedy eihat e etrue toe end n nelh\n",
            " ohe ls ia \n",
            "\n",
            " 60% 3/5 [00:03<00:01,  1.02it/s][0m 3s (4 80%) 3.3437]\n",
            "Whe onlwe\n",
            "la heitd tooan nI hlhoeato oto tnerosp nnia h,io ttort d tre roae h loeo hg eoret i iwsiolr  \n",
            "\n",
            " 80% 4/5 [00:03<00:00,  1.04it/s][0m 4s (5 100%) 3.3029]\n",
            "Whamy id po way eeti, ooaprtar uo ro huht ito  hihrodyy tioe t al pfnd hll a oh dei ooo oi anoo a l tl \n",
            "\n",
            "100% 5/5 [00:04<00:00,  1.03it/s]\n",
            "Saving...\n",
            "Saved as input.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python generate.py input.pt --prime_str \"The \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iuqzws1nxz_c",
        "outputId": "7c6569b2-cb51-491d-e4fc-de971ab103cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/generate.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder = torch.load(args.filename)\n",
            "The geb\n",
            "fad \n",
            "\n",
            "lg n ntida ee ain\n",
            "o. uh nbt ehu otet, nbl ohrst i e yy tiemf ntho otuT bths rbe rrhyo uain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 50 Epochs - PyTorch RNN"
      ],
      "metadata": {
        "id": "nq_8dFUgnINx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py input.txt --n_epochs 50 --print_every 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRD-nhRKm9si",
        "outputId": "dfb63bac-6501-4f66-dcca-e9ab7e901eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50 epochs...\n",
            " 18% 9/50 [00:08<00:35,  1.14it/s][0m 9s (10 20%) 2.8814]\n",
            "Whnilas iy Oi \n",
            "IThnt whin heedn Nwei woin eed mwis os os o on o, yoef, thhres;f boournsr gh\n",
            "a berg oen \n",
            "\n",
            " 38% 19/50 [00:17<00:27,  1.13it/s][0m 18s (20 40%) 2.4561]\n",
            "What I wind ind rou'd wes th Mornl tos.\n",
            "\n",
            "GROASNLENEIUToun to wonerllname hing me the borgou wimt he ri \n",
            "\n",
            " 58% 29/50 [00:27<00:20,  1.04it/s][0m 28s (30 60%) 2.2695]\n",
            "Whe.\n",
            "\n",
            "MINIS:\n",
            "Kere hid wing thithe:\n",
            "Thor be, Cotrer wlecen sont hean,\n",
            "Hing wis I hat in.\n",
            "\n",
            "INy. I, food  \n",
            "\n",
            " 78% 39/50 [00:37<00:11,  1.03s/it][0m 38s (40 80%) 2.1421]\n",
            "Whee lis:\n",
            "I hay.\n",
            "\n",
            "GE:\n",
            "Cethe ik a ous arind eet, whe gortart the this\n",
            "and buleast, caiul mors arstirded \n",
            "\n",
            " 98% 49/50 [00:46<00:00,  1.09it/s][0m 48s (50 100%) 2.0778]\n",
            "Whif do the ousters.\n",
            "\n",
            "LAKRI: I prevers bade the is;\n",
            "And my seest a\n",
            "frod leat,\n",
            "And my corgul ion me.\n",
            "\n",
            "C \n",
            "\n",
            "100% 50/50 [00:48<00:00,  1.04it/s]\n",
            "Saving...\n",
            "Saved as input.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python generate.py input.pt --prime_str \"The \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdyShBtDnbDo",
        "outputId": "7caa5c20-47da-4edf-de3d-b3415e2e41f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/generate.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder = torch.load(args.filename)\n",
            "The wite be to dis reseish gof shey's sise the more, Haster londe surth:\n",
            "The ileth got seece, bord mer j\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 500 Epochs - PyTorch RNN\n"
      ],
      "metadata": {
        "id": "l-x6ezxGnkfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py input.txt --n_epochs 500 --print_every 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQPcHbH0ng9_",
        "outputId": "799e7322-9f0b-4377-f01f-8e4fe9612b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 500 epochs...\n",
            " 20% 99/500 [01:32<05:55,  1.13it/s][1m 33s (100 20%) 1.7423]\n",
            "Whered and whent the hear the with we stite,\n",
            "Mince with your her? and the camoth all in the of the be  \n",
            "\n",
            " 40% 199/500 [03:05<04:49,  1.04it/s][3m 6s (200 40%) 1.5728]\n",
            "Which friends are it creder'd.\n",
            "\n",
            "LUCIO:\n",
            "Let thee with-mendless see frought have with the war,\n",
            "And me th \n",
            "\n",
            " 60% 299/500 [04:38<02:56,  1.14it/s][4m 38s (300 60%) 1.4992]\n",
            "What he made is pointly power\n",
            "The wink, good sail to to us have make\n",
            "Supples thee me to by brother wha \n",
            "\n",
            " 80% 399/500 [06:13<01:35,  1.05it/s][6m 14s (400 80%) 1.4602]\n",
            "Which news\n",
            "What severes her far Richard is not judst and done,\n",
            "Thou wasting so a pity the appresence\n",
            "A \n",
            "\n",
            "100% 499/500 [07:47<00:00,  1.12it/s][7m 48s (500 100%) 1.4740]\n",
            "What she has much of me,\n",
            "With 'twas nest others:\n",
            "I am such here outched and them for the duke,\n",
            "Whose d \n",
            "\n",
            "100% 500/500 [07:48<00:00,  1.07it/s]\n",
            "Saving...\n",
            "Saved as input.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python generate.py input.pt --prime_str \"The \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ss7q27io7D5",
        "outputId": "5d74879c-03c2-4e03-dbe5-8fad4b0fae5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/generate.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder = torch.load(args.filename)\n",
            "The deaths to all; we will please of my fain?\n",
            "It do thy save your break him madous:\n",
            "The present of they \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a very clear difference between 5, 50, and 500 epochs. The 5 epoch model is complete gibberish and the characters are barely grouped into words of reasonable lengths. At 50 epochs, we get a few real words and gibberish that almost looks like it's in the shape of a sentence. At 500 epochs we finally get a readable sentence with actual words and even well placed punctuation. For the first line and a half there's even some meter, and the first line is basically in iambic pentameter, which is very impressive.\n",
        "\n",
        "Interestingly enough, the PyTorch implementation seems worse than the original Torch code. As 5 epochs in the original was as good if not better than the PyTorch implementation's 50 epochs. Although because it ran significantly faster, the PyTorch 500 results were actually possible to achieve and had the best generated output."
      ],
      "metadata": {
        "id": "RVsQieheo6uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Epochs - PyTorch LSTM\n"
      ],
      "metadata": {
        "id": "f9B_32vsr0Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py input.txt --model lstm --n_epochs 5 --print_every 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr0gNCixrV9I",
        "outputId": "5c86d54a-36ab-4509-89fd-39d551b0d9be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 5 epochs...\n",
            "  0% 0/5 [00:00<?, ?it/s][0m 2s (1 20%) 4.6234]\n",
            "Wh,Kj3\u000b'd#]:Z)]\n",
            "[b\\y{=RbcpIu?:k\fwtWy8#NbWf|u\f%tC|kkvsy2uMo\fxzt >-N ~VYi\tM2 \t[fvkV$z\"Hmg \n",
            "\n",
            " 20% 1/5 [00:02<00:09,  2.26s/it][0m 3s (2 40%) 4.4013]\n",
            "Wh1ds`ftad4exeb \n",
            "c  }\n",
            "et=tr0: &'u   SOetyqeTDsgevcel RA ,--ancLp\n",
            "Fadhcrenb~ ru$-reti rst*cR%wT1 ;l s6O \n",
            "\n",
            " 40% 2/5 [00:04<00:05,  1.96s/it][0m 5s (3 60%) 3.7237]\n",
            "Wh`PH2adaetthBuoo\n",
            "nsyl gfhno octbzlrr oettqCdsh an ^dtotstwolm  te yptFrr uda,osl e  l a l oy s _c sme \n",
            "\n",
            " 60% 3/5 [00:05<00:03,  1.78s/it][0m 7s (4 80%) 3.4377]\n",
            "WhM   l  er raee P e aoa onoerr etdre ,e\n",
            "hduim   t ee tatt , ar \n",
            "  \n",
            "fs o=dttsseo  sar e  e,ttna i da n \n",
            "\n",
            " 80% 4/5 [00:07<00:01,  1.81s/it][0m 9s (5 100%) 3.3964]\n",
            "WhMu n u e n suhls  ei retnr\n",
            "rn oee: eoeyrfa p oesetho  ot\n",
            "etofNnmeai no  heo aio ro aa toue.ec  o tau \n",
            "\n",
            "100% 5/5 [00:09<00:00,  1.83s/it]\n",
            "Saving...\n",
            "Saved as input.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python generate.py input.pt --prime_str \"The \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTF2XSIyrTsc",
        "outputId": "adc68a04-ce81-46f0-ef3e-c7c7c05117fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/generate.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder = torch.load(args.filename)\n",
            "The hsf r  suera hinn ogedoues errreu luofsi ehr oioabr rr  tlre ee  f  o 6i   e o tgursohtto nreueeed t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 50 Epochs - PyTorch LSTM"
      ],
      "metadata": {
        "id": "DQoRuQ1isTvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py input.txt --model lstm --n_epochs 50 --print_every 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmhiT4uIrToK",
        "outputId": "d74a66c4-5513-4efb-c522-086a487b63d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50 epochs...\n",
            " 18% 9/50 [00:12<00:53,  1.30s/it][0m 13s (10 20%) 3.2389]\n",
            "Whrs h Hln nhy Tsviem\n",
            "a oeA Lytn  hf.e edidndace bIylII rlibar  tSe iiNkuoI,rcNetnc eeiOEk e oiAehh o  \n",
            "\n",
            " 38% 19/50 [00:26<00:39,  1.28s/it][0m 27s (20 40%) 2.8496]\n",
            "Whi ohoe bhned nDYte erereerce hmen raee so.,s hmes hioe has nor eatdlI.Wmid detatl ain oa ttiTn\n",
            "teee  \n",
            "\n",
            " 58% 29/50 [00:39<00:26,  1.27s/it][0m 41s (30 60%) 2.5417]\n",
            "Whle, ses seldid wre'nle,\n",
            "Nor, waind wet wou, tealg shafeld Ailange btehes, kanes sasr me sell ankh fo \n",
            "\n",
            " 78% 39/50 [00:52<00:13,  1.26s/it][0m 54s (40 80%) 2.3595]\n",
            "Whar ins.\n",
            "\n",
            "GLROCRTEHENIN YTIRRIID:\n",
            "The vemet, os rove dow whend duved moweme thou soo sis be hean so:\n",
            " \n",
            "\n",
            " 98% 49/50 [01:06<00:01,  1.42s/it][1m 8s (50 100%) 2.2429]\n",
            "Whee,\n",
            "Thas mavition then Tice, the in sot eras by of the the theatig ceid harle Borden thad. Lecoss a  \n",
            "\n",
            "100% 50/50 [01:08<00:00,  1.37s/it]\n",
            "Saving...\n",
            "Saved as input.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python generate.py input.pt --prime_str \"The \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PSAYA0IrTkL",
        "outputId": "e7b99b62-4dbd-4c5d-d782-c2cd13377c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/generate.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder = torch.load(args.filename)\n",
            "The thas whis the cirs me panges as best my vathat; thor yoe! nour the pout gomins leon, srest bat welle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 500 Epochs - PyTorch LSTM"
      ],
      "metadata": {
        "id": "M8ToEI3PsWB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py input.txt --model lstm --n_epochs 500 --print_every 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQe_RBZ0rTcV",
        "outputId": "d5d0537f-367f-47c8-beb8-08544d580f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 500 epochs...\n",
            " 20% 99/500 [02:12<08:46,  1.31s/it][2m 13s (100 20%) 1.9349]\n",
            "Whe horge.\n",
            "\n",
            "Listrith thy kisted the fave and rown heeserd row biin not ston\n",
            "That him the treave to see \n",
            "\n",
            " 40% 199/500 [04:23<07:07,  1.42s/it][4m 24s (200 40%) 1.6748]\n",
            "What chey axe they-reser\n",
            "Druth quetious him fortharate to the jown use.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "This, who, \n",
            "\n",
            " 60% 299/500 [06:34<04:10,  1.25s/it][6m 35s (300 60%) 1.5741]\n",
            "What, I go sting him thee on the death marry?\n",
            "\n",
            "CORIOLANUS:\n",
            "That day you have I have moither,\n",
            "The mine  \n",
            "\n",
            " 80% 399/500 [08:46<02:14,  1.33s/it][8m 47s (400 80%) 1.5328]\n",
            "Who stand of that as the perciced out me thee\n",
            "Couse fuelad whom in this winding let him\n",
            "As good merk t \n",
            "\n",
            "100% 499/500 [10:57<00:01,  1.23s/it][10m 58s (500 100%) 1.4894]\n",
            "When now that be;\n",
            "And say that which all go to men,\n",
            "To heavy news would not another'd\n",
            "A come and reven \n",
            "\n",
            "100% 500/500 [10:58<00:00,  1.32s/it]\n",
            "Saving...\n",
            "Saved as input.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python generate.py input.pt --prime_str \"The \""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONr9UTKjvxjZ",
        "outputId": "fc0f23df-cc6d-49ac-b3a1-d4f756bbd250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/generate.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder = torch.load(args.filename)\n",
            "The world brother good men excarents,\n",
            "That which another with a same of honour shall\n",
            "To-morrowly shall b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the Long Short-Term Memory model is more complex than the original Recurrent Neural Network, it's not clear that the LSTM performs much better than the RNN. Both have nearly coherent sentences at 500 epochs, and are using real (if Shakespearean) words. One difference is that the LSTM did not seem to have picked up on meter. It is difficult to point out clear differences based on the ouput, but here is a comparison:\n",
        "\n",
        "\n",
        "**The Pytorch RNN at 50:**\n",
        "> The wite be to dis reseish gof shey's sise the more, Haster londe surth:\n",
        "The ileth got seece, bord mer j\n",
        "\n",
        "**The Pytorch LSTM at 50:**\n",
        "> The thas whis the cirs me panges as best my vathat; thor yoe! nour the pout gomins leon, srest bat welle\n",
        "\n",
        "**The Pytorch RNN at 500:**\n",
        "> The deaths to all; we will please of my fain?\n",
        ">\n",
        "> It do thy save your break him madous:\n",
        ">\n",
        "> The present of they\n",
        "\n",
        "**The PyTorch LSTM at 500**\n",
        "> The world brother good men excarents,\n",
        ">\n",
        "> That which another with a same of honour shall\n",
        ">\n",
        "> To-morrowly shall b"
      ],
      "metadata": {
        "id": "o4uOiILLtdx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "\n",
        "I have decided to use a Superstore Sales dataset from Kaggle which contains 4 years of sales information. The problem is to predict what customers will be buying in the future and from where. I need to use a sequential model so that it can analyze the purchases over time using the timestamp data.\n",
        "https://www.kaggle.com/datasets/rohitsahoo/sales-forecasting"
      ],
      "metadata": {
        "id": "rzkOBwuRw8sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import tensorflow as tf\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import scipy.stats as stats\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, LSTM, GRU\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import sklearn\n",
        "import xgboost\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "31XqT_Wbz7Gq"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "!pip install kagglehub[pandas-datasets]\n",
        "\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "file_path = \"train.csv\"\n",
        "\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"rohitsahoo/sales-forecasting\",\n",
        "  file_path,\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I89UwznR07md",
        "outputId": "16bc2ba1-0c37-4cdb-ec7e-cb490db19beb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.11/dist-packages (0.3.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.9), please consider upgrading to the latest version (0.3.10).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rohitsahoo/sales-forecasting?dataset_version_number=2&file_name=train.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 480k/480k [00:00<00:00, 775kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting zip of train.csv...\n",
            "First 5 records:    Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
            "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
            "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
            "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
            "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
            "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
            "\n",
            "     Customer Name    Segment        Country             City       State  \\\n",
            "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
            "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
            "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
            "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
            "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
            "\n",
            "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
            "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
            "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
            "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
            "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
            "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
            "\n",
            "                                        Product Name     Sales  \n",
            "0                  Bush Somerset Collection Bookcase  261.9600  \n",
            "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
            "2  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
            "3      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
            "4                     Eldon Fold 'N Roll Cart System   22.3680  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get a sense of whats in the data\n",
        "df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "URjOGKYQIg8O",
        "outputId": "fab7127f-f373-4a98-8119-88685cba5863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Row ID        Order ID  Order Date   Ship Date       Ship Mode  \\\n",
              "8161    8162  CA-2016-120915  28/09/2016  03/10/2016    Second Class   \n",
              "5227    5228  CA-2017-119865  21/06/2017  26/06/2017  Standard Class   \n",
              "6886    6887  CA-2018-123036  10/09/2018  17/09/2018  Standard Class   \n",
              "2973    2974  CA-2018-111808  16/12/2018  20/12/2018  Standard Class   \n",
              "4915    4916  CA-2016-161830  24/09/2016  26/09/2016    Second Class   \n",
              "\n",
              "     Customer ID       Customer Name   Segment        Country           City  \\\n",
              "8161    JJ-15445    Jennifer Jackson  Consumer  United States  New York City   \n",
              "5227    AS-10090  Adam Shillingsburg  Consumer  United States  New York City   \n",
              "6886    HA-14905       Helen Abelman  Consumer  United States    Springfield   \n",
              "2973    AR-10510      Andrew Roberts  Consumer  United States          Tulsa   \n",
              "4915    ME-17725           Max Engle  Consumer  United States        Seattle   \n",
              "\n",
              "           State  Postal Code   Region       Product ID         Category  \\\n",
              "8161    New York      10035.0     East  OFF-AP-10002578  Office Supplies   \n",
              "5227    New York      10011.0     East  FUR-BO-10003272        Furniture   \n",
              "6886        Ohio      45503.0     East  TEC-PH-10003580       Technology   \n",
              "2973    Oklahoma      74133.0  Central  OFF-BI-10004656  Office Supplies   \n",
              "4915  Washington      98105.0     West  OFF-BI-10001097  Office Supplies   \n",
              "\n",
              "     Sub-Category                                       Product Name    Sales  \n",
              "8161   Appliances  Fellowes Premier Superior Surge Suppressor, 10...  293.520  \n",
              "5227    Bookcases     O'Sullivan Living Dimensions 5-Shelf Bookcases  353.568  \n",
              "6886       Phones                 Cisco IP Phone 7961G-GE VoIP phone  259.896  \n",
              "2973      Binders                 Peel & Stick Add-On Corner Pockets   10.800  \n",
              "4915      Binders                          Avery Hole Reinforcements   14.952  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24767a97-81a8-4cc9-9ccc-9f6d5d029514\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row ID</th>\n",
              "      <th>Order ID</th>\n",
              "      <th>Order Date</th>\n",
              "      <th>Ship Date</th>\n",
              "      <th>Ship Mode</th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>Segment</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Postal Code</th>\n",
              "      <th>Region</th>\n",
              "      <th>Product ID</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sub-Category</th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8161</th>\n",
              "      <td>8162</td>\n",
              "      <td>CA-2016-120915</td>\n",
              "      <td>28/09/2016</td>\n",
              "      <td>03/10/2016</td>\n",
              "      <td>Second Class</td>\n",
              "      <td>JJ-15445</td>\n",
              "      <td>Jennifer Jackson</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>New York City</td>\n",
              "      <td>New York</td>\n",
              "      <td>10035.0</td>\n",
              "      <td>East</td>\n",
              "      <td>OFF-AP-10002578</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Appliances</td>\n",
              "      <td>Fellowes Premier Superior Surge Suppressor, 10...</td>\n",
              "      <td>293.520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5227</th>\n",
              "      <td>5228</td>\n",
              "      <td>CA-2017-119865</td>\n",
              "      <td>21/06/2017</td>\n",
              "      <td>26/06/2017</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>AS-10090</td>\n",
              "      <td>Adam Shillingsburg</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>New York City</td>\n",
              "      <td>New York</td>\n",
              "      <td>10011.0</td>\n",
              "      <td>East</td>\n",
              "      <td>FUR-BO-10003272</td>\n",
              "      <td>Furniture</td>\n",
              "      <td>Bookcases</td>\n",
              "      <td>O'Sullivan Living Dimensions 5-Shelf Bookcases</td>\n",
              "      <td>353.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6886</th>\n",
              "      <td>6887</td>\n",
              "      <td>CA-2018-123036</td>\n",
              "      <td>10/09/2018</td>\n",
              "      <td>17/09/2018</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>HA-14905</td>\n",
              "      <td>Helen Abelman</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Springfield</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>45503.0</td>\n",
              "      <td>East</td>\n",
              "      <td>TEC-PH-10003580</td>\n",
              "      <td>Technology</td>\n",
              "      <td>Phones</td>\n",
              "      <td>Cisco IP Phone 7961G-GE VoIP phone</td>\n",
              "      <td>259.896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2973</th>\n",
              "      <td>2974</td>\n",
              "      <td>CA-2018-111808</td>\n",
              "      <td>16/12/2018</td>\n",
              "      <td>20/12/2018</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>AR-10510</td>\n",
              "      <td>Andrew Roberts</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Tulsa</td>\n",
              "      <td>Oklahoma</td>\n",
              "      <td>74133.0</td>\n",
              "      <td>Central</td>\n",
              "      <td>OFF-BI-10004656</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Binders</td>\n",
              "      <td>Peel &amp; Stick Add-On Corner Pockets</td>\n",
              "      <td>10.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4915</th>\n",
              "      <td>4916</td>\n",
              "      <td>CA-2016-161830</td>\n",
              "      <td>24/09/2016</td>\n",
              "      <td>26/09/2016</td>\n",
              "      <td>Second Class</td>\n",
              "      <td>ME-17725</td>\n",
              "      <td>Max Engle</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Seattle</td>\n",
              "      <td>Washington</td>\n",
              "      <td>98105.0</td>\n",
              "      <td>West</td>\n",
              "      <td>OFF-BI-10001097</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Binders</td>\n",
              "      <td>Avery Hole Reinforcements</td>\n",
              "      <td>14.952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24767a97-81a8-4cc9-9ccc-9f6d5d029514')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24767a97-81a8-4cc9-9ccc-9f6d5d029514 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24767a97-81a8-4cc9-9ccc-9f6d5d029514');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1b627237-7923-4988-9587-287173bcd375\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1b627237-7923-4988-9587-287173bcd375')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1b627237-7923-4988-9587-287173bcd375 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix3HKEWYJZ6X",
        "outputId": "dbd3c02a-ff2d-4d93-f905-7f7c937ea754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9800, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "YBHr6sd5Ji1g"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "CYEcMC3_JmNH",
        "outputId": "2c3c27c4-5d49-4dd7-9cfa-703587447805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row ID           0\n",
              "Order ID         0\n",
              "Order Date       0\n",
              "Ship Date        0\n",
              "Ship Mode        0\n",
              "Customer ID      0\n",
              "Customer Name    0\n",
              "Segment          0\n",
              "Country          0\n",
              "City             0\n",
              "State            0\n",
              "Postal Code      0\n",
              "Region           0\n",
              "Product ID       0\n",
              "Category         0\n",
              "Sub-Category     0\n",
              "Product Name     0\n",
              "Sales            0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Row ID</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Order ID</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Order Date</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ship Date</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ship Mode</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Customer ID</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Customer Name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Segment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>City</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>State</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Postal Code</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Region</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Product ID</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Category</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sub-Category</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Product Name</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sales</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  print(col)\n",
        "  print(df[col].unique()[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMAkQHbxJ0HN",
        "outputId": "4c12bf1a-45d0-41e2-aeb3-8b280a1541ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row ID\n",
            "[ 1  2  3  4  5  6  7  8  9 10]\n",
            "Order ID\n",
            "['CA-2017-152156' 'CA-2017-138688' 'US-2016-108966' 'CA-2015-115812'\n",
            " 'CA-2018-114412' 'CA-2017-161389' 'US-2016-118983' 'CA-2015-105893'\n",
            " 'CA-2015-167164' 'CA-2015-143336']\n",
            "Order Date\n",
            "['08/11/2017' '12/06/2017' '11/10/2016' '09/06/2015' '15/04/2018'\n",
            " '05/12/2017' '22/11/2016' '11/11/2015' '13/05/2015' '27/08/2015']\n",
            "Ship Date\n",
            "['11/11/2017' '16/06/2017' '18/10/2016' '14/06/2015' '20/04/2018'\n",
            " '10/12/2017' '26/11/2016' '18/11/2015' '15/05/2015' '01/09/2015']\n",
            "Ship Mode\n",
            "['Second Class' 'Standard Class' 'First Class' 'Same Day']\n",
            "Customer ID\n",
            "['CG-12520' 'DV-13045' 'SO-20335' 'BH-11710' 'AA-10480' 'IM-15070'\n",
            " 'HP-14815' 'PK-19075' 'AG-10270' 'ZD-21925']\n",
            "Customer Name\n",
            "['Claire Gute' 'Darrin Van Huff' \"Sean O'Donnell\" 'Brosina Hoffman'\n",
            " 'Andrew Allen' 'Irene Maddox' 'Harold Pawlan' 'Pete Kriz'\n",
            " 'Alejandro Grove' 'Zuschuss Donatelli']\n",
            "Segment\n",
            "['Consumer' 'Corporate' 'Home Office']\n",
            "Country\n",
            "['United States']\n",
            "City\n",
            "['Henderson' 'Los Angeles' 'Fort Lauderdale' 'Concord' 'Seattle'\n",
            " 'Fort Worth' 'Madison' 'West Jordan' 'San Francisco' 'Fremont']\n",
            "State\n",
            "['Kentucky' 'California' 'Florida' 'North Carolina' 'Washington' 'Texas'\n",
            " 'Wisconsin' 'Utah' 'Nebraska' 'Pennsylvania']\n",
            "Postal Code\n",
            "[42420. 90036. 33311. 90032. 28027. 98103. 76106. 53711. 84084. 94109.]\n",
            "Region\n",
            "['South' 'West' 'Central' 'East']\n",
            "Product ID\n",
            "['FUR-BO-10001798' 'FUR-CH-10000454' 'OFF-LA-10000240' 'FUR-TA-10000577'\n",
            " 'OFF-ST-10000760' 'FUR-FU-10001487' 'OFF-AR-10002833' 'TEC-PH-10002275'\n",
            " 'OFF-BI-10003910' 'OFF-AP-10002892']\n",
            "Category\n",
            "['Furniture' 'Office Supplies' 'Technology']\n",
            "Sub-Category\n",
            "['Bookcases' 'Chairs' 'Labels' 'Tables' 'Storage' 'Furnishings' 'Art'\n",
            " 'Phones' 'Binders' 'Appliances']\n",
            "Product Name\n",
            "['Bush Somerset Collection Bookcase'\n",
            " 'Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back'\n",
            " 'Self-Adhesive Address Labels for Typewriters by Universal'\n",
            " 'Bretford CR4500 Series Slim Rectangular Table'\n",
            " \"Eldon Fold 'N Roll Cart System\"\n",
            " 'Eldon Expressions Wood and Plastic Desk Accessories, Cherry Wood'\n",
            " 'Newell 322' 'Mitel 5320 IP Phone VoIP phone'\n",
            " 'DXL Angle-View Binders with Locking Rings by Samsill'\n",
            " 'Belkin F5C206VTEL 6 Outlet Surge']\n",
            "Sales\n",
            "[261.96   731.94    14.62   957.5775  22.368   48.86     7.28   907.152\n",
            "  18.504  114.9   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=['O'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "WADFwZAFLNii",
        "outputId": "25628591-7535-4ee7-e79e-eec6e8ed04f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
              "count             9789        9789        9789            9789        9789   \n",
              "unique            4916        1229        1326               4         793   \n",
              "top     CA-2018-100111  05/09/2017  26/09/2018  Standard Class    WB-21850   \n",
              "freq                14          38          34            5849          35   \n",
              "\n",
              "        Customer Name   Segment        Country           City       State  \\\n",
              "count            9789      9789           9789           9789        9789   \n",
              "unique            793         3              1            529          48   \n",
              "top     William Brown  Consumer  United States  New York City  California   \n",
              "freq               35      5096           9789            891        1946   \n",
              "\n",
              "       Region       Product ID         Category Sub-Category     Product Name  \n",
              "count    9789             9789             9789         9789             9789  \n",
              "unique      4             1860                3           17             1848  \n",
              "top      West  OFF-PA-10001970  Office Supplies      Binders  Staple envelope  \n",
              "freq     3140               18             5903         1492               47  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5076e80e-22d6-4e9f-8454-25d8613c8749\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Order ID</th>\n",
              "      <th>Order Date</th>\n",
              "      <th>Ship Date</th>\n",
              "      <th>Ship Mode</th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>Segment</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Region</th>\n",
              "      <th>Product ID</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sub-Category</th>\n",
              "      <th>Product Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "      <td>9789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>4916</td>\n",
              "      <td>1229</td>\n",
              "      <td>1326</td>\n",
              "      <td>4</td>\n",
              "      <td>793</td>\n",
              "      <td>793</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>529</td>\n",
              "      <td>48</td>\n",
              "      <td>4</td>\n",
              "      <td>1860</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>1848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>CA-2018-100111</td>\n",
              "      <td>05/09/2017</td>\n",
              "      <td>26/09/2018</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>WB-21850</td>\n",
              "      <td>William Brown</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>New York City</td>\n",
              "      <td>California</td>\n",
              "      <td>West</td>\n",
              "      <td>OFF-PA-10001970</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Binders</td>\n",
              "      <td>Staple envelope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>14</td>\n",
              "      <td>38</td>\n",
              "      <td>34</td>\n",
              "      <td>5849</td>\n",
              "      <td>35</td>\n",
              "      <td>35</td>\n",
              "      <td>5096</td>\n",
              "      <td>9789</td>\n",
              "      <td>891</td>\n",
              "      <td>1946</td>\n",
              "      <td>3140</td>\n",
              "      <td>18</td>\n",
              "      <td>5903</td>\n",
              "      <td>1492</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5076e80e-22d6-4e9f-8454-25d8613c8749')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5076e80e-22d6-4e9f-8454-25d8613c8749 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5076e80e-22d6-4e9f-8454-25d8613c8749');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b7fbeb6e-35f0-498a-b5f1-6904682590c8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7fbeb6e-35f0-498a-b5f1-6904682590c8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b7fbeb6e-35f0-498a-b5f1-6904682590c8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Order ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4916,\n          \"14\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Order Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-01 00:00:00.000000038\",\n        \"max\": \"2017-05-09 00:00:00\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1229,\n          \"38\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ship Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-01 00:00:00.000000034\",\n        \"max\": \"2018-09-26 00:00:00\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1326,\n          \"34\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ship Mode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4,\n          \"5849\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Customer ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          793,\n          \"35\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Customer Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          793,\n          \"35\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Segment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          \"5096\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Country\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"9789\",\n          1,\n          \"United States\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"City\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          529,\n          \"891\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"State\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          48,\n          \"1946\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Region\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4,\n          \"3140\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1860,\n          \"18\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          \"5903\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sub-Category\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          17,\n          \"1492\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1848,\n          \"47\",\n          \"9789\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['order_date'] = pd.to_datetime(df['Order Date'], format='%d/%m/%Y').dt.strftime('%m/%d/%Y')"
      ],
      "metadata": {
        "id": "u_LpV-JpMRiL"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['ship_date'] = pd.to_datetime(df['Ship Date'], format='%d/%m/%Y').dt.strftime('%m/%d/%Y')"
      ],
      "metadata": {
        "id": "pv3ggy1nMc10"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Order Date2'] = pd.to_datetime(df['order_date'])\n",
        "df.set_index('Order Date2', inplace=True)\n",
        "\n",
        "# sales per month\n",
        "sales_by_month = df.resample('M').size()\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "sales_by_month.plot(kind='bar')\n",
        "plt.xlabel('Order Month')\n",
        "plt.ylabel('Number of Sales')\n",
        "plt.title('Monthly Sales')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "FmMwSOBeNYkG",
        "outputId": "2fbf48db-1b62-493d-843b-1cd2833b6206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HApQQd7bPqIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = df.drop(columns=['Order Date', 'Ship Date'])\n",
        "clean_df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "eL8EYpIlN-Sd",
        "outputId": "8349c0ec-93d2-4852-98e4-bf30333eede9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Row ID        Order ID       Ship Mode Customer ID  Customer Name  \\\n",
              "3254    3255  CA-2017-155474  Standard Class    CC-12220   Chris Cortes   \n",
              "4818    4819  CA-2015-139598  Standard Class    MG-17695  Maureen Gnade   \n",
              "9532    9533  CA-2017-116596  Standard Class    BW-11200    Ben Wallace   \n",
              "983      984  CA-2015-163419    Second Class    TZ-21580      Tracy Zic   \n",
              "627      628  CA-2018-153787  Standard Class    AT-10735  Annie Thurman   \n",
              "\n",
              "       Segment        Country           City         State  Postal Code  \\\n",
              "3254  Consumer  United States        Seattle    Washington      98105.0   \n",
              "4818  Consumer  United States   Philadelphia  Pennsylvania      19134.0   \n",
              "9532  Consumer  United States  New York City      New York      10011.0   \n",
              "983   Consumer  United States     Louisville      Colorado      80027.0   \n",
              "627   Consumer  United States        Seattle    Washington      98115.0   \n",
              "\n",
              "     Region       Product ID         Category Sub-Category  \\\n",
              "3254   West  TEC-PH-10001580       Technology       Phones   \n",
              "4818   East  OFF-ST-10001370  Office Supplies      Storage   \n",
              "9532   East  FUR-CH-10000553        Furniture       Chairs   \n",
              "983    West  FUR-CH-10000665        Furniture       Chairs   \n",
              "627    West  OFF-AP-10001563  Office Supplies   Appliances   \n",
              "\n",
              "                                           Product Name    Sales  order_date  \\\n",
              "3254  Logitech Mobile Speakerphone P710e -speaker p...  107.984  07/08/2017   \n",
              "4818          Sensible Storage WireTech Storage Systems  227.136  12/26/2015   \n",
              "9532              Metal Folding Chairs, Beige, 4/Carton  427.644  10/27/2017   \n",
              "983       Global Airflow Leather Mesh Back Chair, Black  603.920  11/11/2015   \n",
              "627   Belkin Premiere Surge Master II 8-outlet surge...   97.160  05/19/2018   \n",
              "\n",
              "       ship_date  \n",
              "3254  07/14/2017  \n",
              "4818  12/31/2015  \n",
              "9532  10/31/2017  \n",
              "983   11/14/2015  \n",
              "627   05/23/2018  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e4bcf192-bee4-4257-92ef-1079201f2511\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row ID</th>\n",
              "      <th>Order ID</th>\n",
              "      <th>Ship Mode</th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>Segment</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Postal Code</th>\n",
              "      <th>Region</th>\n",
              "      <th>Product ID</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sub-Category</th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Sales</th>\n",
              "      <th>order_date</th>\n",
              "      <th>ship_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3254</th>\n",
              "      <td>3255</td>\n",
              "      <td>CA-2017-155474</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>CC-12220</td>\n",
              "      <td>Chris Cortes</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Seattle</td>\n",
              "      <td>Washington</td>\n",
              "      <td>98105.0</td>\n",
              "      <td>West</td>\n",
              "      <td>TEC-PH-10001580</td>\n",
              "      <td>Technology</td>\n",
              "      <td>Phones</td>\n",
              "      <td>Logitech Mobile Speakerphone P710e -speaker p...</td>\n",
              "      <td>107.984</td>\n",
              "      <td>07/08/2017</td>\n",
              "      <td>07/14/2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4818</th>\n",
              "      <td>4819</td>\n",
              "      <td>CA-2015-139598</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>MG-17695</td>\n",
              "      <td>Maureen Gnade</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Philadelphia</td>\n",
              "      <td>Pennsylvania</td>\n",
              "      <td>19134.0</td>\n",
              "      <td>East</td>\n",
              "      <td>OFF-ST-10001370</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Storage</td>\n",
              "      <td>Sensible Storage WireTech Storage Systems</td>\n",
              "      <td>227.136</td>\n",
              "      <td>12/26/2015</td>\n",
              "      <td>12/31/2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9532</th>\n",
              "      <td>9533</td>\n",
              "      <td>CA-2017-116596</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>BW-11200</td>\n",
              "      <td>Ben Wallace</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>New York City</td>\n",
              "      <td>New York</td>\n",
              "      <td>10011.0</td>\n",
              "      <td>East</td>\n",
              "      <td>FUR-CH-10000553</td>\n",
              "      <td>Furniture</td>\n",
              "      <td>Chairs</td>\n",
              "      <td>Metal Folding Chairs, Beige, 4/Carton</td>\n",
              "      <td>427.644</td>\n",
              "      <td>10/27/2017</td>\n",
              "      <td>10/31/2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>984</td>\n",
              "      <td>CA-2015-163419</td>\n",
              "      <td>Second Class</td>\n",
              "      <td>TZ-21580</td>\n",
              "      <td>Tracy Zic</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Louisville</td>\n",
              "      <td>Colorado</td>\n",
              "      <td>80027.0</td>\n",
              "      <td>West</td>\n",
              "      <td>FUR-CH-10000665</td>\n",
              "      <td>Furniture</td>\n",
              "      <td>Chairs</td>\n",
              "      <td>Global Airflow Leather Mesh Back Chair, Black</td>\n",
              "      <td>603.920</td>\n",
              "      <td>11/11/2015</td>\n",
              "      <td>11/14/2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627</th>\n",
              "      <td>628</td>\n",
              "      <td>CA-2018-153787</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>AT-10735</td>\n",
              "      <td>Annie Thurman</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Seattle</td>\n",
              "      <td>Washington</td>\n",
              "      <td>98115.0</td>\n",
              "      <td>West</td>\n",
              "      <td>OFF-AP-10001563</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Appliances</td>\n",
              "      <td>Belkin Premiere Surge Master II 8-outlet surge...</td>\n",
              "      <td>97.160</td>\n",
              "      <td>05/19/2018</td>\n",
              "      <td>05/23/2018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4bcf192-bee4-4257-92ef-1079201f2511')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e4bcf192-bee4-4257-92ef-1079201f2511 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e4bcf192-bee4-4257-92ef-1079201f2511');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6ef20b62-256d-4aa1-9827-4844527c40ee\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6ef20b62-256d-4aa1-9827-4844527c40ee')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6ef20b62-256d-4aa1-9827-4844527c40ee button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_df = clean_df.copy()\n",
        "seq_df.sample(5)\n",
        "seq_df['order_date'] = pd.to_datetime(seq_df['order_date'], format='%m/%d/%Y')\n",
        "seq_df['order_date'] = seq_df['order_date'].astype(np.int64)"
      ],
      "metadata": {
        "id": "ftMLbAk4PX7N"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "0_mLnYCTQE73",
        "outputId": "f22ebfac-a207-40ed-8403-e71bff74a147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Row ID        Order ID       Ship Mode Customer ID  Customer Name  \\\n",
              "5900    5903  US-2018-153948        Same Day    FM-14290   Frank Merwin   \n",
              "1644    1645  CA-2016-111829     First Class    FH-14365   Fred Hopkins   \n",
              "7027    7030  CA-2017-145548  Standard Class    EB-13750  Edward Becker   \n",
              "784      785  US-2016-157014    Second Class    BM-11785    Bryan Mills   \n",
              "4403    4405  CA-2016-164336  Standard Class    MW-18220   Mitch Webber   \n",
              "\n",
              "          Segment        Country           City         State  Postal Code  \\\n",
              "5900  Home Office  United States  San Francisco    California      94122.0   \n",
              "1644    Corporate  United States        Seattle    Washington      98115.0   \n",
              "7027    Corporate  United States  New York City      New York      10011.0   \n",
              "784      Consumer  United States       Columbus          Ohio      43229.0   \n",
              "4403     Consumer  United States   Philadelphia  Pennsylvania      19140.0   \n",
              "\n",
              "     Region       Product ID         Category Sub-Category  \\\n",
              "5900   West  OFF-PA-10000157  Office Supplies        Paper   \n",
              "1644   West  TEC-CO-10001766       Technology      Copiers   \n",
              "7027   East  OFF-ST-10002562  Office Supplies      Storage   \n",
              "784    East  TEC-AC-10000057       Technology  Accessories   \n",
              "4403   East  TEC-AC-10002345       Technology  Accessories   \n",
              "\n",
              "                                   Product Name     Sales  \\\n",
              "5900                                  Xerox 191    59.940   \n",
              "1644                         Canon PC940 Copier  3149.930   \n",
              "7027                              Staple magnet    28.140   \n",
              "784   Microsoft Natural Ergonomic Keyboard 4000    47.984   \n",
              "4403          HP Standard 104 key PS/2 Keyboard    34.800   \n",
              "\n",
              "               order_date   ship_date  \n",
              "5900  1541462400000000000  11/06/2018  \n",
              "1644  1458345600000000000  03/20/2016  \n",
              "7027  1510358400000000000  11/16/2017  \n",
              "784   1475452800000000000  10/06/2016  \n",
              "4403  1467676800000000000  07/10/2016  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0dc3625f-c177-497c-9fd0-abb860d7e2a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row ID</th>\n",
              "      <th>Order ID</th>\n",
              "      <th>Ship Mode</th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>Segment</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Postal Code</th>\n",
              "      <th>Region</th>\n",
              "      <th>Product ID</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sub-Category</th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Sales</th>\n",
              "      <th>order_date</th>\n",
              "      <th>ship_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5900</th>\n",
              "      <td>5903</td>\n",
              "      <td>US-2018-153948</td>\n",
              "      <td>Same Day</td>\n",
              "      <td>FM-14290</td>\n",
              "      <td>Frank Merwin</td>\n",
              "      <td>Home Office</td>\n",
              "      <td>United States</td>\n",
              "      <td>San Francisco</td>\n",
              "      <td>California</td>\n",
              "      <td>94122.0</td>\n",
              "      <td>West</td>\n",
              "      <td>OFF-PA-10000157</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Paper</td>\n",
              "      <td>Xerox 191</td>\n",
              "      <td>59.940</td>\n",
              "      <td>1541462400000000000</td>\n",
              "      <td>11/06/2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1644</th>\n",
              "      <td>1645</td>\n",
              "      <td>CA-2016-111829</td>\n",
              "      <td>First Class</td>\n",
              "      <td>FH-14365</td>\n",
              "      <td>Fred Hopkins</td>\n",
              "      <td>Corporate</td>\n",
              "      <td>United States</td>\n",
              "      <td>Seattle</td>\n",
              "      <td>Washington</td>\n",
              "      <td>98115.0</td>\n",
              "      <td>West</td>\n",
              "      <td>TEC-CO-10001766</td>\n",
              "      <td>Technology</td>\n",
              "      <td>Copiers</td>\n",
              "      <td>Canon PC940 Copier</td>\n",
              "      <td>3149.930</td>\n",
              "      <td>1458345600000000000</td>\n",
              "      <td>03/20/2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7027</th>\n",
              "      <td>7030</td>\n",
              "      <td>CA-2017-145548</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>EB-13750</td>\n",
              "      <td>Edward Becker</td>\n",
              "      <td>Corporate</td>\n",
              "      <td>United States</td>\n",
              "      <td>New York City</td>\n",
              "      <td>New York</td>\n",
              "      <td>10011.0</td>\n",
              "      <td>East</td>\n",
              "      <td>OFF-ST-10002562</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Storage</td>\n",
              "      <td>Staple magnet</td>\n",
              "      <td>28.140</td>\n",
              "      <td>1510358400000000000</td>\n",
              "      <td>11/16/2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784</th>\n",
              "      <td>785</td>\n",
              "      <td>US-2016-157014</td>\n",
              "      <td>Second Class</td>\n",
              "      <td>BM-11785</td>\n",
              "      <td>Bryan Mills</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Columbus</td>\n",
              "      <td>Ohio</td>\n",
              "      <td>43229.0</td>\n",
              "      <td>East</td>\n",
              "      <td>TEC-AC-10000057</td>\n",
              "      <td>Technology</td>\n",
              "      <td>Accessories</td>\n",
              "      <td>Microsoft Natural Ergonomic Keyboard 4000</td>\n",
              "      <td>47.984</td>\n",
              "      <td>1475452800000000000</td>\n",
              "      <td>10/06/2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4403</th>\n",
              "      <td>4405</td>\n",
              "      <td>CA-2016-164336</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>MW-18220</td>\n",
              "      <td>Mitch Webber</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Philadelphia</td>\n",
              "      <td>Pennsylvania</td>\n",
              "      <td>19140.0</td>\n",
              "      <td>East</td>\n",
              "      <td>TEC-AC-10002345</td>\n",
              "      <td>Technology</td>\n",
              "      <td>Accessories</td>\n",
              "      <td>HP Standard 104 key PS/2 Keyboard</td>\n",
              "      <td>34.800</td>\n",
              "      <td>1467676800000000000</td>\n",
              "      <td>07/10/2016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0dc3625f-c177-497c-9fd0-abb860d7e2a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0dc3625f-c177-497c-9fd0-abb860d7e2a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0dc3625f-c177-497c-9fd0-abb860d7e2a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-98d36803-4c01-4fe5-aea2-ca118c6953d5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98d36803-4c01-4fe5-aea2-ca118c6953d5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-98d36803-4c01-4fe5-aea2-ca118c6953d5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Sub-Category'].unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEebXhhS5YDM",
        "outputId": "c9068e55-0daa-4df2-d4b9-958113c7c1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Bookcases' 'Chairs' 'Labels' 'Tables' 'Storage' 'Furnishings' 'Art'\n",
            " 'Phones' 'Binders' 'Appliances' 'Paper' 'Accessories' 'Envelopes'\n",
            " 'Fasteners' 'Supplies' 'Machines' 'Copiers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ship Mode\n",
        "ship_map = {'Same Day': 1, 'First Class': 2, 'Second Class': 3, 'Standard Class': 4}\n",
        "ship_reverse = {idx: value for value, idx in ship_map.items()}\n",
        "# Segment\n",
        "segment_map = {'Consumer': 1, 'Corporate': 2, 'Home Office': 3}\n",
        "segment_reverse = {idx: value for value, idx in segment_map.items()}\n",
        "# Region\n",
        "region_map = {'South': 1, 'West': 2, 'Central': 3, 'East': 4}\n",
        "region_reverse = {idx: value for value, idx in region_map.items()}\n",
        "# Category\n",
        "category_map = {'Furniture': 1, 'Office Supplies': 2, 'Technology': 3}\n",
        "category_reverse = {idx: value for value, idx in category_map.items()}\n",
        "# Sub-Category\n",
        "sub_category_map = {value: idx for idx, value in enumerate(df['Sub-Category'].unique())}\n",
        "sub_category_reverse = {idx: value for value, idx in sub_category_map.items() }\n",
        "# Customer ID\n",
        "customer_map = {value: idx for idx, value in enumerate(df['Customer ID'].unique())}\n",
        "customer_reverse = {idx: value for value, idx in customer_map.items()}\n",
        "# Country\n",
        "country_map = {value: idx for idx, value in enumerate(df['Country'].unique())}\n",
        "country_reverse = {idx: value for value, idx in country_map.items()}\n",
        "# City\n",
        "city_map = {value: idx for idx, value in enumerate(df['City'].unique())}\n",
        "city_reverse = {idx: value for value, idx in city_map.items()}\n",
        "# State\n",
        "state_map = {value: idx for idx, value in enumerate(df['State'].unique())}\n",
        "state_reverse = {idx: value for value, idx in state_map.items()}\n",
        "# Product ID\n",
        "product_map = {value: idx for idx, value in enumerate(df['Product ID'].unique())}\n",
        "product_reverse = {idx: value for value, idx in product_map.items()}"
      ],
      "metadata": {
        "id": "aMjea5J44jTC"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping customer name and product name bc we have customer id and product id, dropping order id because it isnt helpful\n",
        "num_df = seq_df.drop(columns=['Customer Name', 'Product Name', 'Order ID'])\n",
        "num_df['Ship Mode'] = num_df['Ship Mode'].map(ship_map)\n",
        "num_df['Segment'] = num_df['Segment'].map(segment_map)\n",
        "num_df['Region'] = num_df['Region'].map(region_map)\n",
        "num_df['Category'] = num_df['Category'].map(category_map)\n",
        "num_df['Sub-Category'] = num_df['Sub-Category'].map(sub_category_map)\n",
        "num_df['Customer ID'] = num_df['Customer ID'].map(customer_map)\n",
        "num_df['Country'] = num_df['Country'].map(country_map)\n",
        "num_df['City'] = num_df['City'].map(city_map)\n",
        "num_df['State'] = num_df['State'].map(state_map)\n",
        "num_df['Product ID'] = num_df['Product ID'].map(product_map)"
      ],
      "metadata": {
        "id": "wYFR5veVAH-Q"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "44vVFaUT__9f",
        "outputId": "b88bee8d-5e6a-4e6b-ff25-41573e31ad4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Row ID  Ship Mode  Customer ID  Segment  Country  City  State  \\\n",
              "8309    8312          4          214        1        0    38     32   \n",
              "2088    2089          4            5        1        0    20     15   \n",
              "8174    8177          2          449        3        0    20     15   \n",
              "1421    1422          4          459        2        0   123     16   \n",
              "5951    5954          2          737        2        0    20     15   \n",
              "\n",
              "      Postal Code  Region  Product ID  Category  Sub-Category    Sales  \\\n",
              "8309      31907.0       1         116         1             5   62.720   \n",
              "2088      10009.0       4        1239         2            13    3.960   \n",
              "8174      10035.0       4          80         2             4  128.340   \n",
              "1421      85204.0       2         983         2             9   60.672   \n",
              "5951      10009.0       4        1706         2             6   10.500   \n",
              "\n",
              "               order_date   ship_date  \n",
              "8309  1516924800000000000  01/31/2018  \n",
              "2088  1498176000000000000  06/28/2017  \n",
              "8174  1506038400000000000  09/25/2017  \n",
              "1421  1474848000000000000  09/30/2016  \n",
              "5951  1427673600000000000  03/31/2015  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eda03265-2228-4859-a512-3cf4ceadfdce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row ID</th>\n",
              "      <th>Ship Mode</th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Segment</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Postal Code</th>\n",
              "      <th>Region</th>\n",
              "      <th>Product ID</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sub-Category</th>\n",
              "      <th>Sales</th>\n",
              "      <th>order_date</th>\n",
              "      <th>ship_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8309</th>\n",
              "      <td>8312</td>\n",
              "      <td>4</td>\n",
              "      <td>214</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>32</td>\n",
              "      <td>31907.0</td>\n",
              "      <td>1</td>\n",
              "      <td>116</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>62.720</td>\n",
              "      <td>1516924800000000000</td>\n",
              "      <td>01/31/2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2088</th>\n",
              "      <td>2089</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>15</td>\n",
              "      <td>10009.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1239</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>3.960</td>\n",
              "      <td>1498176000000000000</td>\n",
              "      <td>06/28/2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8174</th>\n",
              "      <td>8177</td>\n",
              "      <td>2</td>\n",
              "      <td>449</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>15</td>\n",
              "      <td>10035.0</td>\n",
              "      <td>4</td>\n",
              "      <td>80</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>128.340</td>\n",
              "      <td>1506038400000000000</td>\n",
              "      <td>09/25/2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421</th>\n",
              "      <td>1422</td>\n",
              "      <td>4</td>\n",
              "      <td>459</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>16</td>\n",
              "      <td>85204.0</td>\n",
              "      <td>2</td>\n",
              "      <td>983</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>60.672</td>\n",
              "      <td>1474848000000000000</td>\n",
              "      <td>09/30/2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5951</th>\n",
              "      <td>5954</td>\n",
              "      <td>2</td>\n",
              "      <td>737</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>15</td>\n",
              "      <td>10009.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1706</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>10.500</td>\n",
              "      <td>1427673600000000000</td>\n",
              "      <td>03/31/2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eda03265-2228-4859-a512-3cf4ceadfdce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eda03265-2228-4859-a512-3cf4ceadfdce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eda03265-2228-4859-a512-3cf4ceadfdce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-843b7f4a-7e9a-4d74-8d9c-7533478be071\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-843b7f4a-7e9a-4d74-8d9c-7533478be071')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-843b7f4a-7e9a-4d74-8d9c-7533478be071 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN using TensorFlow"
      ],
      "metadata": {
        "id": "r-NfbvHHhfCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = num_df['order_date'].values.reshape(-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 10\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "\n",
        "# 80% train 20% test\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, activation='tanh', input_shape=(seq_length, 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=20, validation_split=0.2) # 80% 20% for validation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfgNmP_Kkk9E",
        "outputId": "c6368788-4f8f-4b2f-d10c-23a303dc8adb"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1717 - val_loss: 0.0653\n",
            "Epoch 2/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0643 - val_loss: 0.0638\n",
            "Epoch 3/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0615 - val_loss: 0.0649\n",
            "Epoch 4/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0623 - val_loss: 0.0643\n",
            "Epoch 5/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0643 - val_loss: 0.0629\n",
            "Epoch 6/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0616 - val_loss: 0.0619\n",
            "Epoch 7/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0624 - val_loss: 0.0623\n",
            "Epoch 8/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0644 - val_loss: 0.0625\n",
            "Epoch 9/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0620 - val_loss: 0.0637\n",
            "Epoch 10/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0620 - val_loss: 0.0618\n",
            "Epoch 11/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0626 - val_loss: 0.0628\n",
            "Epoch 12/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0638 - val_loss: 0.0617\n",
            "Epoch 13/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0613 - val_loss: 0.0644\n",
            "Epoch 14/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0631 - val_loss: 0.0632\n",
            "Epoch 15/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0631 - val_loss: 0.0619\n",
            "Epoch 16/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0619 - val_loss: 0.0622\n",
            "Epoch 17/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0624 - val_loss: 0.0620\n",
            "Epoch 18/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0621 - val_loss: 0.0620\n",
            "Epoch 19/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0617 - val_loss: 0.0687\n",
            "Epoch 20/20\n",
            "\u001b[1m196/196\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0614 - val_loss: 0.0651\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ed73e10d690>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root of Mean Squared Error: \", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rva6p4IZktBp",
        "outputId": "1765ff99-7a96-4beb-9b98-cf0976a6ddbf"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m62/62\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Root of Mean Squared Error:  0.25784471610415977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM using TensorFlow"
      ],
      "metadata": {
        "id": "FKmtdx_IhpMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = num_df['order_date'].values.reshape(-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 10\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "\n",
        "# 80% train 20% test\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='tanh', input_shape=(seq_length, 1))) # LSTM\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=20,validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvM44SS4fXDC",
        "outputId": "24203356-70b4-42bd-cf63-6485d380c5b4"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1005 - val_loss: 0.0698\n",
            "Epoch 2/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0640 - val_loss: 0.0691\n",
            "Epoch 3/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0631 - val_loss: 0.0627\n",
            "Epoch 4/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0604 - val_loss: 0.0626\n",
            "Epoch 5/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0625 - val_loss: 0.0633\n",
            "Epoch 6/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0603 - val_loss: 0.0623\n",
            "Epoch 7/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0623 - val_loss: 0.0637\n",
            "Epoch 8/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0622 - val_loss: 0.0644\n",
            "Epoch 9/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0616 - val_loss: 0.0625\n",
            "Epoch 10/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0619 - val_loss: 0.0625\n",
            "Epoch 11/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0619 - val_loss: 0.0628\n",
            "Epoch 12/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0599 - val_loss: 0.0624\n",
            "Epoch 13/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0613 - val_loss: 0.0624\n",
            "Epoch 14/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0612 - val_loss: 0.0625\n",
            "Epoch 15/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0631 - val_loss: 0.0628\n",
            "Epoch 16/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0643 - val_loss: 0.0625\n",
            "Epoch 17/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0611 - val_loss: 0.0626\n",
            "Epoch 18/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0594 - val_loss: 0.0625\n",
            "Epoch 19/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0632 - val_loss: 0.0625\n",
            "Epoch 20/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0626 - val_loss: 0.0625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ed7391d5c10>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root of Mean Squared Error: \", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-A5XRTKk-Ia",
        "outputId": "1c00c9b8-6b8c-4e6c-9724-4566c84b47d3"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m62/62\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Root of Mean Squared Error:  0.24993610651157186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GNU using TensorFlow"
      ],
      "metadata": {
        "id": "3jtBaeonhtZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = num_df['order_date'].values.reshape(-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 10\n",
        "X, y = create_sequences(data_scaled, seq_length)\n",
        "\n",
        "# 80% train 20% test\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(50, activation='tanh', input_shape=(seq_length, 1)))  # GRU layer\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=20, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GyAiPElhU_2",
        "outputId": "9f4d5677-8186-4712-94dd-b13ca33f7b14"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0953 - val_loss: 0.0640\n",
            "Epoch 2/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0604 - val_loss: 0.0633\n",
            "Epoch 3/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0604 - val_loss: 0.0623\n",
            "Epoch 4/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0635 - val_loss: 0.0624\n",
            "Epoch 5/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0609 - val_loss: 0.0633\n",
            "Epoch 6/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0628 - val_loss: 0.0635\n",
            "Epoch 7/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0636 - val_loss: 0.0624\n",
            "Epoch 8/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0611 - val_loss: 0.0630\n",
            "Epoch 9/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0630 - val_loss: 0.0629\n",
            "Epoch 10/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0607 - val_loss: 0.0626\n",
            "Epoch 11/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0622 - val_loss: 0.0624\n",
            "Epoch 12/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0601 - val_loss: 0.0624\n",
            "Epoch 13/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0620 - val_loss: 0.0627\n",
            "Epoch 14/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0620 - val_loss: 0.0633\n",
            "Epoch 15/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0623 - val_loss: 0.0624\n",
            "Epoch 16/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0607 - val_loss: 0.0628\n",
            "Epoch 17/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0617 - val_loss: 0.0626\n",
            "Epoch 18/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0613 - val_loss: 0.0626\n",
            "Epoch 19/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0614 - val_loss: 0.0627\n",
            "Epoch 20/20\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0623 - val_loss: 0.0634\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ed73c6ff9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root of Mean Squared Error: \", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCPYVDbRlOaX",
        "outputId": "33199ac8-ce5b-4787-a825-f848adda9cc9"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m62/62\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
            "Root of Mean Squared Error:  0.25170731477173164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using hyperbolic tangent for my activation function, each model has the Adam optimizer, and the metric I am using is Mean Squared Error as the Loss Function. I kept 20% of the original dataset for testing, then the 80% that went to training got further split as 20% is used for validation. After training on this dataset, the plain RNN, LSTM, and GNU models all have fairly similar Root of Mean Squared Error values and do not have major differences. RNN's use sequential time series data to make sequential predictions by associating each layer of the network with where that element is in the sequence. LSTM maintians a short term memory and caries the cell state through the gates of the network to avoid potential RNN pitfalls like the vanishing gradient problem, and make it more accurate for longer sequences. A GRU network only has two gates to do reset and update, so is less complex and uses less memory than LSTM. For this application, there doesn't appear to be a significant difference between the 3 implementations.\n",
        "\n",
        "Although I think you could technically use a traditional feed-forward network to solve the problem of predicting customer sales time series data, they wouldn't be able to handle the sequential changes that the time series data goes through. You could remove the timestamp and then simply predict based on which products are bought more often, but this would fail to encode information like how product popularitty changes over time."
      ],
      "metadata": {
        "id": "OBDEVztTh43s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n",
        "I decided to use GloVe word embeddings that are pre-trained on Wikipedia 2014 page datseta and the Gigaword Fifth Edition archive of newswire text data. Unlike Word2Vec which focuses on only the local context for word, GloVe is able to look at the global context using co-occurence for the relationships between words. BERT is a more complicated and newer language model and unlike GloVe and Word2Vec, breaks words into chunks to make its vectors.\n",
        "\n",
        "**Sources:**\n",
        "\n",
        "https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "https://github.com/stanfordnlp/GloVe\n",
        "\n",
        "https://medium.com/biased-algorithms/word2vec-vs-glove-which-word-embedding-model-is-right-for-you-4dfc161c3f0c"
      ],
      "metadata": {
        "id": "FQjPAoYNikHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Implementing Word Embeddings"
      ],
      "metadata": {
        "id": "Al7x6lzc0Trp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "\n",
        "embedded_vectors = {}\n",
        "\n",
        "with open(\"/content/glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embedded_vectors[word] = vector"
      ],
      "metadata": {
        "id": "UwovUmGSlpli"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word):\n",
        "  if word not in embedded_vectors:\n",
        "    return \"Word is not in the vocabulary. Check if you spelled it correctly. \\n Try a synonym, similar word, or the same word with a different word ending\"\n",
        "  return embedded_vectors[word]"
      ],
      "metadata": {
        "id": "HtnjSUHiytgV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar(input):\n",
        "  return sorted(embedded_vectors.keys(), key=lambda word: spatial.distance.cosine(embedded_vectors[word], embedded_vectors[input]))"
      ],
      "metadata": {
        "id": "MaTj0Pj-5eQJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = input(\"Enter word 1 \")\n",
        "word2 = input(\"Enter word 2 \")\n",
        "\n",
        "print(\"\\nWord 1 (\" + word1 + \") Embedding\\n\", get_embedding(word1))\n",
        "print(get_similar(word1)[:10])\n",
        "\n",
        "print(\"\\nWord 2 (\" + word2 + \") Embedding\\n\", get_embedding(word2))\n",
        "print(get_similar(word2)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmFP3-L7xg_Z",
        "outputId": "799b13b9-7731-4d84-82f7-33e19504f975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter word 1 dinosaur\n",
            "Enter word 2 tree\n",
            "\n",
            "Word 1 (dinosaur) Embedding\n",
            " [ 1.205     0.10877  -0.094795  0.3586    0.93909   0.46756  -0.96862\n",
            " -1.0826    0.84181  -0.13962  -0.2765    0.037012  0.65822   1.0477\n",
            "  0.38418   0.1716    0.56626   0.41853  -0.65172   0.83056  -0.87654\n",
            " -1.1372    0.14675  -0.58977   0.49775  -0.22689  -1.835     0.17539\n",
            " -0.39579  -1.0242    0.52122  -1.5274    0.59411  -0.032497 -0.15303\n",
            "  0.277    -0.051661  0.29419   0.29804  -0.20573  -1.546    -1.1246\n",
            " -0.24908   0.90377   1.1106    0.061206  0.31303  -0.51548  -0.30526\n",
            " -0.39686 ]\n",
            "['dinosaur', 'dinosaurs', 'fossils', 'fossilized', 'theropod', 'reptile', 'hominid', 'sauropod', 'prehistoric', 'mammal']\n",
            "\n",
            "Word 2 (tree) Embedding\n",
            " [ 0.62231   1.1986   -0.014116  0.20125   0.69419   0.12068  -0.90399\n",
            " -1.4023    0.43357  -0.48537  -0.4645    0.15756   0.54261  -0.32467\n",
            " -0.025646  0.45742   0.16561   0.18819   0.062099 -0.86418  -1.0425\n",
            " -0.81157   0.3126   -0.20279   0.55734  -0.28634  -0.14874   1.0098\n",
            "  0.25041  -0.53195   2.3793   -0.76966  -0.63219   0.3203    0.15072\n",
            "  0.23326  -0.26254  -0.29461   0.7671   -0.11577  -0.68129  -0.65413\n",
            " -0.58914   0.24684   1.5904    0.33025   0.41513  -1.7468    0.82453\n",
            " -1.0886  ]\n",
            "['tree', 'trees', 'pine', 'flower', 'oak', 'green', 'leaf', 'bark', 'planted', 'cedar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Cosine Similarity Computation\n",
        "\n",
        "We need to use cosine similarity rather than just the Euclidian distance between the vectors because unless the vectors are similar in length, we need to normalize the data to get standardized results by finding the angle between the direction of the two vectors. Cosine similarity is very useful because you can calculate which words are similar to each other, which has applications in text parsing and sentiment analysis."
      ],
      "metadata": {
        "id": "mjdZ7DtP0YA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spatial.distance.cosine(embedded_vectors[word1], embedded_vectors[word2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIHHrT8Z3qu2",
        "outputId": "248a0778-5cf2-428b-d47a-c6ac171ee36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4470889848781706"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_words(word1, word2):\n",
        "  cos_similarity = spatial.distance.cosine(embedded_vectors[word1], embedded_vectors[word2])\n",
        "  cos_similarity = 1 - cos_similarity\n",
        "  print(cos_similarity)"
      ],
      "metadata": {
        "id": "g_NkBacaEZdh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  cont = input(\"\\nEnter to continue or type 'q' to quit \")\n",
        "  if cont == 'q':\n",
        "      break\n",
        "  word1 = input(\"Enter word 1 \")\n",
        "  word2 = input(\"Enter word 2 \")\n",
        "  compare_words(word1, word2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2FjeEb4GVoD",
        "outputId": "c21ba2ab-f30d-4300-8d87-a606909587e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter to continue or type 'q' to quit \n",
            "Enter word 1 dinosaur\n",
            "Enter word 2 tree\n",
            "0.5529110151218294\n",
            "\n",
            "Enter to continue or type 'q' to quit \n",
            "Enter word 1 rock\n",
            "Enter word 2 pillow\n",
            "0.2802894057859757\n",
            "\n",
            "Enter to continue or type 'q' to quit \n",
            "Enter word 1 paper\n",
            "Enter word 2 pencil\n",
            "0.6001403137250114\n",
            "\n",
            "Enter to continue or type 'q' to quit \n",
            "Enter word 1 aardvark\n",
            "Enter word 2 pencil\n",
            "-0.12055060521809735\n",
            "\n",
            "Enter to continue or type 'q' to quit \n",
            "Enter word 1 stapler\n",
            "Enter word 2 dragon\n",
            "-0.18699547255872928\n",
            "\n",
            "Enter to continue or type 'q' to quit q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = ['dog', 'goat', 'sheep', 'llama', 'lion', 'tiger', 'jaguar', 'kangaroo', 'emu', 'ostrich' 'leopard', 'salamander', 'axolotl', 'puffin', 'jay', 'oyster', 'shark', 'parrot', 'salmon', 'tuna', 'whale', 'orca', 'squid', 'octopus', 'starfish', 'clam', 'fish', 'cat', 'horse', 'aardvark', 'dragon', 'cow', 'bunny', 'pterodactyl', 'sheep', 'mouse', 'mice', 'rat', 'pig', 'gecko', 'koala', 'wallaby', 'duck', 'chicken', 'yak', 'snake', 'lizard', 'deer', 'platypus']\n",
        "animals2 = []\n",
        "\n",
        "vectors = []\n",
        "for animal in animals:\n",
        "  vec = get_embedding(animal)\n",
        "  if isinstance(vec, str):\n",
        "    continue\n",
        "  animals2.append(animal)\n",
        "  vectors.append(get_embedding(animal))\n",
        "\n",
        "\n",
        "vector_df = pd.DataFrame(vectors)"
      ],
      "metadata": {
        "id": "m8qz4mMxMaRA"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = StandardScaler().fit_transform(vector_df)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(x)\n",
        "pca_df = pd.DataFrame(data=principalComponents, columns=['PrincipalComponent1', 'PrincipalComponent2'])\n",
        "\n",
        "pca_df['Animal'] = animals2\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(x='PrincipalComponent1', y='PrincipalComponent2', data=pca_df, hue='Animal', palette=\"viridis\", s=100)\n",
        "\n",
        "for i, row in pca_df.iterrows():\n",
        "    plt.text(row['PrincipalComponent1'], row['PrincipalComponent2'], row['Animal'], fontsize=9, ha='right')\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('2D PCA ScatterPlot of Animal Embeddings')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yo_4MgnESvv4",
        "outputId": "ca8a24d2-486a-4e2d-b3a3-35b8716260e7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Designing a Novel Dissimilarity Metric\n",
        "\n",
        "Here we are using Euclidian distance as our new dissimilarity metric. This helps us use magnitude as well as angle.\n",
        "\n",
        "Reference:\n",
        "https://www.cs.toronto.edu/~lczhang/321/lec/glove_notes.html#Measuring-Distance"
      ],
      "metadata": {
        "id": "qsLuquFuVmCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similarity(word1, word2, mode=\"cos\"):\n",
        "  if mode == \"cos\":\n",
        "    return 1- spatial.distance.cosine(embedded_vectors[word1], embedded_vectors[word2])\n",
        "  elif mode == \"euc\":\n",
        "    return np.linalg.norm(embedded_vectors[word1] - embedded_vectors[word2])"
      ],
      "metadata": {
        "id": "yfrFsdArL_GY"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_animals = []\n",
        "evil_animals = []\n",
        "valid_animals = []\n",
        "\n",
        "for animal in animals:\n",
        "  if animal not in embedded_vectors:\n",
        "    continue\n",
        "  valid_animals.append(animal)\n",
        "  good_vec = find_similarity(animal, \"good\")\n",
        "  good_animals.append(good_vec)\n",
        "  evil_vec = find_similarity(animal, \"evil\")\n",
        "  evil_animals.append(evil_vec)"
      ],
      "metadata": {
        "id": "Omifc1m1XTCs"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {}\n",
        "data['good'] = good_animals\n",
        "data['evil'] = evil_animals\n",
        "\n",
        "plt.scatter('good', 'evil', color='lightgray', data=data)\n",
        "plt.xlabel('Good')\n",
        "plt.ylabel('Evil')\n",
        "plt.title('Animal Good vs Evil w/ Cosine Similarity')\n",
        "\n",
        "for i, animal in enumerate(valid_animals):\n",
        "    plt.text(good_animals[i], evil_animals[i], animal, fontsize=9, ha='right', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "UMQRsSJobNvw",
        "outputId": "06770d2d-95ed-44d0-eeab-6a9dcd18af16"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "good_animals2 = []\n",
        "evil_animals2 = []\n",
        "valid_animals2 = []\n",
        "\n",
        "for animal in animals:\n",
        "  if animal not in embedded_vectors:\n",
        "    continue\n",
        "  valid_animals2.append(animal)\n",
        "  good_vec = find_similarity(animal, \"good\", \"euc\")\n",
        "  good_animals2.append(good_vec)\n",
        "  evil_vec = find_similarity(animal, \"evil\", \"euc\")\n",
        "  evil_animals2.append(evil_vec)"
      ],
      "metadata": {
        "id": "gTZeOUn2aVGU"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = {}\n",
        "data2['good'] = good_animals2\n",
        "data2['evil'] = evil_animals2\n",
        "\n",
        "plt.scatter('good', 'evil', color='lightgray', data=data2)\n",
        "plt.xlabel('Good')\n",
        "plt.ylabel('Evil')\n",
        "plt.title('Animal Good vs Evil w/ Euclidian Disimilarity')\n",
        "\n",
        "for i, animal in enumerate(valid_animals2):\n",
        "    plt.text(good_animals2[i], evil_animals2[i], animal, fontsize=9, ha='right', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "QULjhZb3alQd",
        "outputId": "93b183f2-1b14-42ed-e249-7ddf475e768b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAsilJREFUeJzs3Xl8TNf/+PHXJJkkskwm6ySRZYidIpTaJbbY+VAksQWlqKqltJYqpSilaldascRStbRVS6lobUVtFTSaSkRWRFYiErm/P/xyv0b2CNnO8/GYh+Tec++cOxkz73uW91FIkiQhCIIgCIJQTuiVdAUEQRAEQRCKkwhuBEEQBEEoV0RwIwiCIAhCuSKCG0EQBEEQyhUR3AiCIAiCUK6I4EYQBEEQhHJFBDeCIAiCIJQrIrgRBEEQBKFcEcGNIAiCIAjlighuhGI1e/ZsFApFhXvu4uLn54dWqy3pahS7nK5LoVAwe/bsEqlPafAyr0lO73WtVoufn1/xVfAVKsr/1bCwMBQKBf7+/sVWj+PHj6NQKDh+/Li87VX9H6zo7/fXTQQ3AqtXr0ahUPDWW2+VdFVeqxMnTtC/f38qV66MoaEhFhYWvPXWW3z22WfExsaWdPVKlFarRaFQ5Pjo3LlzSVevwOLj4zEwMOD777/PtYyfn1+u12psbPwaa1s2+fv7Z3vNHB0d8fLyYvny5SQnJ5d0FUul06dPM3v2bBISEkq6KuWSQUlXQCh5AQEBaLVazp07R0hICNWqVSvyuWbOnMnHH39cjLV7NWbNmsXcuXOpWrUqfn5+VK1alcePH3PhwgWWLFnCpk2b+O+//0q6miWqYcOGTJ48Odt2R0fHQp9r/fr1ZGZmFke1CuXw4cMoFAo6deqUZzkjIyM2bNiQbbu+vv6rqlqOUlNTMTAo2sdycHAwenold7/62WefUaVKFdLT04mJieH48eNMmDCBpUuX8tNPP1G/fn25bFE+J1xdXUlNTUWpVBZbndu0aUNqaiqGhobFds7cvPi3PX36NHPmzMHPzw+1Wv3Kn7+iEcFNBRcaGsrp06fZs2cP7777LgEBAXz66adFPp+BgUGRP5xfl507dzJ37lz69+/Pli1bsn2wffXVV3z11VclVLvSo3LlygwaNKhYzlWcX0iFceDAAVq2bJnvl4eBgUGxXevLeJmWIiMjo2KsSeF16dKFN998U/592rRpHDt2jO7du9OzZ09u3LhBpUqVgKJ9TryKljQ9Pb1X2jqXmZnJkydPMDY2Fq2Ar5nolqrgAgICsLS0pFu3brz99tsEBARkK5PV1/3ll1/yzTff4ObmhpGREU2aNOH8+fM6ZXPqS1coFIwbN45du3ZRp04dKlWqRPPmzbl69SoA69ato1q1ahgbG+Ph4UFYWJjO8SdOnKBfv364uLhgZGSEs7MzEydOJDU1tUjXPGvWLGxsbPj2229zvGOzsLDIsW989erV1K1bFyMjIxwdHXnvvfdybFLetWsXjRs3plKlStjY2DBo0CAiIyOzldu3bx/16tXD2NiYevXqsXfv3gLVv3v37lStWjXHfc2bN9f5gjly5AitWrVCrVZjZmZGzZo1mT59eoGeJz9ffvklCoWC27dvZ9s3bdo0DA0NiY+PB4o2jkGSJGxsbJg0aZK8LTMzE7Vajb6+vs5r/8UXX2BgYEBKSopO2UOHDtGtW7fCXVguchsnktUt8+L79uDBg7Rt2xZzc3NUKhVNmjRh27ZteT5HTuMyTp48SZMmTTA2NsbNzY1169bleOyLY24ePHjAhx9+yBtvvIGZmRkqlYouXbpw5coVneOyxp18//33fP755zg5OWFsbEz79u0JCQnJs775adeuHZ988gm3b99m69at8vacXsv83qs5jbnx8/PDzMyM8PBwunfvjpmZGZUrV2bVqlUAXL16lXbt2mFqaoqrq2u21z+nMTc5+fLLL2nRogXW1tZUqlSJxo0b88MPP2Qrl/VZFxAQIH9WHDp0SN6X9bedPXs2U6ZMAaBKlSpyl15YWBht27alQYMGOdajZs2aeHl55VlX4RkR3FRwAQEB9OnTB0NDQ3x8fPj333+zBSxZtm3bxuLFi3n33XeZN28eYWFh9OnTh/T09Hyf58SJE0yePJmhQ4cye/Zsbty4Qffu3Vm1ahXLly9n7NixTJkyhTNnzjB8+HCdY3ft2sWjR48YM2YMK1aswMvLixUrVjBkyJBCX+/Nmze5efMmvXv3xszMrMDHzZ49m/feew9HR0eWLFlC3759WbduHZ06ddK5fn9/f/r374++vj4LFixg5MiR7Nmzh1atWul8Gf/666/07dsXhULBggUL6N27N8OGDeOvv/7Kty4DBgwgNDQ029/p9u3b/Pnnn3h7ewNw7do1unfvTlpaGp999hlLliyhZ8+enDp1qkDXnJ6ezv3797M9soLK/v37y1+KL/r+++/p1KkTlpaWBXqunCgUClq2bMkff/whb/v7779JTEwE0LmOEydO4O7urvM3PX/+PPfu3aNr164Fer6crjUpKalIdff396dbt248ePCAadOmsXDhQho2bCh/0RXU1atX6dSpE3fv3mX27NkMGzaMTz/9tECB8K1bt9i3bx/du3dn6dKlTJkyhatXr9K2bVuioqKylV+4cCF79+7lww8/ZNq0afz5558MHDiwUPXNyeDBg4Fn7/ncvMx79enTp3Tp0gVnZ2cWLVqEVqtl3Lhx+Pv707lzZ958802++OILzM3NGTJkCKGhoYW+hq+//hp3d3c+++wz5s+fj4GBAf369eOXX37JVvbYsWNMnDiRAQMG8PXXX+cY1Pfp0wcfHx/gWUvxli1b2LJlC7a2tgwePJi///6boKAgnWPOnz/PzZs3S0ULY5kgCRXWX3/9JQHSkSNHJEmSpMzMTMnJyUn64IMPdMqFhoZKgGRtbS09ePBA3v7jjz9KgPTzzz/L2z799FPpxbcVIBkZGUmhoaHytnXr1kmAZG9vLyUlJcnbp02bJgE6ZR89epSt7gsWLJAUCoV0+/btPJ/7RVl1XrZsmc72zMxM6d69ezqP9PR0SZIk6e7du5KhoaHUqVMn6enTp/IxK1eulADpu+++kyRJkp48eSLZ2dlJ9erVk1JTU+Vy+/fvlwBp1qxZ8raGDRtKDg4OUkJCgrzt119/lQDJ1dU1z2tITEyUjIyMpMmTJ+tsX7Rokc5r8tVXX0mAdO/evTzPlxNXV1cJyPGxYMECuVzz5s2lxo0b6xx77tw5CZA2b94sbxs6dGi26wKkTz/9NM96LF68WNLX15ffI8uXL5dcXV2lpk2bSh999JEkSZL09OlTSa1WSxMnTtQ59pNPPsn3tcyqW27X6uXlJZfL7f21ceNGnfdsQkKCZG5uLr311ls67wNJevY+e/5583tNevfuLRkbG+u8z69fvy7p6+tnq4urq6s0dOhQ+ffHjx/rvF8l6dn/ZSMjI+mzzz6TtwUGBkqAVLt2bSktLU3e/vXXX0uAdPXq1WzXnNP1nz9/PtcyFhYWkru7u/z7i69lQd6rWZ9DGzdulLdl/e3mz58vb4uPj5cqVaokKRQKaceOHfL2f/75J9vrm3XtgYGBOud88e/y4mfQkydPpHr16knt2rXT2Q5Ienp60rVr17LV/8XnXrx4cbbPOkl69v4xNjaW399Zxo8fL5mamkopKSnZzi1kJ1puKrCAgAA0Gg2enp7AszvlAQMGsGPHDp4+fZqt/IABA3TuxFu3bg08u0PMT/v27XXuYLJmZvXt2xdzc/Ns258/Z1Y/PcDDhw+5f/8+LVq0QJIkLl26VJBLlWXdib/YapOYmIitra3O4/LlywAcPXqUJ0+eMGHCBJ0BmyNHjkSlUsl3b3/99Rd3795l7NixOv3r3bp1o1atWnK56OhoLl++zNChQ7GwsJDLdezYkTp16uR7DVndC99//z2SJMnbd+7cSbNmzXBxcQGQx5n8+OOPRRrM+9Zbb3HkyJFsj6w7Tnj2nrhw4YLO4OudO3diZGREr169Cv2cL2rdujVPnz7l9OnTwLMWmtatW9O6dWtOnDgBQFBQEAkJCfL7McuBAwcK3CVlbGyc47UuXLiw0HU+cuQIycnJfPzxx9nGWRRm+vPTp085fPgwvXv3lv+mALVr1y5Q14SRkZH8fn369ClxcXFyd8/FixezlR82bJhON21h/n/nx8zMLM9ZUy/7Xn3nnXd0zlWzZk1MTU3p37+/vL1mzZqo1eoiXc/zn0Hx8fEkJibSunXrHF/Htm3bFuj/cW4sLCzo1asX27dvl/9/P336lJ07d9K7d29MTU2LfO6KRAQ3FdTTp0/ZsWMHnp6ehIaGEhISQkhICG+99RaxsbH89ttv2Y55/gMWkAOdrHEVeXnx2KwvdWdn5xy3P3/O8PBw/Pz8sLKywszMDFtbW9q2bQsgd1EUVFYg9fzYDHj24Zv1hZbVF54la0xJzZo1dbYbGhpStWpVeX9u5QBq1aqVrVz16tWzlcvp2JwMGDCAO3fucObMGQD+++8/Lly4wIABA3TKtGzZknfeeQeNRoO3tzfff/99gb88bGxs6NChQ7aHq6urXKZfv37o6emxc+dO4Nk4mV27dtGlSxdUKlWBnicvjRo1wsTERA5ksoKbNm3a8Ndff/H48WN5X6tWreTjYmJiuHjxYoGDG319/RyvtWHDhoWuc1agV69evUIf+7x79+6Rmppa5PdJZmYmX331FdWrV8fIyAgbGxtsbW11uvae9zL/v/OTkpKicxPzopd5rxobG2Nra6uzzcLCAicnp2zBpIWFRZGuZ//+/TRr1gxjY2OsrKywtbVlzZo1Ob6OVapUKfT5XzRkyBDCw8Pl9/bRo0eJjY2Vu/iE/IngpoI6duwY0dHR7Nixg+rVq8uPrDudnAYW5zYt9vnWg9zkdmx+53z69CkdO3bkl19+4aOPPmLfvn0cOXJEHlRY2Lu8WrVqAWTrzzYwMJC/0F7mrut16dGjByYmJvJ4l++//x49PT369esnl6lUqRJ//PEHR48elfvxBwwYQMeOHXNsmSsKR0dHWrduLdfjzz//JDw8XCfIehlKpZK33nqLP/74g5CQEGJiYmjdujWtWrUiPT2ds2fPcuLECWrVqqXzBXfw4EGMjY3lVsnikFurS3G9lsVt/vz5TJo0iTZt2rB161YOHz7MkSNHqFu3bo7/b17m/3deIiIiSExMzDPFxMu8V4v62VJQJ06coGfPnhgbG7N69WoOHDjAkSNH8PX1zfFcz7fyFJWXlxcajUYehL1161bs7e3p0KHDS5+7ohDBTQUVEBCAnZ0du3btyvbw8fFh7969RZ6NVJyuXr3KzZs3WbJkCR999BG9evWiQ4cORcq1As/ueKtXr86+fft4+PBhgY7JaqkIDg7W2f7kyRNCQ0Pl/bmVy9r2Yrl///03x3IFYWpqSvfu3dm1axeZmZns3LmT1q1bZ3td9PT0aN++PUuXLuX69et8/vnnHDt2jMDAwAI9T0EMGDCAK1euEBwczM6dOzExMaFHjx7Fdv7WrVtz7tw5jh49io2NDbVq1cLKyoq6dety4sQJTpw4QZs2bXSO+eWXX/D09CyWL5osWS0ZL86Qe3G2mJubG5A9gC4sW1tbKlWqVOT3yQ8//ICnpyfffvst3t7edOrUiQ4dOrz2pHFbtmwByLcr7XW8V4ti9+7dGBsbc/jwYYYPH06XLl2KJcjIq4tSX18fX19ffvjhB+Lj49m3bx8+Pj6vPe9SWSaCmwooNTWVPXv20L17d95+++1sj3HjxpGcnMxPP/1U0lWV/zM/f4ckSRJff/11kc85e/Zs7t+/z8iRI3Oc6fXi3ViHDh0wNDRk+fLlOvu+/fZbEhMT5a6PN998Ezs7O9auXUtaWppc7uDBg9y4cUMu5+DgQMOGDdm0aZNOs/aRI0e4fv16ga9jwIABREVFsWHDBq5cuZKtteTBgwfZjsnqZnm+fi+rb9++6Ovrs337dnbt2kX37t2LdVxA69atSUtLY9myZbRq1Ur+UmjdujVbtmwhKipKZ7xNeno6R44cKbYp4FmygpbnZ289fPiQTZs26ZTr1KkT5ubmLFiwgMePH+vsK0yrgb6+Pl5eXuzbt4/w8HB5+40bNzh8+HCBjn/x+Xbt2pVjWoJX5dixY8ydO5cqVarkOfPqdb1Xi0JfXx+FQqHTghQWFsa+ffte6rxZ/0dyCzYHDx5MfHw87777LikpKWKWVCGV7mxrwivx008/kZycTM+ePXPc36xZM2xtbQkICCi27oWiqlWrFm5ubnz44YdERkaiUqnYvXv3S40D8PX1JSgoiAULFnDu3Dm8vb2pUqUKDx8+JCgoiO3bt2Nubi7fqdva2jJt2jTmzJlD586d6dmzJ8HBwaxevZomTZrIHzpKpZIvvviCYcOG0bZtW3x8fIiNjZWng06cOFGuw4IFC+jWrRutWrVi+PDhPHjwgBUrVlC3bt1s44Fy07VrV8zNzfnwww/R19enb9++Ovs/++wz/vjjD7p164arqyt3795l9erVODk56YxPyU1kZKRObpIsZmZm9O7dW/7dzs4OT09Pli5dSnJycrG/Z5o3b46BgQHBwcGMGjVK3t6mTRvWrFkDoBPcnDx5kqSkpEIFNxkZGTleK8D//vc/TE1N6dSpEy4uLowYMYIpU6agr6/Pd999h62trU7woVKp+Oqrr3jnnXdo0qQJvr6+WFpacuXKFR49epQtGMrLnDlzOHToEK1bt2bs2LFkZGTI75O///47z2O7d+/OZ599xrBhw2jRogVXr14lICAg1xxJL+vgwYP8888/ZGRkEBsby7Fjxzhy5Aiurq789NNPeSaxe9n36qvUrVs3li5dSufOnfH19eXu3busWrWKatWq5fs3yEvjxo0BmDFjBt7e3iiVSnr06CEHPe7u7tSrV49du3ZRu3ZtGjVqVCzXU2GUwAwtoYT16NFDMjY2lh4+fJhrGT8/P0mpVEr379+Xp2AuXrw4WzlemN6Y21Tw9957T2dbbufMmpq5a9cuedv169elDh06SGZmZpKNjY00cuRI6cqVK9mmhRZkKvjzjh8/Lr399tuSg4ODpFQqJZVKJb355pvSp59+KkVHR2crv3LlSqlWrVqSUqmUNBqNNGbMGCk+Pj5buZ07d0ru7u6SkZGRZGVlJQ0cOFCKiIjIVm737t1S7dq1JSMjI6lOnTrSnj17cpyGmpeBAwdKgNShQ4ds+3777TepV69ekqOjo2RoaCg5OjpKPj4+0s2bN/M9b15TwXOq3/r16yVAMjc3zzb9WZKKPhU8S5MmTSRAOnv2rLwtIiJCAiRnZ2edsh9++KFUp06dAp03q265XSsvTNW9cOGC9NZbb0mGhoaSi4uLtHTp0mxTwbP89NNPUosWLaRKlSpJKpVKatq0qbR9+3ad5y3Ia/L7779LjRs3lgwNDaWqVatKa9euzfG9ntNU8MmTJ0sODg5SpUqVpJYtW0pnzpyR2rZtK7Vt21Yul9P/OUnKeep1TrKuP+thaGgo2dvbSx07dpS+/vprnVQPWV6sf0Heq7lNBTc1Nc12/rZt20p169bNtt3V1VXq1q1btmvPbyr4t99+K1WvXl0yMjKSatWqJW3cuLHAn3XP73vxbzt37lypcuXKkp6eXo7voUWLFmWb6i4UjEKSXnK0mCAIQilSp04dunfvzqJFi0q6KoLwUr7++msmTpxIWFhYttlsQt5Et5QgCOXGkydPGDBggE5+E0EoiyRJ4ttvv6Vt27YisCkCEdwIglBuGBoavtTCr4JQ0h4+fMhPP/1EYGAgV69e5ccffyzpKpVJoltKEARBEEqJsLAwqlSpglqtZuzYsXz++eclXaUySQQ3giAIgiCUKyLPjSAIgiAI5YoIbgRBEARBKFcq3IDizMxMoqKiMDc3L9QKvYIgCIIglBxJkkhOTsbR0VFe8T43FS64iYqKyrYStSAIgiAIZcOdO3dwcnLKs0yFC27Mzc2BZy+OSqUq4doIgiAIglAQSUlJODs7y9/jealwwU1WV5RKpRLBjSAIgiCUMQUZUiIGFAuCIAiCUK6I4EYQBEEQcuDh4cGyZctKuhpCEYjgRhAEQRCEckUEN4IgCGWQVqtl3759APj7+9OwYcMCHRcWFoZCoSAhIeGV1U0QSpoIbgRBEIRy6+uvv8bDw0Nn244dO6hTpw6XLl2iVatWWFlZYWtri4+PD3FxcTmeJyUlBS8vLwYOHEh6evprqLnwMkRwIwiCIJRbgwYN4uzZs4SGhsrbNm7cyLBhw9DT02PhwoXExsYSFBREZGQkH3/8cbZz3Lt3D09PT+rWrcvWrVtRKpWv8xKEIhDBjSAIwmu0c+dOmjVrJv/et29fHBwc5N8nT57M+++/z6+//sqbb76JhYUFDg4OjB07ltTU1AI9x9KlS6levTrm5ua4ubmxcuXKbGV27dqFVqvF2tqasWPH8uTJEwDc3d3x9/fXKdu5c2e++OKLIlxtybO2tqZnz55s2rQJgMjISH7//XcGDx5MgwYNaNWqFUqlEo1Gw6RJkzh+/LjO8bdu3aJly5b069ePpUuXisz2ZYQIbgRBEF4jDw8PLly4QHJyMpIkcfLkSYyNjblx4wYAx44do127dlSqVIn169fz4MEDTp06RWBgIEuXLi3Qc7i6unLs2DGSkpLYsGEDU6ZM4dSpUzpl9u7dy+XLl7l69SqnT59mwYIFAIwYMUInuImMjCQwMJAhQ4YUzwtQAoYPH87mzZuRJInNmzfTqVMn7O3tCQkJoVevXjg6OqJSqRg0aBD379/XOfb7779HT0+PMWPGlFDthaIQwY0gCMJrpNFoqFGjBidOnODy5cu4urrSvXt3AgMDefDgAUFBQXh4eNC6dWvc3d3R19enatWqvPvuu9laFXLTt29fnJ2dUSgUeHp64uXlle3Y2bNno1arcXR0ZNq0aWzZsgWAgQMHcu7cObkbZ/PmzXTs2FGndams6dixIxkZGfz+++9s2rSJYcOGATB69GgqV67M9evXSUpKYuvWrUiSpHPs1KlTad68OV5eXiQlJZVE9YUiEMGNIAjCa+bp6UlgYCDHjh3D09OT9u3bExgYSGBgIPXr18fS0pLz58/ToUMHNBoNKpWK6dOnZ2tVyE1AQACNGjXCysoKtVrNgQMHsh3r6uqq83NkZCQAlpaW9OrVS+7G2bRpE8OHDy+mKy8Zenp6DBs2jAkTJvDgwQO6d+8OPEvnb25ujkql4s6dOyxevDjHY7/99lvq1KlDp06dSEhIICUlRf73xWBIKB1EcCMIgvCaPR/ctGvXDg8PD06cOMFvv/2Gp6cnAD4+Pnh6enLr1i2SkpKYP39+gb5Iw8PDGTp0KIsWLeLu3bskJCTQtWvXbMfevn1b55jKlSvLv48YMYLNmzdz+vRp4uLi6NGjB5Iklekv9WHDhvH3338zaNAgeUDw0qVL2b9/PyqVil69etG3b98cj9XT02P9+vXUqVOH1q1bc+XKFSIiIggLCyM4OJjExMTXeSlCAVS4taUEQRBKWtu2bfH29sbc3Jzvv/8eU1NTnJycCAgIYPv27cCzVgW1Wo2pqSk3btxgzZo1VKpUKd9zZwUednZ26OnpceDAAX799VdGjRqlU+6zzz5j69atPHr0iAULFjBw4EB5X/v27ZEkibFjxzJo0CAePXpEdHQ0GRkZchkDAwMcHBywsLAoplfl1bKzs8PExESnFapVq1Zcu3ZNp9ykSZPkn5/vyktKStLZlyUjI4M7d+4AlJnXoiIQLTeCIAivmY2NDXXq1KFOnTqYmpoCzwKKR48e0aZNGwDWrVvHl19+iZmZGaNHj8bb27tA565Tpw4zZsygXbt2WFtbs3PnTnr27JmtXK9evWjYsCH16tXjrbfeYvr06fI+hULBsGHDuHLlCm+//TZ37tzRCWzg/77Uy0KrhSRJrFixAnd3d+rVq1ek46Ojo/MsExMTU+Zas8ozhVTB/hpJSUlYWFiQmJgoVgUXBEHIxebNm1m+fDlbt27NFtg8T6lUUqNGjVI7Rfrp06eo1WpsbGzYvXs3jRo1KvQ5UlJSCAsLy7ecVqvFzMysCLUUCqIw39+iW0oQBEHQkZKSwvLlyxk2bFiegQ1Aeno6Dx8+LLVf6vr6+iQnJ7/UOfJ7DQpbTnj1RLeUIAiCINuyZQsajYbKlSszYMCAAh1T3r/UDQwK1g5Q0HLCqyeCG0EQBEE2ePBgHj58yI8//oixsXGBjinvX+qmpqb5XqNSqZTHTwklTwQ3giAIQo7El/ozCoUi3ySG9vb2pXbcUUUkghtBEAQhR+JL/f9YWFjg7OycLdhTKpU4OzuLaeClTPluSxQEQRBeStaX9ot5bpRKJfb29hXqS93CwgKVSsXDhw/JyMjAwMAAU1PTChHclTUiuBEEQRDyJL7U/49CoSi1M8OE/yOCG0EQBCFf4ktdKEvEmBtBEARBEMoVEdwIgiAIglCuiOBGEARBEIRyRQQ3giAIgiCUKyK4EQRBEAShXBHBjSAIgiAI5YoIbgRBEARBKFdEcCMIgiAIQrkightBEARBEMoVEdwIgiAIglCuiOBGEIRySavVsm/fvpKuhiAIJUAEN4IgCIIglCsiuBEEodzp168f4eHh+Pj4YGZmRufOnVEoFCQkJMhlJkyYgJ+fHwBhYWEoFAq2bNlCtWrVUKvV+Pn5kZ6eDkBKSgq9evXCzs4OCwsL2rRpw5UrV0rgygTh1fjzzz+pU6cO5ubmLF++HIBRo0ZhZWWFvb094eHhmJmZkZiYWMI1LRgR3AiCUO7s2rULFxcXtm/fTkpKCmvXri3QcQcPHuTSpUtcv36d3377jYCAAAAyMzPx9fUlNDSU2NhY3N3d6d+/P5IkvcrLEITXZubMmfj4+JCcnMz48eM5efIkP/zwA6GhocTExODi4kJKSgoWFhYlXdUCEcGNIAjC/zdr1izMzc1xdHSkc+fOXLhwAQCVSsWAAQMwNTXF2NiYOXPmcPPmTaKiokq4xoJQPEJDQ3njjTd0fndxcSkzwcyLRHAjCK+In58fEyZMKOlqCIVgb28v/2xqakpycjIAqampjB07Fq1Wi0qlQqvVAnD//v2SqKYg5Eir1fL555/TqFEjVCoVXl5eREVFyd2uuXXL2tvbExoaKnfjLl++nJEjR3L16lXMzMzw8/PLdg4/Pz9GjhyJt7c35ubm1KxZk+PHj7/2a86NCG4EQSiX9PT+7+PNzMwMgEePHsnboqOjC3yuJUuWcOHCBU6ePElSUhJhYWEAoltKKHU2bNjAtm3biImJwd7enkGDBuV7TFa3U1Y37vjx41m7di1vvPEGKSkp+Pv753jczp07GT16NAkJCQwePFgOlkoDEdwIglAuaTQa/vvvPwBsbGxwcXFh06ZNZGZmEhgYyIEDBwp8rqSkJIyNjbG0tCQlJYXp06cDz4KblJQUEhISSElJEcGOUOLGjBlDrVq1MDExYdGiRQQGBhIREfFKnqtr1654eHigr6/PsGHDuH37NnFxca/kuQpLBDeCUABLly7FxcUFc3NztFotGzZsIDw8nI4dO2Jra4ulpSXdunWT7+hflNWk+91331G1alXMzMyYOnUq0dHRdOzYEZVKRdu2bYmJiZGPCQkJwcvLCysrK9zc3Fi2bJm8z9/fn4YNGzJ37lzs7OzQaDQ6+wWYPn06K1euRK1WM3bsWL777js2btyIhYUF69atw9vbu8DnmjRpEvr6+mg0GurVq0fz5s2BZ3/XsLAwIiIiCAsLIzg4uMzMJhHKJ1dXV/lnjUaDkZERSqXylTzXi924gNyVW9IMSroCglDa3bx5k5kzZ3Lx4kVq1apFbGwssbGxZGZmMmnSJDw9PXny5AkjRoxg5MiRHDlyJNdzBQYGcvXqVW7fvo27uztnzpxh7dq1VKtWje7duzN//nyWL19ORkYG3bt3p2fPnvz444/cvHmTzp07Y2dnh6+vLwDXrl1j8ODBREZGcurUKTp27EiPHj1wc3N7XS9NqfLOO+8QEBBAamoqAD169KBHjx46ZW7evJnjsVqtNlury/PBor29PceOHZN/T0xM5OrVq9nOk5GRwZ07dwDK7EBMoWy7ffu2/PPdu3dJS0ujcuXKwLNuWbVaDTzrlq1UqVJJVPG1EC03gpAPfX19JEni2rVrpKamotFoqF+/Plqtli5dumBsbIxKpWLGjBmcOHGCzMzMXM81c+ZMTE1NqVOnDg0aNKBVq1bUrVsXIyMj/ve//3Hx4kUAzp49S3R0NPPmzcPY2Jj69eszbtw4nb5vGxsbJk+ejFKpxMPDA61Wy+XLl1/xqyFIkpTveJ2YmBjRRSWUiHXr1hEcHExqaiofffQRbdq0wcnJ6aW6ZQujtHTViuBGEPLh5ubGpk2bWLlyJRqNhk6dOnH58mXu3buHr68vzs7OqFQq2rRpQ1paWp7NshqNRv7ZxMQk2+8pKSkARERE4OjoiKGhoby/atWqOn3nzx8LurN7hFfn4cOHZGRk5FkmPT2dhw8fZtuetSREVreiIBS34cOH4+Pjg0ajITIyUs7V9DLdsoVx69atUtFVK4IbQSiA/v37ExgYSGxsLA0aNGDw4MFMmzaNR48ecfHiRZKSkvjjjz+A4plB4+TkRFRUlJwhF56N73Bycnrpc5cHvXr1wsDAAIVCgYGBAUOHDpX3tW/fHj09PfT09Pjf//6nc9z48eOpVKkSCoUCU1NT1q1bJ+979OgRbdq0QalUoqenh729vdySBqBQKOjTpw9WVla88cYbdOzYMc8WnPwCIEF4FerWrSt/Jv3666/yZ0b79u25efMmycnJ7Nixg/Xr1+u0BIeFhdG7d2/5dz8/P52W4Kyu26xuLX9/f52uW4VCwdWrV7PddGV11b7uAEcEN4KQj+DgYI4cOUJqaiqGhoaYmZlhYGBAUlISJiYmqNVq4uLimDNnTrE9Z9OmTdFoNMyaNYu0tDSCgoJYsWKFzpd4TkpLk/CrdPjwYX766Sd++uknJEni8uXL8tiax48fyy1gS5cuZd++ffJYmTlz5rBq1So2bNhAeno67733HmPGjOHff/8FwMPDg6CgIM6fP09SUhJOTk54enpme+5jx45x6NAhUlNTGTFiRK71NDAQQxqFiqE0dtWK4EYQ8vHkyRM++eQTNBoN1tbWHDt2DH9/f+bMmUNISAiWlpa0bNmSLl26FNtzKpVK9u/fz4ULF7C3t6dnz55MmjRJHkyck6dPnxITE1MqmoRfpayZH4GBgTx48IB69erx9ttvA89y2/z888+YmJgwYcIElEolBw8eBGDVqlX06tWLgQMHYmBgwKJFizAzM2PJkiVkZmZy/vx51q5dS8OGDTEzM+Po0aMkJSVx9uxZ+blHjRpFy5YtcXV1ZdKkSdy5cyfHFhqlUinPHimIpUuXUr16dczNzXFzc2PlypXyvqLMtJs6dSqurq6Ym5tTp04ddu3aVbgXWRAK4WW6al8ZqQS5urpKQLbH2LFjcz3m+++/l2rWrCkZGRlJ9erVk3755ZdCPWdiYqIESImJiS9bfUEoNRISEqSrV6/m+khISCjpKharCRMmSGq1WgIkKysraceOHdKIESMkY2NjnXLGxsbSiBEjJEmSJCMjoxw/b7y8vKTr16/nuA+Q1q5dK0mSJAHShg0bJEl69nrv3r1bAqTdu3cX+PV2dXWV9u7dK23cuFFq0KCBvP2HH36QwsPDpczMTOnYsWOSsbGxdPLkSUmSJCk0NFQCpEGDBkkpKSnStWvXJENDQ6lVq1ZSUFCQ9PjxY6lDhw7S+++/L59v69atUmxsrJSRkSFt375dMjIykm7dulVsr78gPC8+Pj7Pz5+sR3x8/Es9T2G+v0u05eb8+fNER0fLj6wptP369cux/OnTp/Hx8WHEiBFcunSJ3r1707t3b4KCgl5ntQWhVJFKYZPwq/bVV18RHx9PXFwcVatWzbe7Dp5lKfbx8UGSJJ3HoUOHqF69OgC//PJLtv3vvvuufI6szxoLCwu5paRq1aryfqVSibOzc77TwB8/fkxERAQuLi7Y2dnx448/olKpuH37Nu3ataNWrVr06NEDMzMzFixYADwbqOng4MCYMWOoU6cOrVq1kte68vLykscHTZgwgSNHjmBnZ4e+vj7e3t7UqlWL06dPF+IVFoSCK2gX7Ovsqi3R4MbW1hZ7e3v5sX//ftzc3Gjbtm2O5b/++ms6d+7MlClTqF27NnPnzqVRo0Y6TbiCUNGUyibhV+jgwYMsXLiQBw8eYGZmhpmZmc5SC7kZM2YMu3btYuvWrWRmZnL//n0WLVrE+fPnMTAwoHHjxvj5+cndUP/++y8ffPCBzjm++eYbLl68yO3bt5k1axZVq1bF1tYWU1NTLC0tqVatWoHy23z33Xc8ffqUv//+m9DQUP7991+qVKlCgwYNALhy5Qq+vr6cO3dOHvT55Zdfcu/ePQwNDbl3716uM+0Arl+/Tt26dbGwsECtVhMUFCTWwRJeGVNT03wDl8J21b6sUjPm5smTJ2zdupXhw4ejUChyLHPmzBk6dOigs83Ly4szZ87ket60tDSSkpJ0HoJQnhR0Vk55mb3z6NEj5s2bh7W1NUZGRly6dCnXtW+eN3fuXN5//31GjRqFvr4+dnZ2LFmyRH5djh8/Tv369WndujUKhYLatWvL43WyeHl50axZM7RaLYaGhqxevZr4+HgePnxIfHw8N27cyLcVLTExkb/++ovKlSvLg9HPnTtHcnIy586dA8DT0xMDAwPq1KlDrVq1AKhdu7acDymvKf9RUVFcvnyZzZs3Ex8fT0JCAvXq1StXLXdC6aJQKHBwcMizjL29fa7f7a9CqRnOv2/fPhISEvJceCsmJibbNDONRqMzkO5FCxYsKNZZLIJQ2pTGJuFXqW/fvvTt2zfb9v79+7NhwwadbVnZirMsXbqUpUuX5njerEHEeRkwYAB79uwhOjo61zV0srbn9mF/9+5dJEnixo0bqNVqMjMzyczMxMDAgLt37wLPuuDfeOMNgGxZZE1MTHj69GmudXzy5AkKhQJbW1syMzPx9/cnKChInkmXkZGBgYEBpqamr/XLRijfsloso6OjdW6klEol9vb2rz1jd6lpufn222/p0qULjo6OxXreadOmkZiYKD+yUqMLQnlRGpuEy7PMzMx8FweMi4vLNVO1jY0NCoWCunXrkpCQQFJSEp988gnm5uZ069YNoMAz77JWO09LS5O3GRoa4urqyhtvvIGjoyPXrl3jrbfe4u7du+V+Jp1QsiwsLKhZsyZarRYnJye0Wi01atQokaVISsWt3O3btzl69Ch79uzJs5y9vT2xsbE622JjY3UW73qRkZERRkZGxVJPQSiNspqE8wrcX3eTcHlW0FWP4+LisLW11dmWtbDq/v37sbCw4P79+9jY2DB27Fjc3d1xd3enSpUqfPfdd3KyNGNjY7766iv5d4Bq1aoxYcIEAFxcXHj8+DEXL14kMDCQgwcP0rdvX1auXElGRgZpaWncu3cvW/3EOlhlW1hYGFWqVCE+Pl7nvVHSFAqFHHSXpFLRcrNx40bs7Ozku5bcNG/enN9++01n25EjR+QVegWhorKwsMDZ2TlbC05BZ+8I+ZMkiQEDBuhkjc5LXuX8/f1Rq9U0adIElUpF69atuXDhQpHq9WJa/T59+pCYmCi30uQU2DyvvM2kEwQAhVTC7+rMzEyqVKmCj48PCxcu1Nk3ZMgQKleuLE+FPH36NG3btmXhwoV069aNHTt2MH/+fC5evEi9evUK9HxJSUlYWFiQmJiISqUq9usRhJIkSZI8e0qMq3g17t27l60FOScajSZby82rVtSud61WWyrutoWCK60tN69SYb6/S7zl5ujRo4SHhzN8+PBs+8LDw3VmHrRo0YJt27bxzTff0KBBA3744Qf27dtX4MBGEMq7rCZhtVqNmZmZCGxeAWtr62ItV1wKku8oN+VlJl1pFRERIWeTbty4MfPnz0er1QKQkpLCuHHj5JxHQ4YM0RkL9e+//9KzZ09sbW2xsrKiT58+OT7Hn3/+SeXKleXhHVu3bqV27dqo1WpatWqls05aRVDiwU2nTp2QJIkaNWpk23f8+PFsUzz79etHcHCwvN5O165dX1NNBUEQni3xkF/gYm1tXaDcO8WpIPmOclNeZtKVVr6+vri6uhIbG8v27dv59ttv5X3Dhw/nwYMHcs6j9PR0xo0bBzz7m3bo0IF69eoRFhZGTEwM77//frbz//LLL/Tp04dt27bRp08f/vjjD8aMGcO6deu4d+8eb7/9Np07d65QA8hLPLgRBEEoaxwcHHINcKytrfPN+fEqvBjY/Pbbb3h5eeV7nJhJ92rduXOHEydOsHDhQipVqkSNGjUYPXo08KyLc/fu3axatQq1Wo2pqSmfffYZO3fu5OnTp+zfvx+lUsnnn3+OqakphoaG2RZz3bhxI2PGjOHQoUNyAtwtW7YwaNAgeZX7CRMmYGlpyS+//PLar7+kiHBdEAShCBwcHNBoNMTFxZGeno5SqSyRFpssRW19ETPpXq2oqCiMjY2xsbGRt7m4uADPxs1kjTt9np6eHjExMdy+fRs3N7c8/z5ffPEFw4cPp379+vK2iIgIPDw8dMpVqVKFiIiIYriiskEEN4IgCEWkp6f32gcN5yYr31FBu6aKmlwtK5ATCsbR0ZHHjx/L0/7h2XhSAGdnZ/T09IiKisLExCTbsa6urvz3339IkpRrgHPw4EH+97//YWlpyZQpUwBwcnKS0w5kCQsLw8nJqRivrHQT3VKCIAjlQGRkJGPHjqVZs2b079+fW7duyfsePXrE559/TufOnfH09GT+/PloNBo5sPnvv//o0aMHtra2uLq6Mm/ePDkJob+/Pw0bNuTTTz/F3t4eb2/vErm+ssrZ2ZmWLVsyffp0UlNT+ffff/nmm2+AZ61mvXv3Zty4cfLaXzExMezduxeAbt26kZaWxqxZs3j48CFPnjwhMDBQ5/xVqlTh999/Z82aNfLM4kGDBhEQEMCpU6fIyMhgxYoVxMXF0aZNGxISEkhJSSn30/9FcCMIglAO+Pr64uzszL///suXX37J7t275X2ffvopGRkZBAUFERYWhiRJ8sDUR48e0b59e9q3b09kZCQnTpxgx44dbNy4UT4+KCgIAwMDwsPD2bJly2u/trJu27Zt3Lp1C41Gg7e3N4MGDZKTy+aV8yhrSZALFy7g4uKCg4MDq1atynZ+V1dXfv/9d7799lvmzp1L27ZtWbFiBSNGjMDa2pqAgADWrFlDQkJChclQXeJ5bl43kedGEITy5s6dO7i4uBAbG4udnR2SJGFlZYWBgQHnzp2jWrVq3L9/H0tLS+DZ9OK6deuSmprKnj17mD9/PpcuXZLPt379enbs2MFvv/2Gv78/kydP5t69eyU2nqi8WbBgAceOHePIkSOv/Lnyy31UlpJ8Fub7W4y5EQRBKOOyBq3a2dkBz/IdGRgYoFAouH//fp6DVsPCwggKCtJJBJeZmYmzs7P8e+XKlUVg8xIuXryIiYkJNWvW5OLFi6xYsYLZs2e/8uctSO6jmJgYVCpVuRtULoIbQRCEMi5r0Ordu3flACcjI0NefiOvQavOzs40btyYP//8M9fzZwU25TkDtp+fH2q1mmXLlhX7ue/du8fo0aPllrWRI0cyYsSIIp8vISEBS0tLQkND5WSAOSlI7qP09HQePnxY7jJUi+BGEAShDEtKSuKLL77AyMiIypUrU79+fZYsWUJKSgqWlpbY29vTqVMnqlWrRlpaGpmZmTRu3Jh+/frx7rvv0r17d0aOHEmLFi3QaDT8+uuvODg4MGPGDJ48ecLHH39McnIyS5YsoVu3bjpflgYGBjg4OJSZbo2S4uXlRWho6Gt/3oLOnCuPGapFO6MgCEIZ5ufnR0hICCdOnKBt27b8888/fPDBBzp34osXL+att97C3Nycp0+fcvbsWXktPzMzMzp27Mj58+f5448/UCqVxMfH8/HHH/Pff/+xePFiXFxcmDZtGjExMTrPnbWyeHkemFqW5ZT7SJIknj59mm+5sk4EN4IgCGVUbGwse/fu5ZtvvqFJkyYcPXqUhw8fcuXKFSwsLOQpx/Xq1WPv3r2EhYWRlJTEiRMniI6Olqd7q1Qq3n77beLi4khISGDLli3cv3+fOXPmMGLECA4cOIC5uTn//vtvjvUobSuLL126FBcXF8zNzdFqtWzYsIHw8HA6duyIra0tlpaWdOvWLVsumOcNGjQIR0dHeT2o56dgZ02PnzVrFjY2Ntjb27Nz505OnTpFvXr1sLCwYMSIEfLrm5KSQq9evbCzs8PCwoI2bdpw5coV+XyzZ8+mR48ejBs3DrVajYuLCzt37pT3p6WlMWbMGKysrKhSpQo//PCDTl0lSWL58uXUqlULtVqNh4cHN27ckHMfeXl5sWHDBgYOHEiTJk3477//5GPLa4ZqEdwIgiCUUbdv38bIyEjOeJube/fuyVPFVSoVbdq0IS0tjeTkZLmMRqORfzYxMcHc3JxKlSrJ4zaMjY159OhRjufPGrdRGty8eZOZM2fy66+/kpyczNmzZ2natCmZmZlMmjSJO3fucPv2bUxMTBg5cmSu52nfvj03btwgLi4Ob29v3n77bZ3XKygoCBsbG2JiYvj8888ZNWoUX3/9Nb///js3btxg//797Nu3D3g2QNvX15fQ0FBiY2Nxd3enf//+OgHh4cOHadOmDXFxccybN4933nlHfr7PP/+cM2fOEBQUxKVLl+TFMbOsWbOGb7/9lp9//pn79+/Tp08fevToQXp6urwUyI8//si8efM4e/aszuDy8pqhWgQ3giAIZZSrqytpaWl5TvUFmDZtGo8ePeLixYskJSXxxx9/ABSotaWsjdvQ19dHkiSuXbtGamoqGo2G+vXro9Vq6dKlC8bGxqhUKmbMmMGJEyfk1pUXDRs2DAsLC5RKJVOmTCEzM5O///5b3m9ra8v48eMxMDDAx8eHpKQkOa+Mo6Mjbdu2lVfiVqlUDBgwAFNTU4yNjZkzZw43b94kKipKPl+jRo3o378/+vr6DB48mCdPnnDz5k0AAgICmD59Oo6OjqjVaj799FOduq5atYrPPvuM6tWrY2BgwPjx40lNTeXs2bNYWFigr6+Pt7c3VapUQV9fH6VSKQ82L6/jpURwIwiCUEZpNBp69erF6NGj5W6mS5cuERcXp1MuKSkJExMT1Go1cXFxzJkzp8DPUdDxGKVl3IabmxubNm1i5cqVaDQaOnXqxOXLlwvUepUlMzOTGTNmUL16dVQqFWq1msTERDmLMGRv6cppW0pKCgCpqamMHTsWrVaLSqWSZzg9fz57e3v5Z4VCQaVKleS6RUVF4erqKu9//md4trTCoEGDUKvV8iM+Pl5eS0pPT48333wTrVaLk5MTWq2WGjVqlNvABkRwIwjFSqvVyk3ROTEzM+Pq1av5nmf27Nn07t27+ComlFubNm3C2dmZN998E7VazejRo0lNTdUpM2fOHEJCQrC0tKRly5Z06dKlwOc3NTVFX18/zzKlbdxG//79CQwMJDY2lgYNGjB48OBCtV5t27aNbdu28csvv5CYmEhCQgIWFhZFHle0ZMkSLly4wMmTJ0lKSpLH+hT0fI6Ojty+fVv+PWttqizOzs7s2rWLhIQE+fHo0SN8fHzkMvr6+piZmaFWqzEzMyuXXVHPKx2htiBUEFl3coJQXCwsLFi7di1r167V2f78YNnatWtz7tw5nf2jRo2Sf/b399fZ5+HhQUJCAvCs1efp06ccPnw41zqUpnEbwcHBhIeH06pVKwwNDTEzM8PAwKBQrVdJSUkYGhpiY2PDkydP+OKLL3Js4SmopKQkjI2NsbS0JCUlhenTpxfqeB8fHxYuXEirVq0wMTHhs88+09n/3nvvMWvWLKpUqULNmjVJSkoiMDCQdu3aYW5urlO2POcqep5ouREEQRByVJAMt/r6+qVqKZsnT57wySefoNFosLa25tixY/j7+xeq9Wro0KHUrVsXV1dXqlatSqVKlV5qRe1Jkyahr6+PRqOhXr16NG/evFDHz5w5kzfffJN69erRsGHDbK2648aNw8/Pjz59+qBSqahduzbbtm3Ldp7ExESCg4MJCwsr92tMibWlBKEIkpKSmD59Oj///DPx8fHUrFmTPXv20Lp1a8aOHcuePXu4du0ajRo1YuvWrXIqe4VCwaVLl2jYsCEA27dvZ+HChYSGhmJpacmcOXPw8/Nj9uzZXL58We7imjFjBvv37+fQoUPY2Ngwd+5cAgICSEhIoGXLlqxduxZHR0f5OdasWcPKlSsJDw/Hw8ODLVu2lOv+deHVSElJyXO6dBatVlvuMtyWN+VhjanCfH+LlhtBKIKsxGlnzpwhISGBb775hkqVKgGwdetWtm/fzr179zA1NeWTTz7J8Rw///wz48aN46uvviIhIYHz58/ToEEDnTIZGRmMGDGCU6dO8ccff8iZY0+dOsXJkyeJjo6mRo0aeHt76xz3/fffc+zYMcLDw4mIiOCrr756NS+EUK6VtZlSQs4KusZUeWrrEGNuBKGQshKn3b59W24tcXd3l/ePHTtWziMxcOBAORPsi1avXs0HH3xAu3btALCzs5PXBQJ49OgR//vf/zAyMuLw4cMYGRkhSRKrV6/m1KlTcv6KefPmYWpqyp07d+QWoqlTp8rn6tu3b57rBglCbsraTCkhZxVxjSnxjhSEQsovcdrzUzpNTU1zHYh4+/ZthgwZkuvzXL58maSkJP766y+MjIyAZ1NHHz58SJs2bXQGARoaGuoENwWtgyDkJSvDbV5fjKVtppSQXUVsgRPdUoJQSAVNnFaQ84SEhOS6v0WLFqxatYqOHTty7do1AKytrTExMeHs2bM60z5TU1Np0aJFns+XkpJCQkICKSkp5ar5uTSrW7cu+/fvL+lqFJlCoZBbCHNTmmZKCTmriC1wIrgRhEIqaOK0/Lz77rtyuvbMzEzu3r3LpUuXdMqMGDGCBQsW0L59e/7++2/09PQYPXo0kydPloOruLg4nXVoXpSamsqjR48qxAyJ0ubatWt07969pKvxUiwsLHB2ds72xVfeM9yWJ1ktcHkpby1wIrgRhCIoSOK0/PTu3ZulS5fy3nvvYWFhQZMmTXJM8Ofn58fixYvp2LEjly9fZsGCBTRv3lzOYdG4cWN+/fXXHJ8jKwHZiy01YjXniisjI6PQLXcNGjTgxo0bxZrh1sPDg2XLlhX5eKHgKmILnJgKLgjllCRJBAcH5zteokaNGuXqQ6000Wq1LFu2jEaNGjFixAguX75MRkaG3OWYlYY/LS2NDz74gO+//x4LCwtmzpzJO++8Q2hoKFqtFg8PD3r37s2ECROAZ+Ox3N3d5SBl69atLFy4kNu3b2NpacnQoUP57LPP5L+rQqFgxYoVrF27ln///Zf79+9nS+5WkOsozqzZL16T8OolJiYSHR2t85mgVCqxt7cvEy1wYiq4IAiFmiEhvFr5rUg9b948/vrrL65du8bly5fZu3dvoc5vbW3Nnj17SEpK4qeffuKbb77JlsRt27Zt/PrrryQlJZWr7geh4CwsLKhZs2aFWGNKBDeCUE5VxBkSpVV+K1Jv27aNjz/+GAcHBywsLLKt+pyfLl26yC1wDRs2xMfHh+PHj+uUmTp1Ko6OjhgZGaGnV/SP/tjYWBo1asTUqVM5f/48LVu2RK1WU6dOHbZv3y6Xu3TpEq1atcLKygpbW1t8fHxyHZeWkpJCr169sLOzw8LCgjZt2nDlypUi11HInUKhqBBrTIngRhDKqYo4Q6K0ym9F6qioKHkaP5BrmoHcHD58mBYtWmBjYyOvNfX8itNFOWdOQkJCaNWqFYMHD2b69Ol06dIFb29v7t27x5o1axg5ciSnTp0Cnq1EvXDhQmJjYwkKCiIyMpKPP/44x/NmZmbi6+tLaGgosbGxuLu7079/fzGrTygyEdwIQjlVEWdIlFb5rUjt6Oiok1rgxVWfzczMePTokfz789lmnzx5Qp8+fXj33XeJjIwkMTGR0aNHZwsMXqa1BuCvv/7Cw8ODOXPmMHHiRH755RdsbW15//33USqVtG3bFl9fXzZt2gQ8G4TcqlUrlEolGo2GSZMmZWtNyqJSqRgwYACmpqYYGxszZ84cbt68SVRU1EvVWai4RHAjCOVURZwhUVrltyK1j48PixYtIiYmhsTERObOnauzv1GjRuzZs4fExETu3r3LokWL5H1paWk8fvwYa2trjIyMOHv2bI6LJsKzYKqo+Y42bNiAm5sb/fv3ByAiIkIeEJ2latWqREREAM9aeXr16oWjoyMqlYpBgwZla03KkpqaytixY9FqtahUKvm8uZUXhPyI4EYQyjGRo6R0yG9F6pkzZ9KgQQPq1KlDw4YN6dq1K4CcmXrixIk4ODjg7OxMu3btGDBggHysubk5q1atYtSoUahUKj7//HOd/VmSk5NfakXoZcuWYWxsTL9+/UhPT8fJySnbopphYWHy6tmjR4+mcuXKXL9+naSkJLZu3ZprMLVkyRIuXLjAyZMnSUpKks8ruqWEohKd7YJQzllYWKBSqeTZUwYGBpiamooWm9fg6dOnGBoaUrt2bc6dO6ezb9SoUfLPxsbGrF+/nvXr1wNw5swZDA0N5WU0LC0t+fnnn3WOHz16tM7Pz//+ooSEBO7cuZNt8HhWviMg30DX2NiYH3/8kT59+tC3b1/Wr1/P+PHjWb16NaNGjeLMmTMEBARw8OBB4Flrlbm5OSqVijt37rB48eJcz52UlISxsTGWlpakpKQwffp0eZ8kSeK9KxSaaLkRhAqgosyQKE2io6O5e/cubm5u+Za9e/cugYGBPH36lKioKKZNm0bfvn2L5e9UnCtCGxsbs3fvXiRJYvjw4Rw4cICtW7dibW3NqFGjWLNmDa1atQJg6dKl7N+/H5VKRa9evejbt2+u5500aRL6+vpoNBrq1atH8+bNgZdvbRIqLpHETxAEoZgdPXqUfv36MXz4cJYsWZJv+ejoaLp06UJISAgmJiZ07NiR5cuXY21t/dJ1SUlJydZ9lBOtVluqVoROTEzMc/020a1a8RTm+1t0SwmCIBSzDh06EB8fX+DyDg4OXL58+ZXUpSzmOypoa5NKpRKtkEKORLeUIAjCC/z8/F5qWQCFQvHKgpXCKov5jkR2beFlieBGEAShHCuL+Y7KYmuTULqI4EYQBKEcK4v5jspia5NQuojgRhCECmPnzp00a9ZM/r1v3746X/yTJ0/m/fffB551jXh7e2Nubk7NmjV1sutu3bqVevXqYW5ujouLC5988kmes4127NhB/fr1UavVNGnShNOnTxf/xeWhrOU7KoutTULpIoIbQRAqDA8PDy5cuEBycjKSJHHy5EmMjY25ceMGAMeOHaNdu3bAs0Bo9OjRJCQkMHjwYPz8/OTzFGQV7iwHDhzgww8/xN/fnwcPHjBt2jR69OiR6yKSr0pZWhG6LLY2CaWLCG4EQagwNBoNNWrU4MSJE1y+fBlXV1e6d+9OYGAgDx48ICgoCA8PDwC6du2Kh4cH+vr6DBs2jNu3b8sBSUFW4c6yatUqpkyZQqNGjdDT06NPnz7UqlWLAwcOvKar/j9lKd9RWWttEkoXEdwIZdq9e/do164dKpUKpVJJixYt8j3G39+fhg0bvvrKCaWSp6cngYGBHDt2DE9PT9q3b09gYCCBgYHUr18fS0tLADk7MCB3f2St4l2QVbizhIWFMX36dNRqtfy4fPkykZGRr/hKy76y1NoklC5iNJZQpq1btw59fX0SEhJeetVjoWLw9PRkwYIFaDQaxo8fz1tvvcXo0aOxtbXF09Mz3+OzVuFevXo13t7eGBkZMWHChFwT5Tk7O/P+++/nuTyCkLus1iZBKIwS/zaIjIxk0KBBWFtbU6lSJd544w3++uuvXMsfP34chUKR7RETE/Maay2UFqGhodStW1cENkKBtW3blitXrnDmzBlatWqFWq3GycmJgIAAebxNXgqzCjfAe++9x+LFi7lw4QKSJPHo0SOOHDlCcHBwkVbnFgQhfyX6jRAfH0/Lli1RKpUcPHiQ69evs2TJErlZOC/BwcFER0fLDzs7u9dQY6E06devH5s3b2b16tWYmZnx7bff6nQ3LV26FBcXF8zNzdFqtWzYsEHn+Llz52JnZ4dGo2HZsmWvt/JCibGxsaFOnTrUqVNH7m5q3749jx49ok2bNvkeX9BVuLP06NGDhQsXMnLkSCwtLdFqtcyfP5/w8HCxXpIgvCIlurbUxx9/zKlTpzhx4kSBjzl+/Dienp7Ex8ejVqsL/Zxibanyxc/PD7VazbJly/D392fZsmVcvnyZmzdv0rBhQy5evEitWrWIjY0lNjaW+vXr4+/vz8iRI1m4cCHjx4/n1KlTdOzYkX/++adAixwKQlGJ9ZIEoegK8/1doi03P/30E2+++Sb9+vXDzs4Od3d31q9fX6BjGzZsiIODAx07duTUqVO5lktLSyMpKUnnIZR/+vr6SJLEtWvXSE1NRaPRUL9+fXm/jY0NkydPRqlU4uHhgVarLTXp8oXyqThX5xYEIW8lGtzcunWLNWvWUL16dQ4fPsyYMWMYP348mzZtyvUYBwcH1q5dy+7du9m9ezfOzs54eHhw8eLFHMsvWLAACwsL+eHs7PyqLkcoRdzc3Ni0aRMrV65Eo9HQqVMnneBFo9HolDc1NZVnwgjCqyDWSxKE16dEg5vMzEwaNWrE/PnzcXd3Z9SoUYwcOZK1a9fmekzNmjV59913ady4MS1atOC7776jRYsWfPXVVzmWnzZtGomJifIjryZhoXzp378/gYGBxMbG0qBBAwYPHlzSVRIqMLFekiC8PiUa3Dg4OFCnTh2dbbVr1yY8PLxQ52natCkhISE57jMyMkKlUuk8hPIvODiYI0eOkJqaiqGhIWZmZvmmc5ckiZSUFDGDRXglxHpJgvD6lOj/opYtWxIcHKyz7ebNm7i6uhbqPJcvX843VbdQsTx58oRPPvmE69evo6enR4MGDfD398+1/NOnT4mJidHJVWJgYICDg4MY4CnkqkuXLvTo0YOxY8fmWzZrvaS8WmbEekmCUDxKdLbU+fPnadGiBXPmzKF///6cO3eOkSNH8s033zBw4EDgWbdSZGQkmzdvBmDZsmVUqVKFunXr8vjxYzZs2MCKFSv49ddfad++fb7PKWZLCS8SM1iE10W81wSh6Arz/V2iLTdNmjRh7969TJs2jc8++4wqVaqwbNkyObABiI6O1ummevLkCZMnTyYyMhITExPq16/P0aNHC5RZVBBeVNAZLCqVqlSvwyOUDVmBS3R0tE4LjlKpxN7eXgQ2glBMSjyta/fu3bl69SqPHz/mxo0bjBw5Ume/v7+/zoJ0U6dOJSQkhNTUVOLi4ggMDBSBjVBkYgaL8CKtVsuCBQto0qQJpqamdOnShQcPHjB27FjUajXVq1fn9OnTwLNVxp9PAHnhwgXatWuHlZUVtra2vP/++/K+ixcv4unpSZUqVeTFOivSekmzZ8+md+/eJXa8ULGUeHAjCCVJzGARcrJz50727NlDVFQUd+7coVmzZnTo0IG4uDh8fX1zXCcqMjKSdu3a8fbbbxMVFcXt27fp378/8Kz1r2PHjowZM4Z79+6xb98+FixYwIULF0r96twlQSxuK7wsEdwIFVpOM1N69+7N77//nm85ofwaM2aMPP6la9euWFtb06dPH/T19RkwYABBQUE8efJE55itW7fSuHFjxo4di7GxMSYmJrRu3RqALVu20KZNG/r374++vj716tVj2LBhea5JJQhC0YngRqjQsmawPG/fvn20bdtW/l3MYKl4nk/yaGJiku33rAUwn3f79m2qV6+e4/nCwsI4cOAAarVafixfvjzf8V4lZenSpVSvXh1zc3Pc3NxYuXIlAPv378fOzk6u961bt7C0tCQwMBCAv/76i5YtW6JWq6lTpw7bt2/P9TlCQkLw8vLCysoKNzc3uXvv0qVLjB49mqtXr2JmZoaZmVmh04MIgghuhApNoVDkm0bA3t5edBsI+XJ1dc0135azszP/+9//SEhIkB/JyckcOHDgNdeyYFxdXTl27BhJSUls2LCBKVOmcOrUKbp37463tzdDhgwhLS0NHx8fxo4di6enJwkJCXTu3Blvb2/u3bvHmjVrGDlyZI7L42RkZNC9e3caNGhAVFQUe/fuZdGiRWzbtg13d3fWrl3LG2+8QUpKCikpKbi4uJTAqyCUZSK4ESq8rGU5slpwvLy8+O2337h37x7vv/8+1apVw9LSkm7dusl5cK5cuYK5uTkpKSnyeSIjIzEyMiIqKqokLkMoYQMHDuTcuXOsXbuWtLQ0Hj16JC8KPHjwYI4dO8bu3btJT08nPT2dy5cvc/78+VKZPLJv3744OzujUCjw9PTEy8tLntixePFi7t69S9OmTdHT02POnDkA/PLLL/IgaqVSSdu2bfH19c1xOZ2zZ88SHR3NvHnzMDY2pn79+owbNy7PXFSCUBgiuBEEngU4NWvWRKvVoq+vj52dHVqtlqlTp3Lnzh1u376NiYmJPJuvQYMG1KxZkx9++EE+x+bNm+nQoQOOjo4ldRlCCXJycuK3335j27ZtaDQatFqt/P6oXLkyhw8fZt26dTg4OKDRaHjvvfeIjo4mODiYsLAwIiIiCAsLIzg4mMTExBK9loCAABo1aoSVlRVqtZoDBw5w//594FnW9+HDh/P333/z4YcfyjcFERERaLVanfNUrVqViIiIbOePiIjA0dERQ0PDfMsKQlGIUZKC8P8pFArMzMzQ09OjUqVKVKlShSpVqgBgbGzMjBkzaNasGZmZmejp6TFixAj8/f3x8/MDYNOmTcybN68Er0AoDs9nqYZnU5Cfp9Vq5daV59NUwLOlYP74448cz+vu7s6vv/4q/56V0O/FmXgZGRlyor+SmB4eHh7O0KFDOXToEB4eHhgYGNC7d2/5mm/dusXs2bMZOXIkU6ZMoWPHjqhUKpycnLK9dmFhYTg5OWV7DicnJ6KiokhPT0epVGYrq6cn7ruFlyPeQYKQi3v37uHr64uzszMqlYo2bdqQlpYmrx7u4+PD+fPnCQ0N5cyZM9y/f5+ePXuWcK2FsqCgySNLoosqq2vMzs4OPT09Dhw4IAdlGRkZ+Pr68t577/HNN9/QuHFjeVp8165duXv3LqtXryYjI4MTJ04QEBDAkCFDsj1H06ZN0Wg0zJo1i7S0NIKCglixYgVDhw4Fng3ojo6OJjU1Ncf6laYuPKF0EsGNIORi2rRpPHr0iIsXL5KUlCTfkWd9oKrVav73v/+xadMm/P39GThwoE4zuyDkpjQnj6xTpw4zZsygXbt2WFtbs3PnTjlo/+STT1AoFHJr1vr16zl9+jSbNm3C0tKSgwcPsnXrVqytrRk1ahRr1qyhVatW2Z5DqVSyf/9+Lly4gL29PT179mTSpEn4+voC0K5dO5o1a0blypVRq9WEh4fz+PFjUlJSSl0XnlA6lejaUiVBrC0l5Eer1bJs2TK2bduGgYEBmzZtIikpiXfeeYd9+/YRHx+PWq0GIDAwkGHDhpGQkMDvv/9O/fr15S8uAwMDTE1Ny/xMq6SkJKZPn87PP/9MfHw8NWvWZM+ePRgaGvL+++8TGBhIpUqVGDx4MHPmzMHAwIAuXbrQq1cvRo8eTWJiItbW1nz44YcsXLhQbhU4dOgQjRs3LunLKxEJCQkFGl/i5OQkv9cqMrEmlwCF+/4WLTeCkIs5c+YQEhKCpaUlLVu2pEuXLtnKeHh4oK+vT9WqVdFqtaVycOjL8vPzIyQkhDNnzpCQkMA333xDpUqV8PX1RalUEhoayokTJ9i3bx+LFi0CwNPTU859cvz4cbRarfz733//zdOnT3F3d8/1Oct7htqCJoUUySNLdxeeUHqJ4EYQXvD06VMMDQ2pXbs2586dIyUlhX/++YdRo0YhSZLOnbRCocDV1RUfH588B4eW1QAnNjaWvXv38s033+Do6Iienh7u7u6kpaVx7Ngxli5dipmZGa6ursyYMUOeyuvp6SkPtj127BgffPAB//33H0lJSRw7doy2bdu+8kGjpXnJjJySR76oLCWPTE9Pf2XnLs1deELpJYIbQXhOdHQ0d+/exc3NrUDlz5w5w19//YWHh0ee5QpzZ1maFgi8ffs2RkZG2ZKoRUREYGxsLGfujY2N5dtvv+Xff//FxcWFPXv28PjxY2rVqsXu3bvp0KEDrVq14sSJEyxYsICnT58CzzLhuri4YG5ujlarZcOGDXlmqN2xYwf169dHrVbTpEkTeQFLeNaKNnXqVDp16oSpqSkHDx58Ta9S4ZWV5JGxsbH0798fW1tbXFxcmDFjBhkZGRw/fhy1Ws2aNWtwcXGhRYsWABw5coS33noLtVqNg4MDCxYsAJ7NwOrYsSO2trbZckblR6z/JhSFCG4E4f87evQoderUYdy4cdSsWTPf8p07d6ZLly588cUXVKpUKc+yZfXO0tXVlbS0tGzjHZycnHj8+DGxsbEA+Pr68vjxY9zc3Dhx4gQ//fQTDg4OVKlShXv37lG7dm3atWvH3r17uXfvHhMnTiQ4OJiZM2fyww8/EB0dzZ9//knTpk1zzVB74MABPvzwQ/z9/Xnw4AHTpk2jR48exMXFyfXy9/dn3rx5pKSk0KFDh9f6WhXWi8kjsyiVylIzhiSvrsfk5GSuXLnCP//8w++//86lS5fo1asXU6dO5d69e/zzzz94enoCkJmZyaRJk3LMGZUf0YUnFIlUwSQmJkqAlJiYWNJVEcq4O3fuSB06dJDMzc2l2rVrS+PHj5ccHR2lq1evSoGBgVKnTp0kS0tLyd7eXho5cqR07949SZIkKTk5WerZs6dka2srqVQqqXXr1tLly5clSZKkvXv3SkqlUtLX15dMTU0lU1PTkrxESZIkqVevXlLXrl2lqKgo6enTp9LFixel+/fvS56entKgQYOkmzdvSoBUq1Ytad68eZIkSVJAQIBkY2MjmZubS/r6+tKtW7ekq1evSkZGRpKhoaGUkJAgHT58WDIyMpKWLl0qnT9/Xrpx44aUkJAgSZIkbdy4UWrQoIFOPbp27SotW7ZMZ1uLFi2kzZs3S5IkSW3btpU++OCDV/56FLfMzEwpOTlZio+Pl5KTk6XMzMySrpIkSZIUEREhAVJMTIy8LSAgQKpevboUGBgoAVJ8fLy8b/To0dKwYcMKdO5Lly5JRkZG0tOnT/Mtm5mZKd24cUO6evVqro9//vmn1LxuwqtTmO9v0XIjCEXk6+uLq6sr//33H4sWLWLPnj3yvo8//hgDAwMOHTrEpk2bOHbsGCtWrACe3cX6+voSGhpKbGws7u7u9O/fH0mS6N27N9OnT6d79+5yq0VJ27RpE87Ozrz55puo1WpGjx5Namoq27ZtIzU1lSZNmgDId+3wLNtscnIyycnJNGrUiE2bNlGvXj0yMjJwd3fnzp07ODo6Mm/ePLZv346HhwfDhw/nyJEjuY5PCgsLY/r06TqLT16+fJnIyEi5TFlcgygreaRarcbMzKzEu6KyvNj1CLpZhM3NzXXGn+W1cGh+OaPyUla68ITSRQQ3glAEd+7c4cSJEyxcuBAbGxuqVatG//79gWfjFM6ePcuUKVMwMTHB0dGR0aNHyyskq1QqBgwYgKmpKcbGxsyZM4ebN2+W2jWpLCwsWLt2LZGRkSQlJXH27FmcnJywt7fnhx9+4Nq1awBMnDhRJ9usi4sLkiQxf/58Nm/ezOnTp7G0tGT9+vXyuTt37sx3333H8ePHqVmzJtOmTSMmJibHLypnZ2eWLFmis/jkw4cP+fjjj+Uyenp6pXKtprLoxa5HyDuLcF4Lh+aXMyo/ZaELTyhdRHAjCEUQFRWFsbExNjY28p1l1t1lbGwsRkZG2NjYyOUbNmwo3/GmpqYyduxYtFotKpVKXo8na+2esqZy5cp4enry4Ycf8vDhQ8LDw/n888/lbLPt27dHkiTGjh3LgAED5MAlNDSU06dP8/jxY5RKJSYmJhgYGJCeno6FhUW2DLXvvfceixcv5sKFC0iSxKNHjzh69KhOvpjU1NRyOR2/JOT3d33RyJEj2b59O3v37iUjI4PExET+/PNP4Fl+EhMTE9RqNXFxcfJim4UJQp9f/83JyQmtVkuNGjVEYCPkSAQ3glAEjo6OPH78WA5ILCws5C9ijUZDWloa9+/fl+8s79+/L9/xLlmyhAsXLnDy5EmSkpLkWSNZH+5lcV2drC4qV1dXWrZsSbdu3eQuKoVCwbBhw7hy5Qre3t7yMenp6axatQoPDw9at27N2bNn5bW5WrZsmS1DbY8ePVi4cCEjR47E0tKSKlWq8PXXX5OZmQk8my2TkJBQ7qbjl6S8/q4vatSoEbt37+bzzz/HysqK2rVr8/vvvwPZc0ZlzS4MDw8vVBBaWrvwhNJHZCgWhCJq1aoVderU4euvvyYiIoKuXbuSnp5OUFAQXbt2pXLlyqxfv54HDx7QrVs3vL29mTFjBlOnTuXs2bMcOHAASZKYOnUqa9as4dKlSzRs2JB169axdu1azp8/j76+frnIeLx582aWL1/O8ePHCzQFWKvVYmZmVuDzS5JEcHBwntOBlUolNWrUKJOvX3kisg0LRSUyFAvCa7Bt2zZu3bqFRqPB29ubQYMGYWRkhJmZGd9//z3p6elotdpsd7yTJk1CX18fjUZDvXr1aN68uc55+/Xrh0qlwsbGBgsLizLfxZKSksLy5csZM2bMK0teJxK9lQ2SyDYsvCai5UYQismCBQs4duwYR44ceelzlZe72y1btjB69Gg6dOjA7t27MTAweCXXJtZqKhuyFr7MT2Fb7oSKQbTcCMJrcPHiRf755x8kSeLChQusWLGCfv36vfR5y9Pd7eDBg3n48CE//vij3GLzKma+iERvr978+fPx8fHJdf+yZcvyzdQtsg0Lr4v4ny4IRXTv3j1Gjx5NbGwsdnZ2jBw5khEjRrz0eQvTxVJW724tLCxQqVTFNp4oq7srvzE3ZWWtptJo+vTpL30OEYQKr4t4BwlCEXl5eREaGlrs560od7dZM1+K61wODg55dneJRG8lTwShwusiuqUEoZQRd7dFIxK9/Z+cFiQFWLlyJc7OzlhbWzNjxgwaNmwor+Se04KtarVaXt39xf3Xrl2jWbNmmJub4+npWaAklCLbsPC6iE9HQShlxN1t0RV3d1dZdPPmTWbOnMnFixepVasWsbGxxMbGcuzYMWbMmMGhQ4do3Lgxc+bMISgoqEjPkZGRQc+ePfH29uaPP/7gwoULdOvWjfr16+d7bFaQGR0drfMeVyqV2NvbV6ggVHh1RHAjCKWM6GJ5OcXZ3VUW6evrI0kS165dw9XVFY1Gg0ajYcSIEQwcOFBOPTB79mxWrlxZpOc4c+YM9+/fZ/bs2SiVSpo3b86AAQO4ceNGgY4XQajwqoluKUEohUQXi1BUbm5ubNq0iZUrV6LRaOjUqROXL18mKioKV1dXuZxSqcy3iyg3UVFRODo6ymuJATrnLgiRbVh4lUTLjSCUUuLuViiq/v37079/f1JTU5k1axaDBw+madOm3L59Wy6Tnp6uk3LAzMyMR48eyb8/fPiQpKSkHM/v6OhIVFQU6enpcoATHh7+iq5GEApPtNwIQikm7m6FwgoODubIkSOkpqZiaGiImZkZBgYG+Pj4EBAQwNmzZ3ny5AmfffaZTsbmRo0acebMGf755x8eP37M9OnTc32/NWvWDCsrK+bOncuTJ084e/YsO3fuBAq3GKYgvCoiuCnjtFot+/btK+lqCIJQSjx58oRPPvkEjUaDtbU1x44dw9/fnw4dOjB37lz69u2Lg4MDmZmZ1KtXTz6uXbt2vPvuu7Ro0YJq1arxxhtvYG5unuNzKJVKfvrpJw4fPoyVlRUff/wxAwcO5NGjR2V+uRChfBDLL5RxWq2WZcuWZZvCKQiCkJ+GDRsyYcIE/Pz8Xuo8hV1SQ61Ws2/fvnwzGhfF8ePH6d27NwkJCcV+bqFkieUXhEJLT08v6SoIglAGlaflQoTyQwQ35cDNmzflZFpt27aV76BCQkLw8vLCysoKNzc3li1bJh/j7+9Pw4YN+fTTT7G3t8fb25sHDx7wv//9D0tLS9RqNY0bN5YHIKanpzNr1izc3NywtramZ8+eBUraJQhC+ZbXciFZN02va0V2cZMmZBHBTTmwdetWtm/fzr179zA1NeWTTz4hIyOD7t2706BBA6Kioti7dy+LFi1i27Zt8nFBQUEYGBgQHh7Oli1b+PLLL8nIyCAyMpK4uDi+/fZbuc99xowZnDp1ipMnTxIdHU2NGjXw9vYuqUsWhFJFoVBw+fLlkq5GoV2+fDnHLqmlS5dSvXp1zM3NcXNz08mHM2jQIBwdHVGpVDRu3JjAwEB53759+3j77bdZtWoVHh4eTJ06lczMTFasWEHVqlVxdHRk1apVcvm7d+9iaGioM4srLS0NS0tLzpw5k+/z5XST9qIff/yRypUrc/LkyZd6rYSyRQQ35cDYsWOpUqUKxsbGDBw4kAsXLnD27Fmio6OZN28exsbG1K9fn3Hjxsmp1uHZVOMZM2ZgaGiIiYkJSqWSuLg4/v33X/T19WnYsCFWVlZIksTq1atZunQpDg4OGBoaMm/ePE6dOpVnP7sgCNl5eHjotKKWRq6urhw7doykpCQ2bNjAlClTOHXqFADt27fnxo0bxMXF4e3tzdChQ3VaZUJCQtDX1+fIkSPMnz+fH3/8kR9//JGDBw8SEhLCX3/9RXJyMgB2dnZ06tSJrVu3ysf//PPP2NrayskGX3y+t99+Wz4est+kPW/9+vV88MEH/Prrr7Rq1eqVvV5C6SOCm3LA3t5e/tnU1JTk5GQiIiJwdHTE0NBQ3le1alUiIiLk3ytXroye3v+9BaZMmULr1q3p378/9vb2fPDBB6SmpnL//n0ePnxImzZtUKvVqNVq7O3tMTQ0FMGNIJRDffv2xdnZGYVCgaenJ15eXvIaU8OGDcPCwgKlUsmUKVOQJIn//vtPPtbMzIxRo0ahVCqpVKkSv/zyC4MHD6ZRo0aYmJiwcOFCMjMz5fJDhgzRCUq2bNnC4MGD5d9ffL7MzEz+/vtvef+LN2lZ5s6dy1dffcWJEyeoW7fuq3iZhFJMBDfllJOTk5xkK0tYWBhOTk7y788HNvDsQ+mLL74gODiYM2fO8Ntvv7F69Wqsra0xMTHh7NmzJCQkyI/U1FRatGjx2q5JEEqSVqvl888/p1GjRqhUKry8vHIcd3bp0iVatWqFlZUVtra2+Pj4EBcXB8DkyZM5ceIEH330EWZmZnTp0oWvv/4626yhHTt2UKdOHeDZMgndu3dnxIgRqFQqqlevzt69e+WyL7YEXb58WSc/TUBAgNzFVLlyZebOnZvvtQYEBNCoUSOsrKxQq9UcOHCA+/fvk5mZyYwZM6hevToqlQq1Wp1tqrednZ3OZ8vdu3epU6eOXCeNRoORkZG8v2fPnsTExHDu3Dnu37/PoUOH5OAmt+e7f/++fPyLN2kAqampLF26lAkTJuDs7Jzv9QrljwhuyqmmTZui0WiYNWsWaWlpBAUFsWLFCoYOHZrrMfv37+fmzZtkZmaiUqlQKpUYGBigp6fH6NGjmTx5stxSExcXx86dO5EkSSTtEiqMDRs2sG3bNmJiYrC3t2fQoEHZyujp6bFw4UJiY2MJCgoiMjKSjz/+GIAlS5bQunVrvvjiC1JSUjh48CBffvklZ86cITQ0VD7Hxo0bGTZsmPz7oUOHaNq0KQ8ePGDp0qX4+PjotJbk5uHDh/j5+fHtt9+SnJzMtWvX6Ny5c57HhIeHM3ToUBYtWsTdu3dJSEiga9euSJLEtm3b2LZtG7/88guJiYkkJCRgYWGBiYkJzs7O6Ovr6wQaWcuF3Lt3T9529+5d0tLS5N+NjY3p168fW7ZsYceOHbz11ltotVqAXJ/v+c+ZFwMbgEqVKnH06FFmzpzJjh078n2dhPJHBDfllFKpZP/+/Vy4cAF7e3t69uzJpEmT8PX1zfWYkJAQOnfujLm5OXXq1KF58+aMGTMGgAULFtC8eXPatWuHubk5jRs3Zv/+/QQHB4ukXUKFMWbMGGrVqoWJiQmLFi0iMDBQ7urNaiVt0KABrVq1QqlUYmVlxaRJk+QunZzo6+vTpEkTNm3aBEBkZCS///67TtdMjRo1ePfddzEwMKBHjx54enqyffv2AtVZqVRy48YNkpKSUKvVNGnSJM/yWTcpWS0wBw4c4NdffwWe5RkxNDTExsZGznKcNf7FwsJC7q52cnJCq9VSo0YNBg8ezKpVqwgODiY1NZVp06ZlC0iGDBnCjh072LhxI0OGDJG35/V8+WncuDGHDx/mgw8+YOvWreImrIIRa0uVcWFhYTq/9+7dW07oV6NGDflD6UV+fn7ZZklMmDCBCRMm5Fje0NCQmTNnMnPmTOD/kna9OAU0IyNDbt0RizsKZYFWq2XkyJHs3r2bkJAQmjdvzsaNG3F0dGTq1Kns3LmTBw8ekJaWppPPJWsF7OXLlwPP/k9NmTKFL774gszMTEJCQtDT08PIyIjMzEzc3NyIj48HoHXr1gD069eP8PBwoqKiOH36NNHR0Wi1Wjp16qQzlu7FRSldXV2JjIzM99pMTU35+eefWbJkCVOnTuWNN95g7ty5eHp65npMnTp1mDFjBu3atePp06f07NmTnj17AjB06FCOHj2Kq6srKpWKCRMm6HR1KxQK9PT0UKvV8rbhw4cTGhpK69at0dfXZ8aMGezevVvnOVu1aoW5uTnXr1+nX79+8vb8ni8/7u7u7Nmzh969e3P79m169eoFgIGBAQ4ODuIzqjyTSlhERIQ0cOBAycrKSjI2Npbq1asnnT9/Ps9jAgMDJXd3d8nQ0FByc3OTNm7cWODnS0xMlAApMTHxJWtecWVmZko3btyQrl69muvjn3/+kTIzM0u6qoKQL1dXV0mr1Uo3btyQHj58KA0ZMkTy9PSUJEmStm7dKsXGxkoZGRmSjY2NpK+vL926dUuSJEnas2ePBEgDBw6UAOn06dPSxo0bJUBq0qSJFBsbKz18+FAaP368pFAopL///ltKTU2VnJ2dJY1GI6Wnp8vPv3v3bsnZ2VkKDAyUatasKe3Zs0eu36effirVrl1bp85dunSR5s6dK0mSJHXr1k36/PPP5X0HDhyQcvpof/LkibRs2TJJpVJJGRkZUnJyshQfHy8lJyeX2/+rCQkJeX5OJSQklHQVhUIozPd3iXZLxcfH07JlS5RKJQcPHuT69essWbIES0vLXI8JDQ2lW7dueHp6cvnyZSZMmMA777zD4cOHX2PNK7a8knZleV1JuwShOOTW3TRw4EDs7OzQ19fH1NQUfX19fvjhB1JTU/nmm28A5BwwlSpVAp51M3l4eGBjY0NcXBybNm3CyMiIN954A2NjY5o1a8aDBw84d+6c/Px6enoMGzaMCRMm8ODBA7p3765Tv5s3b7J+/XoyMjL45ZdfOHbsGAMGDACeLXi5Z88eEhMTuXv3LosWLZKPi42NZe/evSQnJ2NgYIBKpUJfX79CdCdLInNyhVaiwc0XX3yBs7MzGzdupGnTplSpUoVOnTrh5uaW6zFr166lSpUqLFmyhNq1azNu3Djefvttvvrqq9dY84otv8CmsOUEoaQ93+2TNZsnMjKSr776irp162JhYUF4eDjp6eksX74cjUbD/fv3MTU11emCyTrXL7/8gkqlolevXhgZGemMMZk8eTIKhYKOHTvqBDHDhg3j77//ZtCgQSiVSp1zdu7cmT///BMrKyt5DEn16tUBmDhxIg4ODjg7O9OuXTs56IFns42+/vpreW2n5cuX8+WXX+pMxYb/604uTwGOuAmr2Eo0uPnpp59488036devH3Z2dri7u7N+/fo8jzlz5gwdOnTQ2ebl5SVnsxRePQODgg3VKmg5QShpz2fIzZrNk56ezuzZs9m8eTPx8fG4uLjg6urK5MmTSUpKYvHixfJ7XJIkGjZsCIC5uTnXrl0jJSWFixcvUr9+fWbNmiWf393dHSMjI44cOcL+/fvlwMfOzg4TExOGDx+erX4GBgZ8++23JCUlERISwttvvy3vs7S05OeffyYpKYmgoCBGjx4tt0Y4ODhw/PhxEhISSExMZPv27TRt2jTX16E8tWSIm7CKrUSDm1u3brFmzRqqV6/O4cOHGTNmDOPHj5dnDeQkJiYGjUajs02j0ZCUlERqamq28mlpaSQlJek8hJdjamqab+CiVCoxNTV9TTUShJezbt06eTbPRx99RJs2bUhKSkJfXx9bW1syMzNJSUkhPDy80OceNGgQK1eu5Pr166SlpTFz5kwqV64sBxkajYaQkBBWrFiBu7s7Wq32lczqqWgtGeImrGIr0b9qZmYmb775JvPnzwee3dEEBQWxdu3aPPOxFMaCBQuYM2dOsZxLeEahUODg4JBndmJ7e3udRGJC+aPValm2bJk8O68sGz58OD4+PoSEhNCsWTMCAgJwdHTk7bff5o033sDIyIj09HRq1apV6HMPGTKE2NhYunfvTnx8PE2bNuXnn3+Wv1Q/+ugj+vTpA0DHjh11ZkAaGBjw+PHjYrnGitaSkXUTltf1iJuw8kshlWAbpKurKx07dmTDhg3ytjVr1jBv3rxcpzm2adOGRo0a6WTk3LhxIxMmTMixvzgtLU0nYVRSUhLOzs4kJiaiUqmK72IqoMTERKKjo3U+PJRKJfb29mKKZQVQXoKb0nAdWakVcpM1ZuZlpKSkZEsdkROtVouZmdlLPVdp8TpeV+H1SUpKwsLCokDf3yXactOyZUuCg4N1tt28eTNbTofnNW/enAMHDuhsO3LkiLzI2ouMjIx0Un0LxcfCwgKVSiU3dxsYGGBqaipabCo4f39/li1bVqyrZPv5+aFWqwu94GR6enq2wbmlTUFn9ahUqpf6v1URWzKyAhdxE1bxlOiYm4kTJ/Lnn38yf/58QkJC2LZtG9988w3vvfeeXGbatGk6GStHjx7NrVu3mDp1Kv/88w+rV6/m+++/Z+LEiSVxCRWeQqHAzMwMtVqNmZmZCGwqqNjYWBo1asTUqVM5dOgQN27cwNzcHDc3N3mqNDxLOqlQKNiyZQvVqlVDrVbj5+enswbaDz/8QLVq1bCwsGDkyJF0795dDpRSUlLo1asXdnZ2WFhY0KZNG65cuSIfm7UO05gxY7CysuLjjz8mPT2dadOm4eLigq2tLQMGDNBZDiAjI4M5c+ZgZWWFm5vba1+x+3WNhcnqTs5LeexOtrCwoGbNmmi1Wp3MySKwKd9KNLhp0qQJe/fuZfv27dSrV4+5c+eybNkyBg4cKJeJjo7WGcRXpUoVfvnlF44cOUKDBg1YsmQJGzZswMvLqyQuQRAqvJCQEFq1asXgwYNZtGgRNjY2VKtWjaSkJDZs2MCUKVM4deqUzjEHDx7k0qVLXL9+nd9++42AgADgWcvt4MGDWblyJXFxcTRt2pRDhw7Jx2VmZuLr60toaCixsbG4u7vTv39/nYG3hw4d4q233uLu3bvMnTuXBQsWsH//fk6ePEloaCgKhUL+jMnIyMDMzIyOHTsSFRXF3r17WbRoEdu2bXsNrxxyHYqzXF4sLCxwdnbONog2aw2o8vqFL27CKp4SHXNTEgrTZycIQu60Wi3Vq1cnMDAQpVKJRqNh5syZGBgYsGzZMvr27cuKFStISkqiffv2/PLLL4SFhVGlShUaNWpEaGgo+vr6qFQqPD092bBhg3yDM2LECC5fvsypU6ewtrbGyspKXv06IyODd955h4iICDZu3IiLiwsRERFUrlyZ2bNns2/fPp0userVqzNv3jw5/0tUVBSVK1cmMjKS0NBQunbtyr179zA0NARg/vz5HD9+PNelS4pbSYyFkSRJdCcLZU5hvr/FwpmCIBRJeno6R48exd3dneTkZM6ePUvTpk05c+YMV65cYcGCBaSlpZGRkcGhQ4d0VrGeN2+evGr248eP+eOPP4BngYeRkRH+/v7MmzePlJQUnRlK9+/fx83NjV27dnH27FneeOMNeXsWFxcXnXpGRETIq0wDODo6YmRkREREBBERETg6OsqBDUDVqlXlxTBfh5JIrSBaMoTyTgQ3giAUmVKpJD09nT59+mBlZYVarWb9+vVYWVmRlJREYmIi3bt3x9zcXKc1pXnz5nJrT6NGjYiJiQGeBR5paWn4+vrStGlTFAqFPHPy/v37uLu7k5aWxo0bN0hOTpZbPJ5vgH5xxWknJyedlpGYmBjS0tJwcnLCycmJqKgonTE/YWFhhVqc8WVV1LEwgvAqieBGEIQiUSqVfPDBB1hYWHDgwAEcHR35888/AbC1tUVPT48DBw7w66+/olQqSU5Olo/19fXF0dERlUrF4cOH5XQN/fv3Jz4+ntTUVDIyMvjuu++4efMmAEePHiUuLg6tVou1tTUpKSlMnz4933oOGjSI+fPnc+fOHVJSUpg0aRIdOnTA0dGRpk2botFomDVrFmlpaQQFBbFixQqGDBlCSkrKK0mml5OKOhZGEF4VEdwIglBkLVu25Pfff+f+/fuoVCpGjBhBly5dCAkJwdramp07d9KzZ89sxzk4OHD9+nWSkpLw8vKSg4eaNWtSq1Ytdu/ejbW1NWfOnKFdu3bo6+vj7e3NmDFj+Pvvv7G1taVevXq5poB43rRp0/Dy8qJ58+ZotVrS09PZunUr8Cx42L9/PxcuXMDe3p6ePXsyZswYGjdu/NoXlhSzegSh+Ii804IgFMnhw4cJDw8nNTUVc3NzhgwZwr59++jXrx+RkZE63VBZ6y5ptVqaNGmCjY0NKpWKO3fu8ODBA0xMTOSydnZ2jBo1igkTJgDPAp6sJVeWLFmChYUFW7Zs4eeff8be3p7k5GR5PMrs2bOz1dPQ0JBFixbprJb9vBo1asiDh7OSvj19+lSnTNbCksArDTayxsIIgvByRMuNIAhF8uTJEz755BM0Gg3W1tYcO3YMf3//fI9bunQp+/fvl1fN7tu3r87+uLg4Hj9+TFpaGkuWLCE6OprKlSvL+z/44AO6d+9Ox44dOXfuXLG1rBQ0mV4Fm2AqCGWSmAouCEKpMm7cOLZt20Z6ejo1a9bkyy+/xMPDA3i16fQr4vIEglCWlJnlFwRBEF60cuVKnazGWV71MgUVbWFJQSjPRLeUIAhlwqtepiC/XDOFLScIQskp8P/S5cuXF/ik48ePL1JlBEEQcvOqW1Yq4sKSglBeFTi4+eqrrwpUTqFQiOBGEIRi96pbVrKS6eU1pqc0JNPbt28fEyZMKND4oOKm1WpZtmwZvXv3fu3PLQiFUeBPgdDQ0FdZD0EQhDy9jpaVrMHI0dHROs+jVCqxt7fPNlhZoVBw6dIleap7WZK1zld8fDxqtbqkqyMIxUp0HguCUCa8rpYVCwsLVCrVa1lYMj09HaVSWeznFYSKrsADiidNmiQP1Js0aVKeD0EQhFfhdS1T8PzCkvXq1WP+/Pk0atQIlUqFl5cXUVFRNG3aFIAWLVpgZmbG/PnzAfjvv//o0aMHtra2uLq6Mm/ePDIzMwHw9/enYcOGfPrpp9jb2+Pt7Y0kSSxZsgQ3NzesrKzo3Lkzt27dkusSERFBp06dUKlUNG7cmOvXr+vUdenSpVSvXh1zc3Pc3NyyzTT7999/6dmzJ7a2tlhZWdGnTx8Auf5OTk6YmZkREBCAu7t7tlxFnTt35osvviiW11UQXpcCt9xcunRJXlzu0qVLuZYr6f5oQRDKt9fZspJlw4YNHDx4EBcXF8aMGcOgQYM4d+4cCoWC06dPy91Sjx49on379kyYMIHdu3cTExND165dcXBwYMSIEQAEBQXRt29fwsPDycjIYMuWLSxdupRDhw5RvXp1ZsyYQY8ePbhy5QoGBgb4+vpSpUoVYmJiCA8Pp0uXLjp1c3V15dixYzg5OXH8+HG6du2Ku7s7LVu25OHDh3To0IGBAweyfft2lEolp06dAuDcuXNUqVKFiIgIuVsqPj4ef39//Pz8AIiMjCQwMJCNGze+stdWEF4JqYJJTEyUACkxMbGkqyIIQhng6uoqffHFF/LvMTExEiDduXNHAqRLly7J+77//nupYcOGOsd/8803Urt27SRJkqSNGzdKVlZW0tOnT+X9HTp0kBYuXCj//vjxY8nc3Fw6deqUFB4eLgFSbGysvH/hwoWSq6trrvXt1auXNG/ePEmSJGnHjh2Sm5ublJmZma1caGioBEjx8fHytgcPHkiVKlWSbt26JUmSJM2fP1/q1q2bzmuxd+/eXJ9bEF6lwnx/FynPzdatW3n06FHxRViCIAilmKurq/yzRqPByMiIyMjIbOXCwsIICgpCrVbLj8mTJxMTEyOXqVy5Mnp6//fRGxERgVarlX83MjLC0dGRiIgIoqKiMDY2xs7OLse6AAQEBNCoUSOsrKxQq9UcOHCA+/fvA3D79m3c3NwK3KplaWlJr1692LRpEwCbNm1i+PDhBTpWEEqTIgU3EydOxM7ODl9fXw4cOJBtkTlBKC327dun88VRVAqFQmchSKFiuX37tvzz3bt3SUtLo3LlytmCBmdnZxo3bkxCQoL8SEpK4tq1a3KZ5wMbeDbm5flp3U+ePCEqKgonJyccHR15/Pgxd+/elfeHh4fr/Dx06FAWLVrE3bt3SUhIoGvXrvL6V66urvz33385rof1Yj2yjBgxgs2bN3P69Gni4uLo0aNHtjKSJJGSkkJCQgIpKSlivS2h1ClScBMdHc2OHTtQKBT0798fBwcH3nvvPU6fPl3c9RMEQShx69atIzg4mNTUVD766CPatGmDk5MTGo2G//77Ty7XvXt3YmNjWb16NY8fP+bp06cEBwdz/PjxXM89aNAgVq5cyfXr10lLS2PmzJlUrlyZpk2b4uzsTMuWLfn4449JTU0lODiYdevWycdmBRZ2dnbo6elx4MABeYVzgG7dupGWlsasWbN4+PAhT548ITAwEABbW1v09PR06g/Qvn17JEli7NixDBo0KNtsrocPHxIcHExYWBgRERHFtnCpIBSnIgU3BgYGdO/enYCAAO7evctXX31FWFgYnp6euLm5FXcdBUEQStTw4cPx8fFBo9EQGRlJQEAAAHPnzmX8+PFYWlqycOFCzMzMOHr0KL/99htarRZra2t8fX11uqVeNGTIEN5//326d++Ovb09V65c4eeff5ZnhG3bto07d+7IreXPdxPVqVOHGTNm0K5dO6ytrdm5cyc9e/aU92fV58KFC7i4uODg4MCqVasAqFSpEp9++ildunRBrVazbds24Fkr5bBhw7hy5QrDhg3TqWtmZib379/PlmsoIyODO3fuiABHKDWKZVXw+/fvs2PHDtauXcuNGzdKdTeVWBW8fIuIiGD48OH8+eefVK9enb59+/LNN98QFhaWLeHasmXL2Ldvn3xXHRMTw4cffshvv/1Gamoq9evX5/Dhw1SqVEnn2Js3b9KlSxcmTpzIuHHjSu5ihdeiImbl3bx5M8uXL+evv/6St0mSRHBwcL5JFGvUqCFmzQqvxGtZFfzRo0fs3buXgIAAfvvtN5ydnfHx8eGHH34o6ikF4aXlN202N5mZmfTo0YO6dety/fp1zM3N+fPPP7ONSzh37hx9+vRh6dKl9O/f/1VcgiCUqJSUFJYvX86YMWN0thdm4VIzM7NXWUVByFeRuqW8vb2xs7Nj4sSJVK1alePHjxMSEsLcuXOpVatWcddREArkzp07nDhxgsWLF2NiYkKtWrUYPXp0gY49f/48N27cYM2aNVhaWmJgYECrVq0wMjKSyxw6dIjevXuzefNmEdgI5dKWLVvQaDRUrlyZoUOH6ux71QuXCkJxKlLLjb6+Pt9//z1eXl7o6+sXd50EoUgKMm02N7dv36Zy5cpUqlQp1zLLli2jffv2tGvX7qXrKpQdJbFAZUkZPHgwgwcPznHfq164VBCKU6Fabrp27UpiYiIBAQF07dqVxYsXk5CQIO+Pi4ujTp06xV1HQSiQ/KbNmpqa6uRnio6Oln92dXUlMjKSx48f53r+bdu2cePGDd5///1irrkglH5ZC5fm5WUXLhWE4lKo4Obw4cOkpaXJv8+fP58HDx7Iv2dkZBAcHFx8tRPKJDMzM65evfranze/abONGjViy5YtZGRkcPnyZbZs2SLva9KkCTVr1mTs2LEkJCSQkZHByZMndd7vVlZW/Pbbb5w5c4bRo0eTnJws8nwIFUbWwqV5KY6FSwWhOBQquHnxA1x8oAvwbDbJvn375N9TUlJ44403SqQueU2bXbFiBWfOnEGtVvPRRx/pjCnQ09Pj559/5tGjR9SsWRMbGxtmzpwpL3iYxdLSkt27d3PmzBmGDRvGnTt3RJ4PocJ4XQuXCsLLKtRUcD09PWJiYuQxDebm5ly5coWqVasCEBsbi6Ojo5gKXsG8zqmyWQsllpTExETu3LmT637xAS9UBJIkvdaFSwUBCvf9XaiWG4VCke0NLN7QFVu/fv0IDw/Hx8cHMzMzRo8erbNUQWZmJjNnzkSj0eDo6MiqVatQq9U6GVt37NhB/fr1UavVNGnSRCfTtYeHB1OnTqVTp06Ymppy8ODB13yF/0eSJJ1xOjmJiYkRLZpCuadQKDAzM0OtVmNmZia+B4RSp1C3wJIk4efnJ0+Pffz4MaNHj5YHkD0/PkGoGHbt2pWt5eb5cS4bN24kICCAEydO4OzszLhx40hOTpb3HzhwgA8//JCffvqJhg0bsm/fPnr06MHNmzextrYGwN/fn/3799OkSZM8B/y+aiLPR/GoiEnxBEF4vQrVcjN06FDs7OywsLDAwsKCQYMG4ejoKP9uZ2fHkCFDXlVdhTJo27ZtvPfee9SoUYNKlSqxcOFCnXEsq1atYsqUKTRq1Ag9PT369OlDrVq1OHDggFzG19eXpk2bolAo8pyq/aqJPB+CIAhlQ6FabjZu3Piq6iGUU1FRUTg7O8u/29raYmxsLP8eFhbG9OnT+fTTT+Vt6enpREZGyr+7uLi8nsrmQ+T5EARBKBuKlKFYEJ734hIFz3N0dNQZgHvv3j2driVnZ2eWLFlCQkKC/Hj48CEff/xxgc7/Ook8H8Xn2rVrNGrUCJVKhZeXF1FRUfL6X8/nzpowYQJ+fn4A8v4tW7ZQrVo11Go1fn5+pKenA3D8+HHUajUbNmzA2dkZa2trpk6dCjwLmDUaTbbVuWvXrs3OnTsLVOesWYEBAQG0aNHipV+Dgrh8+bIYzyIIRVA6vjWEMk2j0fDff//luM/Hx4fVq1cTEhJCamoq06dP1wlW3nvvPRYvXsyFCxeQJIlHjx5x9OhRIiIisp1LkiRSUlJKLLeMyPNRfDZs2MC2bduIiYnB3t6eQYMGFfjYgwcPcunSJa5fv85vv/0mr9ANkJyczPXr1/n33385efIkq1at4vjx4yiVSgYPHoy/v79c9syZM8TGxhZ67M/AgQN1Br0LglD6iOBGeGnTp09n5cqVqNVqxo4dq7Nv+PDheHt706JFC9zc3GjYsCHGxsbyoPQePXqwcOFCRo4ciaWlJVWqVOHrr7/Oll8mMTGR4OBgwsLCiIiIKLHcMiWd56OkEiQWtzFjxlCrVi1MTExYtGgRgYGBOQa0OZk1axbm5uY4OjrSuXNnLly4IO+TJIl58+ZhbGxM7dq1adGihbx/xIgR7N69m5SUFODZQHVfX1+d9cNKUlYLlCAIL08EN8JL69GjB6GhoSQkJLB69WokSaJhw4bAsy6l+fPnc/fuXaKioujTpw+PHj3SGUfTr18/Ll68SEJCArGxsfz888/y/uPHj8vJ8l4cqJuRkcGdO3dKJMCpWbMmWq2Wrl278sMPP8iD67t06cKDBw8YO3YsarWa6tWry3f5ycnJjBo1CgcHBxwcHBg9ejQPHz6Ur1OtVus8T+/evZk9ezYADx484H//+x9KpZLWrVvTuHFjbt++DTz7Upw1axZubm5YW1vTs2dPoqKiXtvrURTPr/ml0WgwMjJCqVQW6Fh7e3v5Z1NTU53ZdyqVChMTkxz3165dm3r16vHDDz/w+PFjdu7cqZPksaD8/f3l9/fu3bsxMzOTH5UqVZJb7sLDw+nYsSO2trZYWlrSrVs3nXWq/Pz8GDFiBP3790elUrF27VoSEhLo378/arWaWrVq8ccffxS6foIgiOBGeMUyMjLYt28f6enpxMfHM2HCBFq0aEHlypULdHxpzS2TledDT0+Pffv2sWfPHqKiorhz5w7NmjWjQ4cOxMXF4evrK69M/sEHHxASEkJQUBBXr17ln3/+YeLEiQV6vi+//JKMjAwiIyOJi4vj22+/xdzcHIAZM2Zw6tQpTp48SXR0NDVq1MDb2/uVXXtxyArMAO7evUtaWpr8nsht/a/iMGLECPz9/dm7dy+urq40atTopc7Xt29fUlJSSElJITExkbZt28oLT2ZmZjJp0iTu3LnD7du3MTExYeTIkTrHb9++nREjRpCQkMCIESMYP348CQkJhIWFcezYMTZv3vxS9ROEikoEN8IrJUkSCxcuxNraGjc3Nx4+fMi2bdsKfHxhcsuUlDFjxshdUl27dsXa2po+ffqgr6/PgAEDCAoK4smTJwQEBLBgwQKsra2xsbFh/vz5bN68OVsXnFar5fPPP+f48eMsWLAALy8vHj9+TFxcHKamply9epWGDRuiVquZMWMGixcv5urVq+zZswc7Ozs6derEqVOn8sykXNLWrVtHcHAwqampfPTRR7Rp0wYnJydcXFzYtGkTmZmZBAYG6qQEKA4DBgzgwoULLFy4UG61Ka6xXOPHj+fRo0ds2LABePZ37NKlC8bGxqhUKmbMmMGJEyd0/t6dOnXCy8sLPT09jIyM2LlzJ/PmzUOtVuPo6MiUKVNe/qIFoQISc1aFV0qpVPLnn38W+fiykFtGo9HIP5uYmGT7XZIk4uPjefLkCVqtVt5XtWpV0tLSuH//frZzbtiwgcaNG9OsWTMiIiK4cOECrVu35syZM7Rv355BgwZRq1Yttm7dCjxLqDlx4kTS09Pp06cPhoaG3LlzR2cafmkyfPhwfHx8CAkJoVmzZvKg4O+++44xY8Ywf/58unXrhre3d7GORTE3N6dfv34EBAQwcOBAEhMTiY6O1nn/GBgY4ODgUKjxU8uWLWPr1q0oFApcXFw4d+4ctWvXpmvXrvz5559y12laWhrJycnyudVqNQqFgvj4eNLS0njy5IlOl93zPwuCUHAiuBFKtfKSW0apVGJoaEhYWJgc/ISFhWFkZISNjQ1mZmakpqbKrQZjxoxh9+7dKJVKFi1ahL29PQEBASxatIjNmzfz0UcfceDAAcaNG8fs2bM5d+4c1tbW2NnZsX//fjw8PErwavOWNe5kxowZ2fa1b9+emzdv5nicVqvN1qqybNky+WcPDw+daeSAzoKuz5+nR48eKJVKVqxYwdatW/nhhx/k/VljuYACBTj79++Xx0bdvn1bPsbHx4f79+9z8eJFbG1tuXz5Mu7u7jrX8PzMQRsbG5RKJbdv35bfI+Hh4fk+vyAI2YluKaFUKy+5ZfT09PD19WXGjBk8ePCAuLg4pk+fzuDBg9HT06NGjRoolUq5yy4iIoJLly4Bz1qGlEolZ86cAZ69JkqlkqSkJFxcXBg9ejSTJ0/m8ePHGBsbk5iYyI4dO0p02nxpde/ePdavX8/o0aOLZSzXlStXGDJkCGPHjqVKlSo6wVBSUhImJiao1Wri4uKYM2dOnufS19enf//+zJo1i4SEBKKioli8eDFQ8mkQBKGsKd23u0KFl5VbJq/xI2Ult8zXX3/NpEmTqFOnDgA9e/ZkyZIlwLNZPuvXr2fq1KlERkby119/4eXlBTwbcJuens7kyZMB6NOnD97e3ty4cYM7d+6wYMECFi1aRNu2bXn8+DEjR46kdevW1KtXT37uonS1lDddu3bl0KFD6OvrM2LECN5++21WrlxJRkYGTZs2BeDHH38kISGBBQsW8N9//6FUKunYsSMrV66Uz7Nw4UIePHhAp06dCAwM5OnTpyxYsAB49n41MDDg33//ZdeuXTRq1AhLS0ssLS3lgKR69eq8/fbbOnX7+eefmTNnDvfu3cPS0hJXV1ccHBwYO3Ysf/31F8HBwS/ddSYIFYlCqmC3AIVZMl0oPXIaG6FUKrG3ty93H/BarRZ9fX0OHDiAi4sLY8eO5datW/z+++8oFAouXbpEw4YN2bBhA/Pnz+fXX3+lcuXKjB8/nu+++44NGzbQpEmTHM/9OnLxlEY3b96kYcOGXLx4kVq1anHz5k2uXr3KjRs3snVLBQcH8/DhQ9544w1MTU155513qFmzJuvXrweedX9dv35dZzHXnTt3smzZMi5fvgw863qrUqUK8fHx8uDgL774gsGDB/Pw4UOuXLlCixYt5HI+Pj6sW7eO5ORk3nrrLebOnYufnx+JiYl5BvYV9e8pVEyF+f4u0W6p2bNno1AodB61atXKtby/v3+28s+vUySUX8/nlnFyckKr1VKjRo1y+8GeNeBWo9EQGRmpk4X3+TLPJ0hs0KBBvvliSmLafGmgr6+PJElcu3aN1NRUHB0dqfn/2rvzqCiufA/g3wYakKVBWbpBGggqoKjgEmUxBo3RuDwxTsQoKknUkSQmeuIWx/jiMmpGM45zonGbMfgE3ugkymTUuEueI24Jo0fBEFCaTcCEAE2zC/X+SKyxI/vSG9/POX2OXfdW8asrND9u3cXPr9G6fn5+GDp0KKRSKdzd3fHee+89tW1DWzdzlUqlyMzMxA8//ABbW9untm9obGFCQ10GgcgY6H3MTUBAAAoKCsTXv/71r2bry2QyrfpPrpdBpu3x2jKOjo6ws7MzikdR7RUQEICUlBSo1WqcOXMGHh4eANDsAomjR49GVVVVs1tEdOW0eYlEIvZcGJo+ffrg4MGD2LlzJ+RyOaZPn46MjIxG6+bk5OCdd97B2LFj4e7ujjlz5jw1o62tm7keO3YMd+7cgZ+fH4YMGYIjR45olTe2MKExLINAZKj0ntxYWFhAoVCIL2dn52brSyQSrfpPTrsl6k6eXCDxp59+wqpVqxAUFNTiz0R7ps0LgoD6+vr2hmoQIiMjcfHiRRQVFSEwMBAffPBBo5uybtiwAXK5HDdu3IBarUZcXNxTvSNt3cx16NCh+OKLL/Djjz9i7dq1mD17NoqKipo9xxiWQSAyVHpPbjIyMuDu7g4fHx9ERUW1OPVRo9HAy8sLSqUSERERSE1N1VGkRIblyQUS+/bti8rKSvzhD39o8bzWTpv39vbGli1bEBwcDBsbG2zevBkDBw6Evb09PD09sXbtWvGX/uMBuaGhobCzs8PmzZvbf2NdID09HWfPnkVVVRUsLS1hZ2cHS0tL+Pv748cff9Taqb6yshIKhQJKpRK5ubnijKX2qq2txaFDh1BSUgIzMzNxm42m/h8EQUBdXZ1WTM0x9GUQiPRBrz8VI0eORGxsLPz8/FBQUID169fjueeew507d8Sl5Z/k5+eHAwcOYPDgwSgrK8PHH3+M0NBQpKamit32v1ZTU4OamhrxvVqt7rL7IeoMT+4/1JwnF0gsLS1t1caT5ubmbZo2Hxsbiy+//BJ9+/bF8ePHMXPmTPTr1w+3bt3ChAkT4O/vj6ioKFy/fh0SiQTJycniYzNDUltbi7Vr1yItLQ1mZmYIDAxEbGwsBgwYgLCwMIwfPx719fW4evUqdu3ahZiYGMhkMvj6+mLOnDkd/iMqISEBS5cuRW1tLTw9PZGQkAAnJyetfbGAnwfOl5SUQKPRNLq4468ZwzIIRPpgULOlSktL4eXlhe3bt2P+/Pkt1q+rq0P//v0xa9YsbNy4sdE669ata3R9Cc6WIlOi0WhalRS5uLi0+lGut7c3li5diqVLlzZavnTpUlRUVIiziJ6cyUVt19LMqMZwthR1J0YzW+rXHB0d4evri8zMzFbVl0qlGDJkSLP1V69ejbKyMvFlyPvtELVXaxY7NDc3h6ura5uu++TA2dOnTyM0NBTOzs5wcHDAnj17WtW7QC1rzcyoJ0mlUiY2RM0wqORGo9Hg3r17zc72eFJ9fT1u377dbH0rKyvIZDKtF5GpebzYYXPc3d3bPMPs8cDZ2tpaTJ8+HYsWLUJ+fj7KysoQExOjNdDWlGevdbXWzIwCft6iwdSXQSDqDHpNbpYvX46vv/4aKpUKycnJePnll2Fubo5Zs2YBAObNm4fVq1eL9Tds2IAzZ87g/v37SElJwZw5c5CdnY0FCxbo6xaIDIaDgwOUSuVTPTid8Vd+TU0Nqqur4eTkBCsrK1y7du2p3d3lcjnu3bsHgNsFtFVrZzxZW1ub/DIIRJ1BrwOK8/LyMGvWLBQXF8PFxQWjRo3C1atX4eLiAuDn9SaenHJZUlKChQsXorCwED179sSwYcOQnJwsLmdP1N05ODhAJpOJPQEWFhawtbXt8C9De3t77Nq1C7/97W+h0WgQHh6OmTNnaj3m3bhxI959913Mnz8f8+fPx+uvvy6WcbuA5pnKBrFEhsKgBhTrArdfIOo63C6gfQRBeGr/qF+TSqXw9fVlrw11W0Y7oJiIjBe3C2i/1oyZMpYNYokMAZMbIuoU3C6gY7pyzBRRd8MHuETUKbhdQMd11Zgpou6GyQ0RdQoOiu0cjzeIJaL242MpIuoUrVlIkNsFEJEuMLn5RWJiIry9vfUdBpHR4qBYIjIU7B8mok7zeNBrQUGB1tgaqVQKhULBQbFEpBNMboioU3FQLBHpW7d9LJWfn4/x48dDJpNh2LBhSEtLE8uKiooQGRkJFxcXeHp6Ys2aNVp/hX7++efo27cvHBwcsHDhQkyZMgXr1q3Tw10QGabHg2IdHR25XQAR6Vy3TW4WLFgANzc3FBYWIj4+Hvv37xfLZs+eDalUiqysLFy6dAmJiYnYunUrAOD777/H3LlzsXPnThQXF2PEiBE4ffq0vm6DiIiIfqXbJjfJycnYtm0bbGxs4O/vj5iYGAA/9+hcuHAB27dvh52dHby8vLBmzRrExsYCAA4fPowXXngBL730EiwsLLBw4UL4+vrq8U6IiIjoSd02ubG2toarq6v43svLC8DPm3laW1tDLpeLZT4+PsjLywMAPHjwAEqlUutanp6eOoiYiIiIWqPbJjfV1dV4+PCh+D4nJwcA4OHhgerqahQVFYllKpUKHh4eAAB3d/enNgZ8fC4RERHpX7dNboKDg/H++++jqqoK6enp2Lt3LwCgd+/eGDNmDJYvX46Kigrk5ORg06ZNiI6OBgBERkbi3LlzOHPmDB49eoQDBw7g+++/hyAI0Gg0KC0thUaj4eaAREREetJtk5u//OUvyM3NhaurK2bPno033nhDLEtISEBVVRW8vLwQFhaGyZMnY+XKlQAAPz8/HDx4EG+++SacnJxw5coVjB49GuXl5VCpVMjLy4NKpUJ6ejrKysr0dXtERETdlkToZl0MarUaDg4OKCsrg0wm6/D1ysrKEBQUhEWLFmHKlClPlXM3XyIioo5ry+/vbttz0xH//Oc/UV5ejurqamzevBk//PADRo0a1WjdwsJCPqIiIiLSISY37XD69Gl4eXnBxcUFx48fxyeffAJHR8dG69bV1aGiokK3ARIREXVjfCzVAaWlpeIU8eZ4eHg0mfwQERFRy/hYSkcsLFq3NVdr6xEREVHHMbnpAFtb2xYTF6lUCltbWx1FRERERExuOkAikcDNza3ZOgqFgpsGEhER6RCfl3TQ42neBQUFWjuHS6VSKBQKTgMnIiLSMSY3ncDBwQEymQwVFRV49OgRLCwsYGtryx4bIiIiPeBjqU4ikUhgZ2cHR0dH2NnZMbEhg5GUlMTZekTUrTC5ISIiIpPC5IbIiBQVFSEyMhIuLi7w9PTEmjVrxLFe3377LcaOHYtevXrBxcUF77zzDoqLizFx4kSUlZXBzs4OdnZ2uHTpEgAgLi4O/fv3h6OjI0aNGoWUlBTx64SHh2PFihUIDw+Hvb09QkJCcPfuXbFcIpHg5s2b4vsdO3YgPDwcACAIAlatWgWFQgGZTAZfX18cP3686xuHiOgXTG6IjMjs2bMhlUqRlZWFS5cuITExEVu3bkV+fj7Gjh2LV155BQ8ePEB2djYiIyPh5OSEr776Cg4ODtBoNNBoNHjuuefwf//3f3jzzTexd+9e/PDDD3jllVfw0ksvaW32+te//hVbtmxBcXExxo4di4iICK1B8005e/YsEhISkJKSArVajXPnzsHX17crm4WISAuTGyIjkZ+fjwsXLmD79u2ws7ODl5cX1qxZg9jYWMTFxWHYsGF46623YG1tDRsbGzz33HNNXuvQoUOYM2cORo8eDalUiqVLl6Jnz544ceKEWOfVV19FSEgILC0tsW7dOhQVFeHq1astximVSlFdXY3U1FTU1dXB09OTyQ0R6RSTGyIjkZeXB2tra8jlcvGYj48P8vLykJ2djX79+rXpWt7e3lrHnnnmGaxduxY7duwAAHh5eYllUqkUbm5uyM/Pb/HaY8aMwfr167F27Vo4OzvjN7/5DbKyslodGxFRRzG5ITISHh4eqK6uRlFRkXhMpVLBw8MDXl5eyMzMbPQ8M7Onf8w9PDygUqm0jqlUKlhZWYnvs7OzxX/X1dWhoKAAvXv3BvDz6tyVlZVieUFBgda13nrrLVy9ehU5OTmwsrLCu+++2/obJSLqICY3REaid+/eGDNmDJYvX46Kigrk5ORg06ZNiI6ORlRUFK5fv449e/agpqYGlZWV4sBhuVyO8vJyPHz4ULzWnDlzEB8fj8uXL+PRo0f45JNPUFxcjF69eol1Dh8+jGvXrqG2thYbNmyAi4sLgoODAQBDhw7FoUOH8OjRI9y8eROHDh0Sz7tx4waSk5NRW1uLHj16iNuUaDQalJaWQqPRoJvt10tEOsZdwYmMSGFhIRYvXoykpCQ8evQIDQ0NEAQBTk5OiIqKwueff47MzExYW1tDEATY29tj9erVSEtLw9GjR1FTU4NnnnkGeXl54uBgQRAwePBg/PnPf8by5csxbdo0JCYmIjAwEIcOHYJarcbQoUOxbds27Nu3DxcuXEB9fT0EQUBlZSVGjRqFoUOH4sqVK0hKSsL58+exbNky3Lt3D1KpFM8++yxWrVoFV1dX8T4sLCzg5ubGFbyJqNXa8vubKxQTGRGFQoHPP/8c33//PYKCgpCSkgJ/f38UFRWhqKgI/fr1w8KFC7Fhwwa8++67uHz5Ml588UV899132LdvH27duoXy8nKMHDkSP/30E2bMmAE/Pz/s379f6+vU1tbi2LFjeO211/DHP/4RABASEoKwsDDcu3cPVVVVeOWVVzBq1Chs3LhR69wXXnhBnCZeVlaG3Nzcp+7j0aNH4nEmOETU2fhYisgImZubQxAEpKamoqqqCnK5HIMHDwYAODs7Y9myZZBKpQgPD4e3t7eYbAQGBmLUqFGQSqWQy+V47733kJSUpHXt+/fv4+bNmwgMDMT27dshkUjwzTffICMjA9u2bYONjQ2cnJzwu9/9DgkJCU3GKAjCU2Nxfq2wsJCPqIio07HnhsgI9enTBwcPHsTOnTvx+uuvIzg4GFu3bgUArdlUwM+Df8vLywEAmZmZWLZsGW7cuAGNRoOGhgZIpVKt+keOHAEAhIWFicdUKhVKS0u1xuQIgoD6+vomY3y811pz6urqUFFRATs7u1bcNRFR67DnhshIRUZG4uLFiygqKkJgYCDmzp3b4jkxMTHo3bs30tLSoFarERcX91TPycqVKzFz5kx8+eWXUKvVAAClUglXV1eUlpaKr7KyMmg0mia/VmsW/GtLPSKi1mJyQ2SE0tPTcfbsWVRVVcHS0hJ2dnawsGi5I1atVsPe3h4ymQy5ubnYtm3bU3XMzMzw17/+FQMGDMD48eNRVlaGZ599FkqlEh988AHKy8shCAKys7Nx8uTJJmdBtSaettQjImotJjcmQKVSQSKRoLS0VN+hkI7U1tZi7dq1kMvlcHJywoULFxAbG9viedu3b8fx48chk8kQERGB3/zmN43WMzMzw/79+xEUFIRx48ZBrVbj+PHjyM/PR//+/eHg4ICJEyciOTkZKpUKeXl5UKlUSE9PF7dweDwFvDlSqRS2trZtvn8iouZwKrgJUKlUeOaZZ1BSUgJHR0d9h0PdQFOzoB5TKpXiz1lr6hERtaQtv7/Zc2Ng1Go1Fi9eDC8vL8hkMjz77LPIzc3F9u3b0a9fP9jb26NPnz7YuXOneM6IESMA/LzqrJ2dHeLj4/UVPnUDbZkF5eDgAKVS+VQPjlQqZWJDRF2GD7sNzGuvvYbKykpcuXIFCoUCt27dQo8ePeDl5YULFy7Aw8MDSUlJmDRpEoYMGYKwsDBcv35dXJiNPTfU1do6C8rBwQEymUw8z8LCAra2tpBIJDqKmIi6GyY3BqSoqAjHjh1DdnY23N3dAQBDhgwBAK2xEWPGjMGECROQlJSkNV2XSBfaMwtKIpFwujcR6YxeH0utW7cOEolE6+Xv79/sOX//+9/h7+8Pa2trDBo0CCdPntRRtF0vOzsbVlZW8PT0fKosPj4eQ4cORa9eveDo6IiTJ0/ixx9/1EOU1N1xFhQRGTq9j7kJCAhAQUGB+PrXv/7VZN3k5GTMmjUL8+fPx7///W9MmzYN06ZNw507d3QYcdfx8vJCTU3NUwMwc3JyEB0dja1bt+Lhw4coLS3FpEmTxGm3T+76LAgCNyg0EN7e3khMTER8fDxCQ0P1HU6n4SwoIjJ0ek9uLCwsoFAoxJezs3OTdf/85z/jpZdewooVK9C/f39s3LgRQ4cO1Rpca8zkcjkiIiIQExODgoICNDQ04N///jdyc3MhCAJcXV1hZmaGkydP4syZM+J5Li4uMDMzw61bt5Cent7k1FzSj6ioKCQnJ+s7jE4jkUjg5ubWbB2FQsExNUSkN3pPbjIyMuDu7g4fHx9ERUUhJyenybpXrlzBuHHjtI5NmDABV65c6eowdebgwYNQKpUYPnw4HB0dERMTAy8vL6xZswZjx46Fk5MTDh8+jKlTp4rn9OjRA6tWrcL06dMxYsQInDhxQix7vEEhExzqTJwFRUSGTK/r3Hz11VfQaDTw8/NDQUEB1q9fj/z8fNy5cwf29vZP1be0tMTBgwcxa9Ys8dinn36K9evXo6ioqNGvUVNTg5qaGvG9Wq2GUqk0qXVuBEFAenp6swM9pVIpfH19+de0Dnl7e2PHjh0oLS3Fjh07xM0ri4qK8M477+DixYvo0aMH5s6di/Xr18PCwgJJSUmYNm0aPv74Y6xfvx6VlZWYP3++uG+UoREEgbOgiEgn2rLOjV5H/E2cOFH89+DBgzFy5Eh4eXnhyJEjmD9/fqd8jS1btmD9+vWdci1DxQ0Kjcvs2bOhUCiQlZWF4uJiTJo0Cba2tvjd734HACgvL0daWhoyMjKQlZWF4cOHY9KkSQgPD9dv4I3gLCgiMkR6fyz1JEdHR/j6+iIzM7PRcoVC8VQPTVFRERQKRZPXXL16NcrKysRXc6ulGituUGg88vPzceHCBWzfvh12dnbiI8cnt04QBAG///3vYW1tjf79+yM0NBTffvut/oImIjIyBpXcaDQa3Lt3r8nBiiEhITh//rzWsbNnzyIkJKTJa1pZWUEmk2m9TA2n5hqPvLw8WFtbQy6Xi8d8fHyQl5cnvpfJZLCxsRHf29raory8XKdxEhEZM70mN8uXL8fXX38NlUqF5ORkvPzyyzA3NxfH1MybNw+rV68W6y9ZsgSnTp3CH//4R3z33XdYt24dvvnmGyxevFhft2AQODXXeHh4eKC6ulqrB1KlUsHDw0OPURERmRa9Jjd5eXmYNWsW/Pz8EBkZCScnJ1y9ehUuLi4Afl7f5ck9bEJDQ5GQkIB9+/YhMDAQn3/+ORITEzFw4EB93YJB4NRc49G7d2+MGTMGy5cvR0VFBXJycrBp0yZER0c3ex7XLyIiaj29Pqf429/+1mx5UlLSU8dmzJiBGTNmdFFExuvx1NuCggKtsTVSqRQKhYJTcw1IQkKCuDlqjx49EBUVhZUrVzZZv66uDsXFxVCpVOIxCwsLuLm58f+ViKgRep0Krg9tmUpmjDg117S0NAiea8oQUXdhNFPBqfNxaq7pEARB67FsYwoLCyGTyZjAEhE9waBmSxHRf7Rl/SIiIvoPJjdEBorrFxERtQ+TGyIDxfWLiIjah8kNkYHi+kVERO3D5IbIQHH9IiKi9mF/NpEB4/pFRERtx+SGyMA5ODhAJpNx/SIiolZickNkBLh+ERFR63HMDREREZkUJjdERERkUpjcEBERkUlhckNEREQmhckNERERmRQmN0RERGRSmNwQERGRSWFyQ0RERCaFyQ0RERGZFCY3REREZFKY3BAREZFJYXJDREREJoXJDREREZkUJjdERERkUpjcEBERkUlhckNEREQmhckNERERmRQmN0RERGRSmNwQERGRSWFyQ0RERCaFyQ0RERGZFCY3REREZFKY3BAREZFJYXJDREREJoXJDREREZkUJjdERERkUpjcEBERkUlhckNEREQmhckNERERmRQmN0RERGRSmNwQERGRSWFyQ0RERCaFyQ0RERGZFINJbj766CNIJBIsXbq0yTqxsbGQSCRaL2tra90FSURERAbPQt8BAMCNGzewd+9eDB48uMW6MpkM6enp4nuJRNKVoREREZGR0XvPjUajQVRUFPbv34+ePXu2WF8ikUChUIgvuVyugyiJiIjIWOg9uXn77bcxefJkjBs3rlX1NRoNvLy8oFQqERERgdTU1C6OkIiIiIyJXh9L/e1vf0NKSgpu3LjRqvp+fn44cOAABg8ejLKyMnz88ccIDQ1FamoqPDw8Gj2npqYGNTU14nu1Wt0psRMREZFh0lvPTW5uLpYsWYL4+PhWDwoOCQnBvHnzEBQUhOeffx5Hjx6Fi4sL9u7d2+Q5W7ZsgYODg/hSKpWddQtERERkgCSCIAj6+MKJiYl4+eWXYW5uLh6rr6+HRCKBmZkZampqtMqaMmPGDFhYWOB///d/Gy1vrOdGqVSirKwMMpms4zdCREREXU6tVsPBwaFVv7/19ljqhRdewO3bt7WOvf766/D398eqVataldjU19fj9u3bmDRpUpN1rKysYGVl1eF4iYiIyDjoLbmxt7fHwIEDtY7Z2trCyclJPD5v3jz07t0bW7ZsAQBs2LABwcHB6Nu3L0pLS7Ft2zZkZ2djwYIFOo+fiIiIDJNBrHPTlJycHJiZ/WdYUElJCRYuXIjCwkL07NkTw4YNQ3JyMgYMGKDHKImIiMiQ6G3Mjb605ZkdERERGYa2/P7W+zo3RERERJ2JyQ0RERGZFCY3REREZFKY3BAREZFJYXJDREREJoXJDREREZkUJjdERERkUpjcEBERkUlhckNEREQmhckNERERmRQmN0TNSExMhLe3t77DICKiNmByQ0RERCaFyQ1ROwiCgPr6en2HQUREjWByQ/SEvLw8jB8/HjKZDMOGDUNaWppY5u3tjS1btiA4OBg2NjZIS0tDXFwcBg4cCHt7e3h6emLt2rUQBEE8JzU1FcHBwbC3t8eYMWOwcuVKhIeHi+WZmZmYMGECevXqhT59+mDHjh1iWWxsLIKCgrBx40a4urpCLpdrlRMRUeOY3BA9Yfbs2XBzc0NhYSHi4+Oxf/9+rfLY2FgcPHgQGo0Gfn5+cHJywtGjR6FWq/Hll19i3759SEhIAADU1dVh6tSpmDhxIoqLi/HRRx/hwIED4rUePXqEKVOmIDAwEA8ePMCxY8ewdetW8Xzg5+TIxsYG+fn5OHz4MFasWIF79+7ppjGIiIwUkxuiX+Tm5uLSpUvYtm0bbGxs4O/vj5iYGK06b775Jvz8/GBubg5LS0tMnDgRvr6+kEgkCAoKwqxZs5CUlAQAuHr1KoqLi7FmzRpYWlpi5MiRmDlzpnita9euoaCgAL///e9hbW2NwYMHY/HixYiNjRXrODs7Y9myZZBKpQgPD4e3tzdu3rypg9YgIjJeTG6IfvHgwQNYW1vD1dVVPObl5aVVx9PTU+v96dOnERoaCmdnZzg4OGDPnj348ccfxeu5ubnBwsKi0fPz8vLg7u4OS0tL8ZiPjw/y8vLE93K5XOvr2draory8vAN3SURk+pjcEP3C3d0d1dXVePjwoXgsJydHq46Z2X9+ZGprazF9+nQsWrQI+fn5KCsrQ0xMjDjmxt3dHYWFhXj06FGj1/Pw8MCDBw9QV1cnHlOpVPDw8Oj0eyMi6k6Y3BD9QqlUIiwsDO+//z6qqqqQnp6OvXv3Nlm/pqYG1dXVcHJygpWVFa5du6Y1XiY4OBiOjo7YsmUL6urqcOPGDRw5ckQsHzFiBORyOf77v/8bNTU1uHPnDj755BNER0c3G6cgCNBoNCgtLYVGo9EawExERExuiLQkJCQgNzcXrq6umD17Nt54440m69rb22PXrl347W9/C5lMhk2bNmmNqZFKpfjHP/6B48ePo2fPnli5ciXmzJkDKysrsfz48eP49ttvoVAoMHXqVLz33nuYPXt2k1+zvr4ehYWFUKlUyMvLg0qlQnp6OsrKyjqvEYiIjJxE6GZ/9qnVajg4OKCsrAwymUzf4VA3s2jRIjQ0NDw1C6s1ysrKkJub22S5UqmEg4NDR8IjIjJYbfn9zZ4boi506dIl5ObmoqGhAefPn0d8fDxmzJjR5usIgoCCgoJm6xQWFvIRFRERAIuWqxBRe92/fx+vvvoqSkpK4OHhgY8++gjjx49v83UqKiq0BiY3pq6uDhUVFbCzs2tvuEREJoHJDVEXio6ObnGAcGu0lNi0tR4RkSnjYykiI/DkWjmdUY+IyJQxuSEyAra2ti0mLlKpFLa2tjqKiIjIcDG5ITICEokEbm5uzdZRKBSQSCQ6ioiIyHCxD5vISDye5l1QUKA1tkYqlUKhUHAaOBHRL5jcEBkRBwcHyGQycfaUhYUFbG1t2WNDRPQEJjdERkYikXC6NxFRMzjmhoiIiEwKkxsiIiIyKUxuiIiIyKQwuSEiIiKTwuSGiIiITAqTGyIiIjIpTG6IiIjIpDC5ISIiIpPC5IaIiIhMSrdboVgQBACAWq3WcyRERETUWo9/bz/+Pd6cbpfclJeXAwCUSqWeIyEiIqK2Ki8vb3GjYInQmhTIhDQ0NODBgwewt7c3+c0G1Wo1lEolcnNzIZPJ9B2OQWIbtQ7bqWVso5axjVrGNmqaIAgoLy+Hu7s7zMyaH1XT7XpuzMzM4OHhoe8wdEomk/GHpAVso9ZhO7WMbdQytlHL2EaNa6nH5jEOKCYiIiKTwuSGiIiITAqTGxNmZWWFDz/8EFZWVvoOxWCxjVqH7dQytlHL2EYtYxt1jm43oJiIiIhMG3tuiIiIyKQwuSEiIiKTwuSGiIiITAqTGyIiIjIpTG6M1Lp16yCRSLRe/v7+zZ7z97//Hf7+/rC2tsagQYNw8uRJHUWrH21to9jY2KfqW1tb6zBi/cnPz8ecOXPg5OSEHj16YNCgQfjmm2+aPScpKQlDhw6FlZUV+vbti9jYWN0EqydtbaOkpKSnvp8kEgkKCwt1GLXueHt7N3q/b7/9dpPndLfPJKDt7dSdP5c6otutUGxKAgICcO7cOfG9hUXT/53JycmYNWsWtmzZgilTpiAhIQHTpk1DSkoKBg4cqItw9aItbQT8vCpoenq6+N7Ut+gAgJKSEoSFhWHMmDH46quv4OLigoyMDPTs2bPJc7KysjB58mTExMQgPj4e58+fx4IFC+Dm5oYJEyboMHrdaE8bPZaenq610qyrq2tXhqo3N27cQH19vfj+zp07ePHFFzFjxoxG63fXz6S2thPQPT+XOkwgo/Thhx8KgYGBra4fGRkpTJ48WevYyJEjhUWLFnVyZIajrW302WefCQ4ODl0Wj6FatWqVMGrUqDads3LlSiEgIEDr2MyZM4UJEyZ0ZmgGoz1tdPHiRQGAUFJS0jVBGbglS5YIffr0ERoaGhot746fSY1pqZ266+dSR/GxlBHLyMiAu7s7fHx8EBUVhZycnCbrXrlyBePGjdM6NmHCBFy5cqWrw9SrtrQRAGg0Gnh5eUGpVCIiIgKpqak6ilR/vvzySwwfPhwzZsyAq6srhgwZgv379zd7Tnf7fmpPGz0WFBQENzc3vPjii7h8+XIXR2oYamtrERcXhzfeeKPJXobu9j3UmNa0E9A9P5c6ismNkRo5ciRiY2Nx6tQp7N69G1lZWXjuuedQXl7eaP3CwkLI5XKtY3K53GSf/wNtbyM/Pz8cOHAA//jHPxAXF4eGhgaEhoYiLy9Px5Hr1v3797F7927069cPp0+fxptvvol3330XBw8ebPKcpr6f1Go1qqqqujpknWtPG7m5uWHPnj344osv8MUXX0CpVCI8PBwpKSk6jFw/EhMTUVpaitdee63JOt3xM+nXWtNO3fVzqcP03XVEnaOkpESQyWTCX/7yl0bLpVKpkJCQoHVs165dgqurqy7CMwgttdGv1dbWCn369BE++OCDLo5Mv6RSqRASEqJ17J133hGCg4ObPKdfv37C5s2btY6dOHFCACBUVlZ2SZz61J42aszo0aOFOXPmdGZoBmn8+PHClClTmq3Dz6TWtdOvdZfPpY5iz42JcHR0hK+vLzIzMxstVygUKCoq0jpWVFQEhUKhi/AMQktt9GtSqRRDhgxpdX1j5ebmhgEDBmgd69+/f7OP8Jr6fpLJZOjRo0eXxKlP7WmjxowYMcLkv5+ys7Nx7tw5LFiwoNl63f0zqbXt9Gvd5XOpo5jcmAiNRoN79+7Bzc2t0fKQkBCcP39e69jZs2cREhKii/AMQktt9Gv19fW4fft2q+sbq7CwMK2ZGADw/fffw8vLq8lzutv3U3vaqDE3b940+e+nzz77DK6urpg8eXKz9brb99Cvtbadfq27fC51mL67jqh9li1bJiQlJQlZWVnC5cuXhXHjxgnOzs7Cw4cPBUEQhLlz5wrvv/++WP/y5cuChYWF8PHHHwt3794VPvzwQ0EqlQq3b9/W1y10uba20fr164XTp08L9+7dE7799lvh1VdfFaytrYXU1FR93YJOXL9+XbCwsBA2bdokZGRkCPHx8YKNjY0QFxcn1nn//feFuXPniu/v378v2NjYCCtWrBDu3r0r7Nq1SzA3NxdOnTqlj1vocu1poz/96U9CYmKikJGRIdy+fVtYsmSJYGZmJpw7d04ft6AT9fX1gqenp7Bq1aqnyviZ9B9taafu+rnUUUxujNTMmTMFNzc3wdLSUujdu7cwc+ZMITMzUyx//vnnhejoaK1zjhw5Ivj6+gqWlpZCQECAcOLECR1HrVttbaOlS5cKnp6egqWlpSCXy4VJkyYJKSkpeohc9/75z38KAwcOFKysrAR/f39h3759WuXR0dHC888/r3Xs4sWLQlBQkGBpaSn4+PgIn332me4C1oO2ttEf/vAHoU+fPoK1tbXQq1cvITw8XLhw4YKOo9at06dPCwCE9PT0p8r4mfQfbWmn7vy51BESQRAEffceEREREXUWjrkhIiIik8LkhoiIiEwKkxsiIiIyKUxuiIiIyKQwuSEiIiKTwuSGiIiITAqTGyIiIjIpTG6IiJ4gkUiQmJio7zCIqAOY3BCRwSksLMSSJUvQt29fWFtbQy6XIywsDLt370ZlZaW+wyMiA2eh7wCIiJ50//59hIWFwdHREZs3b8agQYNgZWWF27dvY9++fejduzemTp2q7zCJyICx54aIDMpbb70FCwsLfPPNN4iMjET//v3h4+ODiIgInDhxAv/1X/8FAMjJyUFERATs7Owgk8kQGRmJoqIirWvt3r0bffr0gaWlJfz8/HDo0CGt8oyMDIwePRrW1tYYMGAAzp49q7P7JKKuw+SGiAxGcXExzpw5g7fffhu2traN1pFIJGhoaEBERAR++uknfP311zh79izu37+PmTNnivWOHTuGJUuWYNmyZbhz5w4WLVqE119/HRcvXgQANDQ0YPr06bC0tMS1a9ewZ88erFq1Sif3SURdixtnEpHBuHbtGoKDg3H06FG8/PLL4nFnZ2dUV1cDAN5++22MGzcOEydORFZWFpRKJQAgLS0NAQEBuH79Op599lmEhYUhICAA+/btE68TGRmJiooKnDhxAmfOnMHkyZORnZ0Nd3d3AMCpU6cwceJEHDt2DNOmTdPdjRNRp2LPDREZvOvXr+PmzZsICAhATU0N7t69C6VSKSY2ADBgwAA4Ojri7t27AIC7d+8iLCxM6zphYWFa5UqlUkxsACAkJEQHd0NEXY0DionIYPTt2xcSiQTp6elax318fAAAPXr00EdYRGRk2HNDRAbDyckJL774Inbu3ImKioom6/Xv3x+5ubnIzc0Vj6WlpaG0tBQDBgwQ61y+fFnrvMuXL2uV5+bmoqCgQCy/evVqZ94OEekJkxsiMiiffvopHj16hOHDh+Pw4cO4e/cu0tPTERcXh++++w7m5uYYN24cBg0ahKioKKSkpOD69euYN28enn/+eQwfPhwAsGLFCsTGxmL37t3IyMjA9u3bcfToUSxfvhwAMG7cOPj6+iI6Ohq3bt3CpUuXsGbNGn3eOhF1FoGIyMA8ePBAWLx4sfDMM88IUqlUsLOzE0aMGCFs27ZNqKioEARBELKzs4WpU6cKtra2gr29vTBjxgyhsLBQ6zqffvqp4OPjI0ilUsHX11f4n//5H63y9PR0YdSoUYKlpaXg6+srnDp1SgAgHDt2TFe3SkRdgLOliIiIyKTwsRQRERGZFCY3REREZFKY3BAREZFJYXJDREREJoXJDREREZkUJjdERERkUpjcEBERkUlhckNEREQmhckNERERmRQmN0RERGRSmNwQERGRSWFyQ0RERCbl/wHw+WZbuENG9wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.kdeplot(x=x, y=y, cmap=\"inferno\", fill=True, thresh=0, levels=100)\n",
        "plt.scatter(x, y, color=\"white\", alpha=0.5, s=10)\n",
        "\n",
        "plt.xlabel('Good')\n",
        "plt.ylabel('Evil')\n",
        "plt.title('Animal Good vs Evil Density Heatmap')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Uvv0naXadMD9",
        "outputId": "a796248e-62b0-40d8-8506-7cf3ffcda72c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}