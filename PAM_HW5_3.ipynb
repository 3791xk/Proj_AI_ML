{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpABSpUNyoczmhyqvtowiQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: NLP and Attention Mechanism"
      ],
      "metadata": {
        "id": "-1SA1owduURV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1"
      ],
      "metadata": {
        "id": "Og5tHrne-p5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Flatten, Dense, LSTM, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "XHFNTqJWFzyU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_tokens = np.array([\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9],\n",
        "    [1.0, 1.1, 1.2],\n",
        "    [0.1, 0.2, 0.3],\n",
        "    [1.3, 1.4, 1.5]\n",
        "])\n",
        "Q = K = V = embedded_tokens\n",
        "\n",
        "print(scaled_dot_product_attention(Q, K, V))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AuJref3R3C5",
        "outputId": "d34e8774-6162-4af7-fd95-05750842c958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6703881  0.7703881  0.8703881 ]\n",
            " [0.77624039 0.87624039 0.97624039]\n",
            " [0.87532355 0.97532355 1.07532355]\n",
            " [0.96179317 1.06179317 1.16179317]\n",
            " [0.6703881  0.7703881  0.8703881 ]\n",
            " [1.03314235 1.13314235 1.23314235]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uYXlpg1MuPl9"
      },
      "outputs": [],
      "source": [
        "# https://medium.com/@funcry/in-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e\n",
        "# https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/\n",
        "\n",
        "def scaled_dot_product_attention(queries, keys, values):\n",
        "  dot_prod = np.dot(queries, keys.T) # QK^T\n",
        "  dk = keys.shape[-1]\n",
        "  scaled = dot_prod / np.sqrt(dk)  # / sqrt(dk)\n",
        "  exp_x = np.exp(scaled - np.max(scaled, axis=-1, keepdims=True))  # softmax\n",
        "  norm = exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "  return np.dot(norm, values) # *V"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2"
      ],
      "metadata": {
        "id": "dfH6HOi3HeS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "# https://arxiv.org/abs/1508.04025v5\n",
        "# https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
        "\n",
        "# returns train, inference_encoder and inference_decoder models\n",
        "def define_models(n_input, n_output, n_units):\n",
        "  # encoder\n",
        "  encoder_inputs = Input(shape=(None, n_input))\n",
        "  encoder = LSTM(n_units, return_state=True)\n",
        "  encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "  encoder_states = [state_h, state_c]\n",
        "  # decoder\n",
        "  decoder_inputs = Input(shape=(None, n_output))\n",
        "  decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
        "\n",
        "  # Attention helps decoder focus on relevant output\n",
        "  decoder_hidden_state = Input(shape=(1, n_units))\n",
        "  context_vector = tf.keras.layers.Lambda(lambda x: scaled_dot_product_attention(x[0], x[1], x[1]), output_shape=(1, n_units))([decoder_hidden_state, encoder_outputs])\n",
        "  decoder_combined_inputs = keras.layers.Concatenate(axis=-1)([context_vector, decoder_inputs])\n",
        "\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder_combined_inputs, initial_state=encoder_states)\n",
        "  decoder_dense = Dense(n_output, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs, decoder_hidden_state], decoder_outputs)\n",
        "\n",
        "  # define inference encoder\n",
        "  encoder_model = keras.Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
        "  # define inference decoder\n",
        "  decoder_state_input_h = Input(shape=(n_units,))\n",
        "  decoder_state_input_c = Input(shape=(n_units,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "  decoder_hidden_state_input = Input(shape=(1, n_units))\n",
        "  context_vector_inf = keras.layers.Lambda(lambda x: scaled_dot_product_attention(x[0], x[1], x[1]), output_shape=(1, n_units))([decoder_hidden_state_input, encoder_outputs])\n",
        "  decoder_combined_inputs_inf = keras.layers.Concatenate(axis=-1)([context_vector_inf, decoder_inputs])\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  decoder_outputs, state_h, state_c = decoder_lstm(decoder_combined_inputs_inf, initial_state=decoder_states_inputs)\n",
        "  # decoder_states = [state_h, state_c]\n",
        "  # decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = keras.Model([decoder_inputs, decoder_hidden_state_input, encoder_outputs] + decoder_states_inputs, [decoder_outputs, state_h, state_c])\n",
        "  # return all models\n",
        "  return model, encoder_model, decoder_model"
      ],
      "metadata": {
        "id": "oAeAEbwgv_jb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3\n",
        "\n",
        "The encoder-decoder model will now be used for a machine translation task, using a subset of the Multi30k dataset.\n",
        "\n",
        "https://github.com/multi30k/dataset\n",
        "\n",
        "\n",
        "https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/\n"
      ],
      "metadata": {
        "id": "QiaM2QUb2ArY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Data"
      ],
      "metadata": {
        "id": "EvEQqJJKNv4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --recursive https://github.com/multi30k/dataset.git multi30k-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HZa8oijK46G",
        "outputId": "631583a4-4250-44eb-81c0-37dfa891c348"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'multi30k-dataset'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 313 (delta 17), reused 21 (delta 16), pack-reused 281 (from 1)\u001b[K\n",
            "Receiving objects: 100% (313/313), 18.21 MiB | 18.24 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n",
            "Submodule 'scripts/subword-nmt' (https://github.com/rsennrich/subword-nmt.git) registered for path 'scripts/subword-nmt'\n",
            "Cloning into '/content/multi30k-dataset/scripts/subword-nmt'...\n",
            "remote: Enumerating objects: 622, done.        \n",
            "remote: Counting objects: 100% (46/46), done.        \n",
            "remote: Compressing objects: 100% (30/30), done.        \n",
            "remote: Total 622 (delta 25), reused 31 (delta 16), pack-reused 576 (from 1)        \n",
            "Receiving objects: 100% (622/622), 261.27 KiB | 1.34 MiB/s, done.\n",
            "Resolving deltas: 100% (374/374), done.\n",
            "Submodule path 'scripts/subword-nmt': checked out '80b7c1449e2e26673fb0b5cae993fe2d0dc23846'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        sentences = f.read().strip().split(\"\\n\")\n",
        "    return sentences\n",
        "\n",
        "path_en = 'multi30k-dataset/data/task1/tok/train.lc.norm.tok.en'\n",
        "train_sentences = load_data(path_en)\n",
        "path_de = 'multi30k-dataset/data/task1/tok/train.lc.norm.tok.de'\n",
        "test_sentences = load_data(path_de)\n",
        "\n",
        "x_train = train_sentences[:500]\n",
        "y_train = test_sentences[:500]\n",
        "\n",
        "print(train_sentences[:5])\n",
        "print(test_sentences[:5])\n",
        "print(len(train_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TabwUUyrMUfq",
        "outputId": "6459951e-6387-4af0-8369-48bc7536abb4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['two young , white males are outside near many bushes .', 'several men in hard hats are operating a giant pulley system .', 'a little girl climbing into a wooden playhouse .', 'a man in a blue shirt is standing on a ladder cleaning a window .', 'two men are at the stove preparing food .']\n",
            "['zwei junge weiße männer sind im freien in der nähe vieler büsche .', 'mehrere männer mit schutzhelmen bedienen ein antriebsradsystem .', 'ein kleines mädchen klettert in ein spielhaus aus holz .', 'ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster .', 'zwei männer stehen am herd und bereiten essen zu .']\n",
            "29000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "xOnRaaXGNSZI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(filters=\"\")\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "x_vocab_size = len(tokenizer.word_index) + 1\n",
        "x_max_length = max(len(seq) for seq in x_sequences)\n",
        "x_padded = pad_sequences(x_sequences, maxlen=x_max_length, padding=\"post\")\n",
        "x_input = x_padded[:, :-1]\n",
        "X1 = to_categorical(x_input, num_classes=x_vocab_size)\n",
        "\n",
        "tokenizer.fit_on_texts(y_train)\n",
        "y_sequences = tokenizer.texts_to_sequences(y_train)\n",
        "y_vocab_size = len(tokenizer.word_index) + 1\n",
        "y_max_length = max(len(seq) for seq in y_sequences)\n",
        "y_padded = pad_sequences(y_sequences, maxlen=y_max_length, padding=\"post\")\n",
        "y_input = y_padded[:, :-1]\n",
        "X2 = to_categorical(y_input, num_classes=y_vocab_size)\n",
        "\n",
        "y_output = y_padded[:, 1:]\n",
        "y_target = to_categorical(y_output, num_classes=y_vocab_size)\n",
        "\n",
        "print(x_padded)\n",
        "print(x_padded.shape)\n",
        "print(x_vocab_size)\n",
        "print(x_max_length)\n",
        "print(y_padded.shape)\n",
        "\n",
        "print(X1.shape)\n",
        "print(X2.shape)\n",
        "print(y_target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a78I2Wv5N1AY",
        "outputId": "f03ee0b2-0570-4a04-b990-8a6ff96310c6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 13  18  11 ...   0   0   0]\n",
            " [103  32   3 ...   0   0   0]\n",
            " [  1  33  26 ...   0   0   0]\n",
            " ...\n",
            " [  1 159  37 ...   0   0   0]\n",
            " [211  14 283 ...   0   0   0]\n",
            " [  1 180 215 ...   0   0   0]]\n",
            "(500, 35)\n",
            "1220\n",
            "35\n",
            "(500, 44)\n",
            "(500, 34, 1220)\n",
            "(500, 43, 2500)\n",
            "(500, 43, 2500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "KzmoWcs2N1sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, infenc, infdec = define_models(x_vocab_size, y_vocab_size, 128)\n",
        "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LGlVYo5WVQDa"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.fit([X1, X2], y_target, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaLnNgJKVn4P",
        "outputId": "b45e9983-ec8e-480e-d854-17cae394e7f7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 884ms/step - accuracy: 0.4709 - loss: 7.6576\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x796c9c7f8210>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.fit([X1, X2], y_target, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjZTQUk9aDlE",
        "outputId": "44cc9ec9-add5-4b28-de8c-71a863f5c22d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 538ms/step - accuracy: 0.7249 - loss: 4.6259\n",
            "Epoch 2/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 459ms/step - accuracy: 0.7144 - loss: 2.1228\n",
            "Epoch 3/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 428ms/step - accuracy: 0.7230 - loss: 1.8363\n",
            "Epoch 4/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 527ms/step - accuracy: 0.7157 - loss: 1.8167\n",
            "Epoch 5/5\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 496ms/step - accuracy: 0.7250 - loss: 1.7493\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x796c96d3ca50>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.fit([X1, X2], y_target, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-MS8JQ_aSof",
        "outputId": "6fca9ea2-3210-4ce0-c64b-3348f5100ef9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 491ms/step - accuracy: 0.7332 - loss: 1.6785\n",
            "Epoch 2/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 500ms/step - accuracy: 0.7371 - loss: 1.6358\n",
            "Epoch 3/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 532ms/step - accuracy: 0.7290 - loss: 1.6688\n",
            "Epoch 4/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 464ms/step - accuracy: 0.7316 - loss: 1.6368\n",
            "Epoch 5/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 465ms/step - accuracy: 0.7320 - loss: 1.6090\n",
            "Epoch 6/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 526ms/step - accuracy: 0.7316 - loss: 1.6041\n",
            "Epoch 7/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 525ms/step - accuracy: 0.7312 - loss: 1.5763\n",
            "Epoch 8/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 472ms/step - accuracy: 0.7282 - loss: 1.5896\n",
            "Epoch 9/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 452ms/step - accuracy: 0.7285 - loss: 1.5888\n",
            "Epoch 10/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 496ms/step - accuracy: 0.7363 - loss: 1.5424\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x796c96b41810>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate target given source sequence\n",
        "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
        "\t# encode\n",
        "\tstate = infenc.predict(source)\n",
        "\t# start of sequence input\n",
        "\ttarget_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
        "\t# collect predictions\n",
        "\toutput = list()\n",
        "\tfor t in range(n_steps):\n",
        "\t\t# predict next char\n",
        "\t\tyhat, h, c = infdec.predict([target_seq] + state)\n",
        "\t\t# store prediction\n",
        "\t\toutput.append(yhat[0,0,:])\n",
        "\t\t# update state\n",
        "\t\tstate = [h, c]\n",
        "\t\t# update target sequence\n",
        "\t\ttarget_seq = yhat\n",
        "\treturn array(output)"
      ],
      "metadata": {
        "id": "nJlwQ8QJbhJG"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = np.expand_dims(X1[0], axis=0)\n",
        "print(test_input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVDgPp0_crwl",
        "outputId": "46a83de3-e331-49cc-faf3-aaf3422f077c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 34, 1220)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X1.shape)\n",
        "my_test = predict_sequence(infenc, infdec, np.expand_dims(X1[0], axis=0) , y_max_length, y_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE7rFuj2bOsE",
        "outputId": "209d4e31-4ac1-4979-bfb2-1be18277711d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 34, 1220)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US3hTgADdKDG",
        "outputId": "85b41f2e-ca14-4ba0-f2a8-c6b376e407bb"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.7809513e-03 1.8604430e-03 7.4829304e-06 ... 3.5775869e-04\n",
            "  6.1969746e-05 5.7597761e-05]\n",
            " [3.1104428e-03 3.0320552e-03 1.3676123e-05 ... 3.7945618e-04\n",
            "  9.9735313e-05 1.0487767e-04]\n",
            " [4.3736571e-03 5.0791660e-03 1.4689128e-05 ... 2.8943416e-04\n",
            "  1.1866018e-04 1.3413813e-04]\n",
            " ...\n",
            " [9.8439121e-01 3.6152357e-03 2.4909036e-07 ... 7.3339891e-07\n",
            "  4.2242568e-06 4.0520817e-06]\n",
            " [9.8439121e-01 3.6152236e-03 2.4908988e-07 ... 7.3339675e-07\n",
            "  4.2242486e-06 4.0520699e-06]\n",
            " [9.8439121e-01 3.6152201e-03 2.4908965e-07 ... 7.3339612e-07\n",
            "  4.2242409e-06 4.0520658e-06]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_sentence = []\n",
        "\n",
        "for pred in my_test:\n",
        "  if isinstance(pred, np.ndarray):\n",
        "    pred_index = np.argmax(pred)\n",
        "  else:\n",
        "    pred_index = pred\n",
        "  # index to word\n",
        "  word = tokenizer.index_word.get(pred_index, '')\n",
        "  if word != '':\n",
        "    final_sentence.append(word)\n",
        "  else:\n",
        "    break\n",
        "\n",
        "result = ' '.join(final_sentence)\n",
        "print(result)\n",
        "print(y_train[0])\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duifITdSdeGe",
        "outputId": "b2070399-4057-4da3-edd8-ba67f9fcbee3"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mann in , , , , , , , , . . . .\n",
            "zwei junge weiße männer sind im freien in der nähe vieler büsche .\n",
            "two young , white males are outside near many bushes .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.geeksforgeeks.org/nlp-bleu-score-for-evaluating-neural-machine-translation-python/\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Calculate BLEU score with weights\n",
        "score = sentence_bleu(x_train[0], result)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUqrdIyBeum_",
        "outputId": "5acc06f7-815a-41f4-c2a2-3fa3a885f511"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2558634180836711e-231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU score is quite bad, which makes sense as this example generated sentence is not at all structured like an actual sentence. This is a bit surprising though as the training accuracy was around $73\\%$, and the loss did decrease from $7.66$ to $1.54$. Something that is likely an issue with this model is that I am using a very small amount of input data that does not allow the network to learn and understand the languages and their relationships."
      ],
      "metadata": {
        "id": "AhUZdWF_fwZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4\n",
        "\n",
        "I made this Transformer for English to German translation based on this tutorial: https://www.tensorflow.org/text/tutorials/transformer. I also used many other sources that are linked below."
      ],
      "metadata": {
        "id": "5gCybxnXiyeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://keras.io/keras_hub/api/tokenizers/byte_pair_tokenizer/\n",
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "def load_data(filepath):\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        sentences = f.read().strip().split(\"\\n\")\n",
        "    return sentences\n",
        "\n",
        "path_en = 'multi30k-dataset/data/task1/tok/train.lc.norm.tok.en'\n",
        "en_sentences = load_data(path_en)\n",
        "path_de = 'multi30k-dataset/data/task1/tok/train.lc.norm.tok.de'\n",
        "de_sentences = load_data(path_de)\n",
        "\n",
        "x_train = en_sentences[:10000]\n",
        "y_train = de_sentences[:10000]\n",
        "\n",
        "print(x_train[:5])\n",
        "print(y_train[:5])\n",
        "print(len(x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDF9QCNXv6sz",
        "outputId": "ea58261c-00b3-46b7-a6c7-d1aea5d0112b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['two young , white males are outside near many bushes .', 'several men in hard hats are operating a giant pulley system .', 'a little girl climbing into a wooden playhouse .', 'a man in a blue shirt is standing on a ladder cleaning a window .', 'two men are at the stove preparing food .']\n",
            "['zwei junge weiße männer sind im freien in der nähe vieler büsche .', 'mehrere männer mit schutzhelmen bedienen ein antriebsradsystem .', 'ein kleines mädchen klettert in ein spielhaus aus holz .', 'ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster .', 'zwei männer stehen am herd und bereiten essen zu .']\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word-level tokenization"
      ],
      "metadata": {
        "id": "SI9byV80s4RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/code/shivanshuman/a-song-of-words-and-tokens\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def get_vocab(sentences):\n",
        "  vocab = Counter()\n",
        "  for sentence in sentences:\n",
        "    for word in sentence.split():\n",
        "      vocab[word] += 1\n",
        "  return vocab\n",
        "\n",
        "en_vocab = get_vocab(x_train)\n",
        "de_vocab = get_vocab(y_train)\n",
        "\n",
        "print(1.most_common(10))\n",
        "print(de_vocab.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn3X4cRHr9Sp",
        "outputId": "a2968990-174e-4188-d272-42292d232066"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 16897), ('.', 9473), ('in', 5015), ('the', 3644), ('on', 2734), ('man', 2606), ('is', 2505), ('and', 2457), ('of', 2233), ('with', 2015)]\n",
            "[('.', 9883), ('ein', 6602), ('einem', 4270), ('in', 3717), (',', 3483), ('eine', 3063), ('mit', 3060), ('auf', 2952), ('und', 2941), ('mann', 2629)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Decoder\n",
        "\n",
        "I did a lot of research into the structure, but due to the complexity and potential issues with mismatches shapes, decided to stick very close to this tutorial and alter it to the simplified specifications: https://www.tensorflow.org/text/tutorials/transformer"
      ],
      "metadata": {
        "id": "-tzg3JSf163u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://arxiv.org/abs/1706.03762\n",
        "# https://arxiv.org/pdf/1706.03762\n",
        "# https://www.tensorflow.org/text/tutorials/transformer\n",
        "# https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "# https://arxiv.org/abs/1508.04025v5"
      ],
      "metadata": {
        "id": "Di4SEo4u_y4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install protobuf~=3.20.3\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "X_I3dY-ZMUPo",
        "outputId": "394a084a-1cf1-4392-e949-c7cc17c4fd60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package libcudnn8 is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "\u001b[1;31mE: \u001b[0mVersion '8.1.0.77-1+cuda11.2' for 'libcudnn8' was not found\u001b[0m\n",
            "\u001b[33mWARNING: Skipping tensorflow-estimator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting protobuf~=3.20.3\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "2e6173569405441b8696d858f6f70ecc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text"
      ],
      "metadata": {
        "id": "eV4lCfMsMWCS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_YeybxFTMway"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "KKlSzEuqOJrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ],
      "metadata": {
        "id": "2a7OP5afOD_5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "4YFSiUUmOLj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ],
      "metadata": {
        "id": "1L0YQRN-OM39"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "m9eGZyLGOco1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ],
      "metadata": {
        "id": "rLFUspVXOilF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Smaller simplified values\n",
        "num_layers = 2\n",
        "d_model = 64\n",
        "dff = 128\n",
        "num_heads = 2\n",
        "dropout_rate = 0.1\n",
        "num_samples = 10000"
      ],
      "metadata": {
        "id": "r_hklax2OoLY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=num_samples,\n",
        "    target_vocab_size=num_samples,\n",
        "    dropout_rate=dropout_rate)"
      ],
      "metadata": {
        "id": "yvqhQ6DBSbse"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}